{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\dataframe\\_pyarrow_compat.py:17: FutureWarning: Minimal version of pyarrow will soon be increased to 14.0.1. You are using 12.0.1. Please consider upgrading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import dask\n",
    "import xarray as xr\n",
    "import xrspatial\n",
    "from dask.diagnostics import ProgressBar\n",
    "from geocube.api.core import make_geocube\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import procesa_bases \n",
    "\n",
    "PATH = \"D:\\World Bank\\CLIENT v2\"\n",
    "DATA_RAW = rf\"{PATH}\\Data\\Data_raw\"\n",
    "DATA_PROC = rf\"{PATH}\\Data\\Data_proc\"\n",
    "DATA_OUT = rf\"{PATH}\\Data\\Data_out\"\n",
    "GPW_PATH = rf\"D:\\Datasets\\Gridded Population of the World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genera y Carga bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_country = procesa_bases.load_WB_country_data()\n",
    "IPUMS_country = procesa_bases.load_IPUMS_country_data(WB_country)\n",
    "IPUMS_country = IPUMS_country.clip(WB_country.total_bounds)\n",
    "population = procesa_bases.load_population_data(bounds=WB_country.total_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesa WB/IPUMS shapes (administrative boundaries) y GPW (population data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterizing shape...\n",
      "Done!\n",
      "Saving WB_country_grid...\n",
      "Rasterizing shape...\n",
      "Done!\n",
      "Saving IPUMS_country_grid...\n"
     ]
    }
   ],
   "source": [
    "### Rasterize WB_country\n",
    "WB_country_grid = procesa_bases.rasterize_shape_like_dataset(\n",
    "    WB_country[[\"ID\", \"geometry\"]], \n",
    "    population\n",
    ")\n",
    "\n",
    "print(\"Saving WB_country_grid...\")\n",
    "WB_country_grid.to_netcdf(rf\"{DATA_PROC}\\WB_country_grid.nc\")\n",
    "        \n",
    "WB_country[[\"ID\", \"OBJECTID\", \"ADM2_CODE\", \"ADM1_CODE\", \"ADM0_CODE\",  \"geometry\"]].to_feather(rf\"{DATA_PROC}\\WB_country_IDs.feather\")\n",
    "\n",
    "\n",
    "### Rasterize IPUMS_country\n",
    "IPUMS_country_grid = procesa_bases.rasterize_shape_like_dataset(\n",
    "    IPUMS_country[[\"ID\", \"geometry\"]], \n",
    "    population\n",
    ")\n",
    "\n",
    "IPUMS_country_path = rf\"{DATA_PROC}\\IPUMS_country_grid.nc\"\n",
    "print(\"Saving IPUMS_country_grid...\")\n",
    "IPUMS_country_grid.to_netcdf(rf\"{DATA_PROC}\\IPUMS_country_grid.nc\")\n",
    "        \n",
    "IPUMS_country.to_feather(rf\"{DATA_PROC}\\IPUMS_country_IDs.feather\")\n",
    "\n",
    "# Export IPUMS IDS to dta\n",
    "dta = IPUMS_country.rename(columns={\"CNTRY_CODE\":\"adm0\", \"GEOLEVEL2\": \"adm2\"})[[\"adm0\", \"adm2\"]]\n",
    "assert dta.duplicated().sum() == 0\n",
    "dta.to_stata(rf\"{DATA_PROC}\\IPUMS_country_IDs.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = IPUMS_country.rename(columns={\"CNTRY_CODE\":\"adm0\", \"GEOLEVEL2\": \"adm2\"})[[\"adm0\", \"adm2\"]]\n",
    "assert dta.duplicated().sum() == 0\n",
    "dta.to_stata(rf\"{DATA_PROC}\\IPUMS_country_IDs.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Droughts, con resolución original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "droughts_path = rf\"{DATA_OUT}\\ERA5_droughts_yearly.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(droughts_path):\n",
    "print(\"Preparing droughts dataset...\")\n",
    "# Genera base de sequías\n",
    "era5 = xr.open_dataset(rf\"{DATA_OUT}\\ERA5_monthly_1970-2021_SPI-SPEI.nc\", chunks={'latitude': 1000, 'longitude': 1000})\n",
    "# Corrije la dimensión x, que va de 0 a 360\n",
    "era5 = era5.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "era5 = utils.coordinates_from_0_360_to_180_180(era5) # FIXME: no se si esto está andando bien, pero creo que si. VERIFICAR\n",
    "\n",
    "# Calcula las sequías anuales\n",
    "spi_yearly = era5.groupby(\"time.year\").min()\n",
    "with ProgressBar():\n",
    "    spi_yearly.to_netcdf(rf\"{DATA_PROC}\\ERA5_yearly_1970-2021_SPI-SPEI.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_yearly = xr.open_dataset(rf\"{DATA_PROC}\\ERA5_yearly_1970-2021_SPI-SPEI.nc\", chunks={\"x\": 900, \"y\": 1800})\n",
    "\n",
    "spi_spei_vars = [var for var in spi_yearly.data_vars if \"-\" in var]\n",
    "for var in spi_spei_vars:\n",
    "    for threshold_str in [\"1_0\", \"1_5\", \"2_0\", \"2_5\"]:\n",
    "        threshold = float(threshold_str.replace(\"_\", \".\"))\n",
    "        threshold_str = threshold_str.replace(\"_\", \"\")\n",
    "        spi_yearly[f\"drought_{var}_{threshold_str}sd\"] = (spi_yearly[var] < -threshold).astype(\"bool\")\n",
    "\n",
    "spi_yearly = spi_yearly[[var for var in spi_yearly.data_vars if \"drought\" in var]]\n",
    "spi_yearly = spi_yearly.rename({\n",
    "    var: var.replace(\"drought_\", \"\").replace(\"-\", \"\") for var in spi_yearly.data_vars\n",
    "})\n",
    "with ProgressBar():\n",
    "    spi_yearly.drop_duplicates(dim=\"x\").to_netcdf(droughts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validación de IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_shp = gpd.read_feather(rf\"{DATA_PROC}\\IPUMS_country_IDs.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_shp = (\n",
    "    id_IPUMS_shp\n",
    "        .rename(columns={\"CNTRY_CODE\":\"country\", \"GEOLEVEL1\":\"geolev1\", \"GEOLEVEL2\": \"geolev2\"})\n",
    "        .drop_duplicates(subset=[\"country\", \"geolev1\", \"geolev2\"]))\n",
    "id_IPUMS_shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\IPUMS_ids\"\n",
    "\n",
    "continentes = os.listdir(path)\n",
    "dfs = []\n",
    "for continente in continentes:\n",
    "    dfs += [pd.read_stata(os.path.join(path, continente), convert_categoricals=False)]\n",
    "\n",
    "id_IPUMS_data = pd.concat(dfs)\n",
    "id_IPUMS_data[id_IPUMS_data.duplicated(subset=\"geolev2\", keep=False)].dropna()\n",
    "# id_IPUMS_data = id_IPUMS_data.fillna(0).drop_duplicates()\n",
    "\n",
    "\n",
    "# id_IPUMS_data.loc[id_IPUMS_data[\"geolev2\"]==0, \"geolev2\"] = id_IPUMS_data.loc[id_IPUMS_data[\"geolev2\"]==0, \"geolev1\"]\n",
    "# id_IPUMS_data = id_IPUMS_data.fillna(0).drop_duplicates()\n",
    "\n",
    "# id_IPUMS_data.loc[id_IPUMS_data[\"geolev2\"]!=0, \"geolev1\"] = id_IPUMS_data.loc[id_IPUMS_data[\"geolev2\"]!=0, \"geolev2\"].astype(str).str.zfill(9).str[:6].astype(float) \n",
    "# id_IPUMS_data = id_IPUMS_data.fillna(0).drop_duplicates()\n",
    "\n",
    "# id_IPUMS_data = id_IPUMS_data[id_IPUMS_data.geolev1 != 0] # Drop unavailable data\n",
    "# id_IPUMS_data = id_IPUMS_data[id_IPUMS_data.geolev2 != 888888888] # Drop unavailable data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gpd.read_file(r\"C:\\Users\\ofici\\Downloads\\geo2_mu1990_2011\\geo2_mu1990_2011.shp\")\n",
    "test[test[\"GEOLEVEL2\"]==\"480014002\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_data[id_IPUMS_data.duplicated(subset=\"geolev2\", keep=False)].dropna()#.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_shp.loc[id_IPUMS_shp[\"geolev2\"]==0, \"geolev2\"] = id_IPUMS_shp.loc[id_IPUMS_shp[\"geolev2\"]==0, \"geolev1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = id_IPUMS_shp.merge(id_IPUMS_data, on=[\"country\", \"geolev2\"], how=\"outer\", indicator=True)\n",
    "m._merge.value_counts()\n",
    "# m[m._merge == \"both\"].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errores = m[m._merge!=\"both\"]\n",
    "pd.crosstab(errores.country, errores._merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errores[errores.country==170].sort_values([\"geolev1\", \"geolev2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errores[errores.geolev2==170005002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huracanes, con resolución completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXED Parameters\n",
    "sshws_min_wind = {\n",
    "    # Saffir-Simpson Hurricane Wind Scale\n",
    "    # Measured in knots\n",
    "    # https://www.nhc.noaa.gov/aboutsshws.php\n",
    "    5: 137,\n",
    "    4: 113,\n",
    "    3: 96,\n",
    "    2: 83,\n",
    "    1: 64,\n",
    "}\n",
    "\n",
    "agency_measurements = {\n",
    "    \"USA\": 1, # 1-m measurement\n",
    "    \"TOK\": 3, # 3-m measurement\n",
    "    \"CMA\": 2, # 2-m measurement\n",
    "    \"HKO\": 10, # 10-m measurement\n",
    "    \"KMA\": 10,\n",
    "    \"NEW\": 3,\n",
    "    \"REU\": 10,\n",
    "    \"BOM\": 10,\n",
    "    \"NAD\": 10,\n",
    "    \"WEL\": 10,\n",
    "    \"DS8\": 1,\n",
    "    \"TD6\": 1,\n",
    "    \"TD5\": 1,\n",
    "    \"NEU\": 1,\n",
    "    \"MLC\": 1,\n",
    "}\n",
    "\n",
    "conversion_factor_to_1m = {\n",
    "    1: 1,\n",
    "    2: (1.22/1.15+1.17/1.11)/2,\n",
    "    3: (1.22/1.12+1.17/1.09)/2,\n",
    "    10: (1.22/1.06+1.17/1.05)/2,\n",
    "}\n",
    "\n",
    "agency_1m_conversion_factor = {\n",
    "    k:conversion_factor_to_1m[v] for k, v in agency_measurements.items()\n",
    "}\n",
    "\n",
    "## Functions\n",
    "def convert_wind_to_1m(wind, agency):\n",
    "    if agency in agency_1m_conversion_factor:\n",
    "        return wind * agency_1m_conversion_factor[agency]\n",
    "    return wind\n",
    "\n",
    "def convert_wind_to_sshws(wind):\n",
    "    for cat, min_wind in sshws_min_wind.items():\n",
    "        if wind >= min_wind:\n",
    "            return cat\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file(r\"D:\\Datasets\\International Best Track Archive for Climate Stewardship (IBTrACS)\\IBTrACS.ALL.list.v04r01.lines.shp\")#, \"BASIN\", \"SUBBASIN\", \"NAME\", \"ISO_TIME\", \"LAT\", \"LON\", \"WMO_WIND\", \"WMO_PRES\", \"WMO_AGENCY\", \"TRACK_TYPE\"])\n",
    "\n",
    "# Fill interpolated xy values\n",
    "gdf[\"WMO_WIND\"] = gdf[\"WMO_WIND\"].ffill()\n",
    "\n",
    "# Data from 1950 onwards\n",
    "gdf[\"year\"] = gdf[\"ISO_TIME\"].str.split(\"-\").str[0].astype(int)\n",
    "gdf = gdf[gdf[\"year\"] >= 1970]\n",
    "\n",
    "# Convert each Agency wind to 1m-MSW\n",
    "wind_cols = [col for col in gdf.columns if \"_WIND\" in col and \"WMO_WIND\" not in col]\n",
    "agencies = [col.replace(\"_WIND\", \"\") for col in wind_cols]\n",
    "\n",
    "for col in tqdm(wind_cols):\n",
    "    agency = col.split(\"_\")[0]\n",
    "    gdf[col] = gdf[col].apply(lambda x: convert_wind_to_1m(x, agency))\n",
    "\n",
    "gdf[\"wind_speed\"] = gdf[wind_cols].max(axis=1)\n",
    "\n",
    "# Convert wind to SSHWS category\n",
    "gdf[\"category\"] = gdf[\"wind_speed\"].apply(convert_wind_to_sshws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "buffers = [0.1, 0.25, 0.50, 1.00]\n",
    "years = gdf[\"year\"].unique()\n",
    "\n",
    "paths_by_year = {}\n",
    "for year in tqdm(years, leave=False):\n",
    "    paths_by_year[year] = []\n",
    "    for buffer in buffers:\n",
    "        ### Filter year and create raster map based on the buffered best-track \n",
    "        ###     of the hurricane\n",
    "        print(buffer)\n",
    "        gdf_year = gdf[gdf.year == year]\n",
    "        gdf_year = gdf_year[[\"wind_speed\", \"geometry\"]].fillna(0)\n",
    "\n",
    "        # Apply buffer to center of the storm\n",
    "        gdf_year[\"geometry\"] = gdf_year.geometry.buffer(buffer)\n",
    "        \n",
    "        # Make the biggest shock at a certain location the one shown in the xr.dataset \n",
    "        gdf_year = gdf_year.sort_values(\"wind_speed\", ascending=True) \n",
    "        \n",
    "        raster = make_geocube(\n",
    "            vector_data=gdf_year,\n",
    "            like=population,\n",
    "        )\n",
    "        raster = raster.assign_coords({\"year\": year})\n",
    "\n",
    "        for category in [3, 4, 5]:\n",
    "            ### Once the raster wind_speed is created, create a new boolean raster \n",
    "            ###     where the winds are greater than the minimum for the category\n",
    "            varname = f\"category_{category}_b{int(buffer*100)}\"\n",
    "            # Keep only hurricanes of a certain category\n",
    "            raster_b = xr.where(\n",
    "                raster.rename({\"wind_speed\":varname})[varname] >= sshws_min_wind[category], \n",
    "                True, \n",
    "                False\n",
    "            )\n",
    "            # Transform wind_speed to boolean\n",
    "            raster_path = rf\"{DATA_PROC}\\shocks_by_grid\\hurricanes_{year}_{varname}.nc\"\n",
    "            raster_b.to_netcdf(raster_path, encoding={varname:{\"zlib\": True, \"complevel\": 7}})\n",
    "            paths_by_year[year] += [raster_path]\n",
    "            break\n",
    "        # xr.concat(dss, dim=\"year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(memory_limit='7GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset(files_year[0])[\"category_3_b10\"].encoding[\"chunksizes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all the data into a single dataset\n",
    "path = rf\"{DATA_PROC}\\shocks_by_grid\"\n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if \"hurricanes_\" in f and f.endswith(\".nc\")]\n",
    "\n",
    "dss = []\n",
    "for year in range(1970, 2021):\n",
    "    \n",
    "    files_year = [os.path.join(path, f) for f in files if f\"{year}\" in f]\n",
    "    ds = xr.open_mfdataset(files_year,  parallel=True, chunks=\"auto\")\n",
    "    ds = ds.assign_coords({\"year\": year})\n",
    "    dss += [ds]\n",
    "    \n",
    "ds = xr.concat(dss, dim=\"year\")\n",
    "ds.to_netcdf(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\IBTrACS_hurricanes_yearly.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\IBTrACS_hurricanes_yearly.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizer\n",
    "year = 2020\n",
    "hurr_by_name = gdf[(gdf.NAME == \"BELNA\") & (gdf.year == year)]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "hurr_by_name.plot(column=\"category\", legend=True, ax=ax)\n",
    "\n",
    "xmin, ymin, xmax, ymax = hurr_by_name.total_bounds\n",
    "hurr_proc.sel(year=year, x=slice(xmin, xmax), y=slice(ymax, ymin))[\"category_1_b10\"].plot(ax=ax, cmap=\"Greys\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatwave y Coldwaves, resolución original (ERA5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shocks = {\n",
    "    \"timeseries-fd-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\": {\n",
    "        \"name\": \"frostdays\",\n",
    "        \"variable\": \"timeseries-fd-monthly-mean\",\n",
    "        \"poslat\": [3,4,5,6,7,8,9,10,11], \n",
    "        \"neglat\": [1,2,3,4,5,9,10,11,12],\n",
    "        \"spell_index\": \"csdi\",\n",
    "    },\n",
    "    \"timeseries-id-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\": {\n",
    "        \"name\": \"icedays\",\n",
    "        \"variable\": \"timeseries-id-monthly-mean\",\n",
    "        \"poslat\": [3,4,5,6,7,8,9,10,11], \n",
    "        \"neglat\": [1,2,3,4,5,9,10,11,12],\n",
    "        \"spell_index\": \"csdi\",\n",
    "    },\n",
    "    \"timeseries-hd35-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\": {\n",
    "        \"name\": \"heatdays35\",\n",
    "        \"variable\": \"timeseries-hd35-monthly-mean\",\n",
    "        \"poslat\": [1,2,3,4,5,9,10,11,12], \n",
    "        \"neglat\": [3,4,5,6,7,8,9,10,11],\n",
    "        \"spell_index\": \"wsdi\",\n",
    "    },\n",
    "    \"timeseries-hd40-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\": {\n",
    "        \"name\": \"heatdays40\",\n",
    "        \"variable\": \"timeseries-hd40-monthly-mean\",\n",
    "        \"poslat\": [1,2,3,4,5,9,10,11,12], \n",
    "        \"neglat\": [3,4,5,6,7,8,9,10,11],\n",
    "        \"spell_index\": \"wsdi\",\n",
    "    }\n",
    "}\n",
    "\n",
    "tasks = []\n",
    "for file, params in shocks.items():\n",
    "    \n",
    "    ds = xr.open_dataset(rf\"{DATA_RAW}\\ERA5_CCKP\\{file}\", chunks=\"auto\")\n",
    "    name = params[\"name\"]\n",
    "    var = params[\"variable\"]\n",
    "\n",
    "    ds = ds.sel(bnds=0) # both bands are the same\n",
    "    ds = ds.drop_vars([\"lon_bnds\", \"lat_bnds\", \"bnds\"]) # drop last timestep\n",
    "    ds[var] =  (ds[var] / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "        \n",
    "    # 1) heatwaves/coldwaves are rare events, n° days > 2/2.5 std\n",
    "    \n",
    "    stand_anomalies = xr.apply_ufunc(\n",
    "        lambda x, m, s: (x - m) / s,\n",
    "        ds.groupby(\"time.month\"),\n",
    "        ds.groupby(\"time.month\").mean(\"time\"),\n",
    "        ds.groupby(\"time.month\").std(\"time\"),\n",
    "        dask=\"parallelized\",\n",
    "    )\n",
    "    \n",
    "    tasks += [stand_anomalies.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_standardized.nc\", compute=False)]\n",
    "    \n",
    "    ds_more_than_2_std = (stand_anomalies > 2)\n",
    "    ds_more_than_25_std = (stand_anomalies > 2.5)\n",
    "    \n",
    "    tasks += [ds_more_than_2_std.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_2_std.nc\", compute=False)]\n",
    "    tasks += [ds_more_than_25_std.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_25_std.nc\", compute=False)]\n",
    " \n",
    "    # 2)  Heatwaves/Coldwaves only occur in summer/winter months:\n",
    "    ds[var] = xr.where(\n",
    "        ds.time.dt.month.isin(params[\"poslat\"]) & (ds.lat > 0), \n",
    "        0,\n",
    "        ds[var], \n",
    "    )\n",
    "    ds[var] = xr.where(\n",
    "        ds.time.dt.month.isin(params[\"neglat\"]) & (ds.lat < 0), \n",
    "        0,\n",
    "        ds[var], \n",
    "    )\n",
    "        \n",
    "    # 3) More than 6 hotdays/coldays\n",
    "    more_than_6_days = (ds[var] >= 6).astype(bool)\n",
    "    tasks += [more_than_6_days.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_6.nc\", compute=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute yearly values\n",
    "tasks = []\n",
    "for file, params in shocks.items():\n",
    "\n",
    "    name = params[\"name\"]\n",
    "    var = params[\"variable\"]\n",
    "\n",
    "    # (1) heatwaves/coldwaves are rare events, n° days > 2/2.5 std\n",
    "    ds_more_than_2_std = xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_2_std.nc\", chunks={\"time\":60})\n",
    "    ds_more_than_25_std = xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_25_std.nc\", chunks={\"time\":60})\n",
    "\n",
    "    # (2+3)  Heatwaves/Coldwaves only occur in summer/winter months:\n",
    "    more_than_6_days = xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_6.nc\", chunks={\"time\":60})\n",
    "\n",
    "    # (1+2+3) all together\n",
    "    ds_2_std = (more_than_6_days[var] * ds_more_than_2_std[var])\n",
    "    ds_25_std = (more_than_6_days[var] * ds_more_than_25_std[var])\n",
    "\n",
    "    # Annual values\n",
    "    ds_2_std = ds_2_std.groupby('time.year').max()\n",
    "    ds_25_std = ds_25_std.groupby('time.year').max()\n",
    "\n",
    "    # WSDI/CSDI (Warm/Cold Spell Duration Index) \n",
    "    spell_index = params[\"spell_index\"]\n",
    "    sdi = xr.open_dataset(rf\"{DATA_RAW}\\ERA5_CCKP\\timeseries-{spell_index}-annual-mean_era_annual_era5-0.5x0.5-timeseries_mean_1950-2020.nc\", chunks={\"year\":5})\n",
    "    sdi = sdi.sel(bnds=0) # both bands are the same\n",
    "    sdi = sdi.drop_vars([\"lon_bnds\", \"lat_bnds\", \"bnds\"]) # drop last timestep\n",
    "    sdi[f\"timeseries-{spell_index}-annual-mean\"] =  (sdi[f\"timeseries-{spell_index}-annual-mean\"] / np.timedelta64(1, 'D')).astype(int)\n",
    "    sdi = (sdi[f\"timeseries-{spell_index}-annual-mean\"] >= 6)\n",
    "    \n",
    "    # Final computation\n",
    "    waves_2_std = (ds_2_std * sdi)\n",
    "    waves_25_std = (ds_25_std * sdi)\n",
    "\n",
    "    tasks += [waves_2_std.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_annual_2_std.nc\", compute=False)]\n",
    "    tasks += [waves_25_std.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_annual_25_std.nc\", compute=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tasks))\n",
    "dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hws = {\n",
    "    \"hw3520\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\heatdays35_annual_2_std.nc\"), \n",
    "    \"hw3525\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\heatdays35_annual_25_std.nc\"), \n",
    "    \"hw4020\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\heatdays40_annual_2_std.nc\"), \n",
    "    \"hw4025\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\heatdays40_annual_25_std.nc\"),\n",
    "}\n",
    "\n",
    "for name, ds in hws.items():\n",
    "    ds = ds.rename({\"__xarray_dataarray_variable__\":name})\n",
    "    hws[name] = ds\n",
    "    \n",
    "ds = xr.combine_by_coords(list(hws.values()))\n",
    "ds = ds.rename({\"lat\":\"y\", \"lon\":\"x\"})\n",
    "ds = ds.sortby(ds.x).sortby(ds.y, ascending=False)\n",
    "ds.to_netcdf(rf\"{DATA_OUT}\\CCKP_heatwaves_yearly.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws = {\n",
    "    \"fd20\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\frostdays_annual_2_std.nc\"), \n",
    "    \"fd25\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\frostdays_annual_25_std.nc\"), \n",
    "    \"id20\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\icedays_annual_2_std.nc\"), \n",
    "    \"id25\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\icedays_annual_25_std.nc\"),\n",
    "}\n",
    "\n",
    "for name, ds in cws.items():\n",
    "    ds = ds.rename({\"__xarray_dataarray_variable__\":name})\n",
    "    cws[name] = ds\n",
    "    \n",
    "ds = xr.combine_by_coords(list(cws.values()))\n",
    "ds = ds.rename({\"lat\":\"y\", \"lon\":\"x\"})\n",
    "ds = ds.sortby(ds.x).sortby(ds.y, ascending=False)\n",
    "ds.to_netcdf(rf\"{DATA_OUT}\\CCKP_coldwaves_yearly.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intense Rain, resolucion original (ERA5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shocks_by_year = []\n",
    "for shock in [\"rx1day\", \"rx5day\"]:\n",
    "    file = f\"timeseries-{shock}-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\"\n",
    "    ds = xr.open_dataset(rf\"{DATA_RAW}\\ERA5_CCKP\\{file}\", chunks={\"time\": 60})\n",
    "    ds = ds.sel(bnds=0) # both bands are the same\n",
    "    ds = ds.drop_vars([\"lon_bnds\", \"lat_bnds\", \"bnds\"])\n",
    "    for rainfall in [100, 200, 300, 400, 500, 600]:\n",
    "        ds_shock = (ds[f\"timeseries-{shock}-monthly-mean\"] >= rainfall)\n",
    "        ds_shock = ds_shock.rename(f\"{shock}_{rainfall}\")\n",
    "        ds_year = ds_shock.groupby('time.year').max()\n",
    "        shocks_by_year += [ds_year]\n",
    "        \n",
    "full_ds = xr.combine_by_coords(shocks_by_year)\n",
    "full_ds = full_ds.rename({\"lat\":\"y\", \"lon\":\"x\"})\n",
    "full_ds = full_ds.sortby(full_ds.x).sortby(full_ds.y, ascending=False)\n",
    "full_ds.to_netcdf(rf\"{DATA_OUT}\\CCKP_intenserain_yearly.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
