{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "import dask\n",
    "import xarray as xr\n",
    "import xrspatial\n",
    "from dask.diagnostics import ProgressBar\n",
    "from geocube.api.core import make_geocube\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PATH = \"D:\\World Bank\\CLIENT v2\"\n",
    "DATA_RAW = rf\"{PATH}\\Data\\Data_raw\"\n",
    "DATA_PROC = rf\"{PATH}\\Data\\Data_proc\"\n",
    "DATA_OUT = rf\"{PATH}\\Data\\Data_out\"\n",
    "GPW_PATH = rf\"D:\\Datasets\\Gridded Population of the World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floods = pd.read_csv(rf\"{DATA_RAW}\\Floods\\GloFAS_floods.csv\")\n",
    "\n",
    "def load_population_data(bounds=None, generate=False):\n",
    "    print(\"Processing Population data...\")\n",
    "\n",
    "    # Select all files in GPW folder\n",
    "    files = os.listdir(GPW_PATH)\n",
    "    files = [f for f in files if f.endswith(\".tif\")]\n",
    "    \n",
    "    # Compile into a single dataset\n",
    "    dss = []\n",
    "    for f in tqdm(files):\n",
    "        \n",
    "        ds = xr.open_dataset(os.path.join(GPW_PATH, f), chunks={\"x\": 1000, \"y\": 1000})\n",
    "        ds[\"band_data\"] = ds[\"band_data\"].astype(np.uint32)\n",
    "        if bounds is not None:\n",
    "            ds = ds.sel(\n",
    "                x=slice(bounds[0], bounds[2]), y=slice(bounds[3], bounds[1])\n",
    "            )\n",
    "        if generate:\n",
    "            with ProgressBar():\n",
    "                ds.sel(band=1).drop_vars(\"band\").band_data.rio.to_raster(rf\"{DATA_PROC}\\{f.replace('.tif','_proc.tif')}\")\n",
    "                print(f\"Saved {f.replace('.tif','_proc.tif')}\")\n",
    "        \n",
    "        ds[\"year\"] = int(f.split(\"_\")[5])\n",
    "        ds = ds.set_coords('year')\n",
    "        dss += [ds]\n",
    "        \n",
    "    population = xr.concat(dss, dim=\"year\")    \n",
    "    \n",
    "    # Filter if bounds are provided\n",
    "    if bounds is not None:\n",
    "        population = population.sel(\n",
    "            x=slice(bounds[0], bounds[2]), y=slice(bounds[3], bounds[1])\n",
    "        )\n",
    "        \n",
    "    # Clean band dimension\n",
    "    population = population.sel(band=1).drop_vars([\"band\"])\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return population\n",
    "\n",
    "def load_precipitation_data():\n",
    "    era5 = xr.open_dataset(\n",
    "        rf\"{DATA_OUT}\\ERA5_monthly_1970-2021_SPI-SPEI.nc\",\n",
    "        chunks={\"latitude\": 100, \"longitude\": 100},\n",
    "    )\n",
    "    era5 = era5.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "    return\n",
    "\n",
    "def load_IPUMS_country_data(wb_map):\n",
    "    from osgeo import gdal\n",
    "    import geopandas as gpd\n",
    "    from IPython.display import display\n",
    "    gdal.SetConfigOption('SHAPE_RESTORE_SHX', 'YES')\n",
    "\n",
    "    print(\"Loading IPUMS country data...\")\n",
    "\n",
    "    path = rf\"{DATA_RAW}\\IPUMS Fixed\"\n",
    "    files = os.listdir(path)\n",
    "    shpfiles = [f for f in files if \".shp\" in f]\n",
    "    geo2_files = [f for f in shpfiles if \"geo2_\" in f]\n",
    "    geo2_countries = [name.split(\"_\")[1][:2] for name in geo2_files]\n",
    "    geo1_files = [f for f in shpfiles if \"geo1_\" in f]\n",
    "    geo1_files = [f for f in geo1_files if f.split(\"_\")[1][:2] not in geo2_countries]\n",
    "    # return geo1_files, geo2_files\n",
    "    files = geo2_files + geo1_files\n",
    "\n",
    "    gdfs = []\n",
    "    for f in tqdm(files):\n",
    "        gdf = gpd.read_file(os.path.join(path, f))\n",
    "        ctry_name  = gdf.CNTRY_NAME[0]\n",
    "        \n",
    "        # MANUAL FIXES\n",
    "        if ctry_name == \"United States\":\n",
    "            print(\"Removing Puerto Rico from US...\")\n",
    "            gdf = gdf[~gdf.GEOLEVEL2.isin([\"840721079\", \"840721080\", \"840721081\", \"840721082\", \"840721083\", \"840721084\", \"840721085\"])]\n",
    "        if ctry_name == \"Nigeria\":\n",
    "            print(\"Removing glitch from Nigeria...\")\n",
    "            gdf = gdf[gdf.GEOLEVEL2 != \"566024008\"]\n",
    "        if ctry_name == \"Israel\":\n",
    "            print(\"Normalizing Israel map to match WB...\")\n",
    "            # FIXME: there should be a better way to write this...\n",
    "            fix = pd.read_csv(rf\"{DATA_PROC}\\fixes\\fix_israel_geo2_adm1.csv\", dtype={\"GEOLEVEL2\": str, \"ADM1_CODE\": int})\n",
    "            gdf = gdf.merge(fix, on=\"GEOLEVEL2\").drop(columns=\"geometry\").drop_duplicates(subset=\"ADM1_CODE\")\n",
    "            gdf = wb_map[[\"geometry\", \"ADM1_CODE\"]].merge(gdf, on=\"ADM1_CODE\")\n",
    "            gdf[\"GEOLEVEL2\"] = gdf[\"ADM1_CODE\"]\n",
    "            gdf = gdf.drop(columns=\"ADM1_CODE\")\n",
    "        if ctry_name == \"Palestine\":\n",
    "            print(\"Normalizing Palestine map to match WB...\")\n",
    "            fix = pd.read_csv(rf\"{DATA_PROC}\\fixes\\fix_palestine_geo1_adm1.csv\", dtype={\"GEOLEVEL1\": str, \"ADM1_CODE\": int})\n",
    "            gdf = gdf.merge(fix, on=\"GEOLEVEL1\").drop(columns=\"geometry\").drop_duplicates(subset=\"ADM1_CODE\")\n",
    "            gdf = wb_map[[\"geometry\", \"ADM1_CODE\"]].merge(gdf, on=\"ADM1_CODE\")\n",
    "            gdf[\"GEOLEVEL1\"] = gdf[\"ADM1_CODE\"]\n",
    "            gdf = gdf.drop(columns=\"ADM1_CODE\")\n",
    "\n",
    "        gdfs += [gdf]\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(pd.concat(gdfs))\n",
    "    gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    gdf.loc[gdf[\"GEOLEVEL1\"].isna(), \"GEOLEVEL1\"] = gdf.loc[gdf[\"GEOLEVEL1\"].isna(), \"GEOLEVEL2\"].str[:6] \n",
    "    id_cols = [\"CNTRY_CODE\", \"GEOLEVEL1\", \"GEOLEVEL2\"] \n",
    "    gdf = gdf[[\"geometry\"] + id_cols]\n",
    "    \n",
    "    for col in id_cols:\n",
    "        gdf[col] = pd.to_numeric(gdf[col]).fillna(0)\n",
    "    gdf = gdf[gdf.GEOLEVEL1 != 0] # Drop unavailable data\n",
    "    gdf = gdf[gdf.GEOLEVEL2 != 888888888] # Drop unavailable data\n",
    "    assert gdf.duplicated(subset=id_cols).sum() == 0, \"There are duplicated rows in the data!\"\n",
    "    \n",
    "    # Create ID\n",
    "    gdf[\"ID\"] = gdf.groupby(id_cols).ngroup()\n",
    "    assert gdf.ID.nunique() == gdf.shape[0], \"ID is not unique!, there's some bug in the code...\"\n",
    "    print(\"Data loaded!\")\n",
    "    return gdf\n",
    "\n",
    "def load_WB_country_data():\n",
    "    print(\"Loading World Bank country data...\")\n",
    "    WB_country = gpd.read_file(rf\"{DATA_RAW}\\world_bank_adm2\\world_bank_adm2.shp\")\n",
    "    \n",
    "    # Assign nan when ADM2 is not available \n",
    "    WB_country.loc[WB_country.ADM2_NAME == \"Administrative unit not available\", \"ADM2_CODE\"] = (\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Create ADM_LAST variable: ADM2_NAME if available, else ADM1_NAME\n",
    "    for col in [\"ADM0_CODE\", \"ADM1_CODE\", \"ADM2_CODE\"]:\n",
    "        WB_country[col] = WB_country[col].fillna(0)\n",
    "\n",
    "    # Dissolve by ADM_LAST and country code    \n",
    "    WB_country = WB_country.dissolve(by=[\"ADM2_CODE\",\"ADM1_CODE\", \"ADM0_CODE\"]).reset_index()\n",
    "    \n",
    "    # # Create ID\n",
    "    WB_country[\"ID\"] = WB_country.groupby([\"ADM2_CODE\", \"ADM1_CODE\", \"ADM0_CODE\"]).ngroup()\n",
    "    assert WB_country.ID.nunique() == WB_country.shape[0], \"ID is not unique!, there's some bug in the code...\"\n",
    "    print(\"Data loaded!\")\n",
    "    return WB_country\n",
    "\n",
    "\n",
    "\n",
    "def rasterize_shape_like_dataset(shape, dataset):\n",
    "    print(\"Rasterizing shape...\")\n",
    "    raster = make_geocube(\n",
    "        vector_data=shape,\n",
    "        like=dataset,\n",
    "    )\n",
    "    # For some reason, like option is not working, so I have to manually add x and y\n",
    "    assert (raster[\"x\"].shape == dataset[\"x\"].shape)\n",
    "    assert (raster[\"y\"].shape == dataset[\"y\"].shape)\n",
    "    raster[\"x\"] = dataset[\"x\"]\n",
    "    raster[\"y\"] = dataset[\"y\"]\n",
    "    raster = raster.drop_vars([\"spatial_ref\"])\n",
    "    print(\"Done!\")\n",
    "    return raster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesa IPUMS shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesa WB/IPUMS shapes (administrative boundaries) y GPW (population data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_country = load_WB_country_data()\n",
    "IPUMS_country = load_IPUMS_country_data(WB_country)\n",
    "IPUMS_country = IPUMS_country.clip(WB_country.total_bounds)\n",
    "population = load_population_data(bounds=WB_country.total_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rasterize WB_country\n",
    "WB_country_grid = rasterize_shape_like_dataset(\n",
    "    WB_country[[\"ID\", \"geometry\"]], \n",
    "    population\n",
    ")\n",
    "\n",
    "print(\"Saving WB_country_grid...\")P\n",
    "WB_country_grid.to_netcdf(rf\"{DATA_PROC}\\WB_country_grid.nc\")\n",
    "        \n",
    "WB_country[[\"ID\", \"OBJECTID\", \"ADM2_CODE\", \"ADM1_CODE\", \"ADM0_CODE\",  \"geometry\"]].to_feather(rf\"{DATA_PROC}\\WB_country_IDs.feather\")\n",
    "\n",
    "\n",
    "### Rasterize IPUMS_country\n",
    "IPUMS_country_grid = rasterize_shape_like_dataset(\n",
    "    IPUMS_country[[\"ID\", \"geometry\"]], \n",
    "    population\n",
    ")\n",
    "\n",
    "IPUMS_country_path = rf\"{DATA_PROC}\\IPUMS_country_grid.nc\"\n",
    "print(\"Saving IPUMS_country_grid...\")\n",
    "IPUMS_country_grid.to_netcdf(rf\"{DATA_PROC}\\IPUMS_country_grid.nc\")\n",
    "        \n",
    "IPUMS_country.to_feather(rf\"{DATA_PROC}\\IPUMS_country_IDs.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERA Base de droughts, con resolución original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "droughts_path = rf\"{DATA_OUT}\\ERA5_droughts_yearly.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(droughts_path):\n",
    "print(\"Preparing droughts dataset...\")\n",
    "# Genera base de sequías\n",
    "era5 = xr.open_dataset(rf\"{DATA_OUT}\\ERA5_monthly_1970-2021_SPI-SPEI.nc\", chunks={'latitude': 1000, 'longitude': 1000})\n",
    "# Corrije la dimensión x, que va de 0 a 360\n",
    "era5 = era5.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "era5 = utils.coordinates_from_0_360_to_180_180(era5) # FIXME: no se si esto está andando bien, pero creo que si. VERIFICAR\n",
    "\n",
    "# Calcula las sequías anuales\n",
    "spi_yearly = era5.groupby(\"time.year\").min()\n",
    "with ProgressBar():\n",
    "    spi_yearly.to_netcdf(rf\"{DATA_PROC}\\ERA5_yearly_1970-2021_SPI-SPEI.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_yearly = xr.open_dataset(rf\"{DATA_PROC}\\ERA5_yearly_1970-2021_SPI-SPEI.nc\", chunks={\"x\": 900, \"y\": 1800})\n",
    "\n",
    "spi_spei_vars = [var for var in spi_yearly.data_vars if \"-\" in var]\n",
    "for var in spi_spei_vars:\n",
    "    for threshold_str in [\"1_0\", \"1_5\", \"2_0\", \"2_5\"]:\n",
    "        threshold = float(threshold_str.replace(\"_\", \".\"))\n",
    "        threshold_str = threshold_str.replace(\"_\", \"\")\n",
    "        spi_yearly[f\"drought_{var}_{threshold_str}sd\"] = (spi_yearly[var] < -threshold).astype(\"bool\")\n",
    "\n",
    "spi_yearly = spi_yearly[[var for var in spi_yearly.data_vars if \"drought\" in var]]\n",
    "spi_yearly = spi_yearly.rename({\n",
    "    var: var.replace(\"drought_\", \"\").replace(\"-\", \"\") for var in spi_yearly.data_vars\n",
    "})\n",
    "with ProgressBar():\n",
    "    spi_yearly.to_netcdf(droughts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERA Base de Huracanes, con resolución completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXED Parameters\n",
    "sshws_min_wind = {\n",
    "    # Saffir-Simpson Hurricane Wind Scale\n",
    "    # Measured in knots\n",
    "    # https://www.nhc.noaa.gov/aboutsshws.php\n",
    "    5: 137,\n",
    "    4: 113,\n",
    "    3: 96,\n",
    "    2: 83,\n",
    "    1: 64,\n",
    "}\n",
    "\n",
    "agency_measurements = {\n",
    "    \"USA\": 1, # 1-m measurement\n",
    "    \"TOK\": 3, # 3-m measurement\n",
    "    \"CMA\": 2, # 2-m measurement\n",
    "    \"HKO\": 10, # 10-m measurement\n",
    "    \"KMA\": 10,\n",
    "    \"NEW\": 3,\n",
    "    \"REU\": 10,\n",
    "    \"BOM\": 10,\n",
    "    \"NAD\": 10,\n",
    "    \"WEL\": 10,\n",
    "    \"DS8\": 1,\n",
    "    \"TD6\": 1,\n",
    "    \"TD5\": 1,\n",
    "    \"NEU\": 1,\n",
    "    \"MLC\": 1,\n",
    "}\n",
    "\n",
    "conversion_factor_to_1m = {\n",
    "    1: 1,\n",
    "    2: (1.22/1.15+1.17/1.11)/2,\n",
    "    3: (1.22/1.12+1.17/1.09)/2,\n",
    "    10: (1.22/1.06+1.17/1.05)/2,\n",
    "}\n",
    "\n",
    "agency_1m_conversion_factor = {\n",
    "    k:conversion_factor_to_1m[v] for k, v in agency_measurements.items()\n",
    "}\n",
    "\n",
    "## Functions\n",
    "def convert_wind_to_1m(wind, agency):\n",
    "    if agency in agency_1m_conversion_factor:\n",
    "        return wind * agency_1m_conversion_factor[agency]\n",
    "    return wind\n",
    "\n",
    "def convert_wind_to_sshws(wind):\n",
    "    for cat, min_wind in sshws_min_wind.items():\n",
    "        if wind >= min_wind:\n",
    "            return cat\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file(r\"D:\\Datasets\\International Best Track Archive for Climate Stewardship (IBTrACS)\\IBTrACS.ALL.list.v04r01.lines.shp\")#, \"BASIN\", \"SUBBASIN\", \"NAME\", \"ISO_TIME\", \"LAT\", \"LON\", \"WMO_WIND\", \"WMO_PRES\", \"WMO_AGENCY\", \"TRACK_TYPE\"])\n",
    "\n",
    "# Fill interpolated xy values\n",
    "gdf[\"WMO_WIND\"] = gdf[\"WMO_WIND\"].ffill()\n",
    "\n",
    "# Data from 1950 onwards\n",
    "gdf[\"year\"] = gdf[\"ISO_TIME\"].str.split(\"-\").str[0].astype(int)\n",
    "gdf = gdf[gdf[\"year\"] >= 1970]\n",
    "\n",
    "# Convert each Agency wind to 1m-MSW\n",
    "wind_cols = [col for col in gdf.columns if \"_WIND\" in col and \"WMO_WIND\" not in col]\n",
    "agencies = [col.replace(\"_WIND\", \"\") for col in wind_cols]\n",
    "\n",
    "for col in tqdm(wind_cols):\n",
    "    agency = col.split(\"_\")[0]\n",
    "    gdf[col] = gdf[col].apply(lambda x: convert_wind_to_1m(x, agency))\n",
    "\n",
    "gdf[\"wind_speed\"] = gdf[wind_cols].max(axis=1)\n",
    "\n",
    "# Convert wind to SSHWS category\n",
    "gdf[\"category\"] = gdf[\"wind_speed\"].apply(convert_wind_to_sshws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "buffers = [0.1, 0.25, 0.50, 1.00]\n",
    "years = gdf[\"year\"].unique()\n",
    "\n",
    "paths_by_year = {}\n",
    "for year in tqdm(years, leave=False):\n",
    "    paths_by_year[year] = []\n",
    "    for buffer in buffers:\n",
    "        ### Filter year and create raster map based on the buffered best-track \n",
    "        ###     of the hurricane\n",
    "        print(buffer)\n",
    "        gdf_year = gdf[gdf.year == year]\n",
    "        gdf_year = gdf_year[[\"wind_speed\", \"geometry\"]].fillna(0)\n",
    "\n",
    "        # Apply buffer to center of the storm\n",
    "        gdf_year[\"geometry\"] = gdf_year.geometry.buffer(buffer)\n",
    "        \n",
    "        # Make the biggest shock at a certain location the one shown in the xr.dataset \n",
    "        gdf_year = gdf_year.sort_values(\"wind_speed\", ascending=True) \n",
    "        \n",
    "        raster = make_geocube(\n",
    "            vector_data=gdf_year,\n",
    "            like=population,\n",
    "        )\n",
    "        raster = raster.assign_coords({\"year\": year})\n",
    "\n",
    "        for category in [3, 4, 5]:\n",
    "            ### Once the raster wind_speed is created, create a new boolean raster \n",
    "            ###     where the winds are greater than the minimum for the category\n",
    "            varname = f\"category_{category}_b{int(buffer*100)}\"\n",
    "            # Keep only hurricanes of a certain category\n",
    "            raster_b = xr.where(\n",
    "                raster.rename({\"wind_speed\":varname})[varname] >= sshws_min_wind[category], \n",
    "                True, \n",
    "                False\n",
    "            )\n",
    "            # Transform wind_speed to boolean\n",
    "            raster_path = rf\"{DATA_PROC}\\shocks_by_grid\\hurricanes_{year}_{varname}.nc\"\n",
    "            raster_b.to_netcdf(raster_path, encoding={varname:{\"zlib\": True, \"complevel\": 7}})\n",
    "            paths_by_year[year] += [raster_path]\n",
    "            break\n",
    "        # xr.concat(dss, dim=\"year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(memory_limit='7GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset(files_year[0])[\"category_3_b10\"].encoding[\"chunksizes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all the data into a single dataset\n",
    "path = rf\"{DATA_PROC}\\shocks_by_grid\"\n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if \"hurricanes_\" in f and f.endswith(\".nc\")]\n",
    "\n",
    "dss = []\n",
    "for year in range(1970, 2021):\n",
    "    \n",
    "    files_year = [os.path.join(path, f) for f in files if f\"{year}\" in f]\n",
    "    ds = xr.open_mfdataset(files_year,  parallel=True, chunks=\"auto\")\n",
    "    ds = ds.assign_coords({\"year\": year})\n",
    "    dss += [ds]\n",
    "    \n",
    "ds = xr.concat(dss, dim=\"year\")\n",
    "ds.to_netcdf(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\IBTrACS_hurricanes_yearly.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\IBTrACS_hurricanes_yearly.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizer\n",
    "year = 2020\n",
    "hurr_by_name = gdf[(gdf.NAME == \"BELNA\") & (gdf.year == year)]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "hurr_by_name.plot(column=\"category\", legend=True, ax=ax)\n",
    "\n",
    "xmin, ymin, xmax, ymax = hurr_by_name.total_bounds\n",
    "hurr_proc.sel(year=year, x=slice(xmin, xmax), y=slice(ymax, ymin))[\"category_1_b10\"].plot(ax=ax, cmap=\"Greys\", alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
