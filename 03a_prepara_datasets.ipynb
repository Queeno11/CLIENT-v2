{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ESTO VA ANTES DE 02c!!!!"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "import dask\n",
    "import xarray as xr\n",
    "import xrspatial\n",
    "from dask.diagnostics import ProgressBar\n",
    "from geocube.api.core import make_geocube\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PATH = \"D:\\World Bank\\CLIENT v2\"\n",
    "DATA_RAW = rf\"{PATH}\\Data\\Data_raw\"\n",
    "DATA_PROC = rf\"{PATH}\\Data\\Data_proc\"\n",
    "DATA_OUT = rf\"{PATH}\\Data\\Data_out\"\n",
    "GPW_PATH = rf\"D:\\Datasets\\Gridded Population of the World\""
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 11,
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\World Bank\\\\CLIENT v2\\\\Data\\\\Data_raw'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_RAW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# floods = pd.read_csv(rf\"{DATA_RAW}\\Floods\\GloFAS_floods.csv\")\n",
    "\n",
    "def load_population_data(bounds=None, generate=False):\n",
    "    print(\"Processing Population data...\")\n",
    "\n",
    "    # Select all files in GPW folder\n",
    "    files = os.listdir(GPW_PATH)\n",
    "    files = [f for f in files if f.endswith(\".tif\")]\n",
    "    \n",
    "    # Compile into a single dataset\n",
    "    dss = []\n",
    "    for f in tqdm(files):\n",
    "        \n",
    "        ds = xr.open_dataset(os.path.join(GPW_PATH, f), chunks={\"x\": 10000, \"y\": 10000})\n",
    "        ds[\"band_data\"] = ds[\"band_data\"].astype(np.uint32)\n",
    "        if bounds is not None:\n",
    "            ds = ds.sel(\n",
    "                x=slice(bounds[0], bounds[2]), y=slice(bounds[3], bounds[1])\n",
    "            )\n",
    "        if generate:\n",
    "            with ProgressBar():\n",
    "                ds.sel(band=1).drop_vars(\"band\").band_data.rio.to_raster(rf\"{DATA_PROC}\\{f.replace('.tif','_proc.tif')}\")\n",
    "                print(f\"Saved {f.replace('.tif','_proc.tif')}\")\n",
    "        \n",
    "        ds[\"year\"] = int(f.split(\"_\")[5])\n",
    "        ds = ds.set_coords('year')\n",
    "        dss += [ds]\n",
    "        \n",
    "    population = xr.concat(dss, dim=\"year\")    \n",
    "    \n",
    "    # Filter if bounds are provided\n",
    "    if bounds is not None:\n",
    "        population = population.sel(\n",
    "            x=slice(bounds[0], bounds[2]), y=slice(bounds[3], bounds[1])\n",
    "        )\n",
    "        \n",
    "    # Clean band dimension\n",
    "    population = population.sel(band=1).drop_vars([\"band\"])\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return population\n",
    "\n",
    "def load_precipitation_data():\n",
    "    era5 = xr.open_dataset(\n",
    "        rf\"{DATA_OUT}\\ERA5_monthly_1970-2021_SPI-SPEI.nc\",\n",
    "        chunks={\"latitude\": 100, \"longitude\": 100},\n",
    "    )\n",
    "    era5 = era5.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "    return\n",
    "\n",
    "def load_WB_country_data(drop_adm2_na=False):\n",
    "    print(\"Loading World Bank country data...\")\n",
    "    WB_country = gpd.read_file(rf\"{DATA_RAW}\\world_bank_adm2.zip\")\n",
    "    \n",
    "    # Assign nan when ADM2 is not available \n",
    "    WB_country.loc[WB_country.ADM2_NAME == \"Administrative unit not available\", \"ADM2_CODE\"] = (\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Create ADM_LAST variable: ADM2_NAME if available, else ADM1_NAME\n",
    "    WB_country[\"ADMLAST_CODE\"] = WB_country.ADM2_CODE\n",
    "    WB_country[\"ADMLAST_NAME\"] = WB_country.ADM2_NAME\n",
    "    WB_country.loc[WB_country.ADM2_CODE.isnull(), \"ADMLAST_CODE\"] = WB_country.ADM1_CODE\n",
    "    WB_country.loc[ WB_country.ADM2_CODE.isnull(), \"ADMLAST_NAME\"] = WB_country.ADM1_NAME\n",
    "\n",
    "    # Dissolve by ADM_LAST and country code\n",
    "    WB_country = WB_country.dissolve(by=[\"ADMLAST_CODE\", \"ADMLAST_NAME\", \"ADM0_CODE\", \"ADM0_NAME\"]).reset_index()\n",
    "    \n",
    "    # Create ID\n",
    "    WB_country[\"ID\"] = WB_country.groupby([\"ADMLAST_CODE\", \"ADMLAST_NAME\", \"ADM0_CODE\", \"ADM0_NAME\"]).ngroup()\n",
    "    assert WB_country.ID.nunique() == WB_country.shape[0], \"ID is not unique!, there's some bug in the code...\"\n",
    "    print(\"Data loaded!\")\n",
    "    return WB_country\n",
    "\n",
    "\n",
    "def rasterize_shape_like_dataset(shape, dataset):\n",
    "    print(\"Rasterizing shape...\")\n",
    "    raster = make_geocube(\n",
    "        vector_data=shape,\n",
    "        like=dataset,\n",
    "    )\n",
    "    # For some reason, like option is not working, so I have to manually add x and y\n",
    "    assert (raster[\"x\"].shape == dataset[\"x\"].shape)\n",
    "    assert (raster[\"y\"].shape == dataset[\"y\"].shape)\n",
    "    raster[\"x\"] = dataset[\"x\"]\n",
    "    raster[\"y\"] = dataset[\"y\"]\n",
    "    raster = raster.drop_vars([\"spatial_ref\"])\n",
    "    raster = raster.chunk({\"x\": 100, \"y\": 100})\n",
    "    print(\"Done!\")\n",
    "    return raster\n",
    "\n",
    "def compute_zonal_stats(dataset, shape, value_var, groupby_var, gridded_groups=None, stats_funcs=[\"sum\"], delayed=True):\n",
    "    \n",
    "    # Rasterize shape\n",
    "    if gridded_groups is None:\n",
    "        gridded_groups = rasterize_shape_like_dataset(shape[[groupby_var, \"geometry\"]], dataset)\n",
    "\n",
    "    # Compute zonal stats  \n",
    "    assert gridded_groups.chunks is not None, \"Please, chunk the dataset before computing zonal stats! (e.g. dataset.chunk({'x': 100, 'y': 100})). Otherwise, you will get a MemoryError.\"\n",
    "    assert dataset.chunks is not None, \"Please, chunk the dataset before computing zonal stats! (e.g. dataset.chunk({'x': 100, 'y': 100})). Otherwise, you will get a MemoryError.\"\n",
    "\n",
    "    print(\"Setting up zonal stats...\")\n",
    "    pop_by_adm = xrspatial.zonal.stats(gridded_groups[groupby_var], dataset[value_var], stats_funcs=stats_funcs)\n",
    "    print(\"Done! Computing zonal stats...\")    \n",
    "    if delayed:\n",
    "        return pop_by_adm\n",
    "    \n",
    "    with ProgressBar():\n",
    "        pop_by_adm = pop_by_adm.compute()\n",
    "    \n",
    "    # Format zonal_stats dataframe\n",
    "    pop_by_adm = pop_by_adm.rename(columns={\n",
    "        \"sum\": value_var,\n",
    "        \"mean\": f\"{value_var}_mean\",\n",
    "        \"zone\": groupby_var,\n",
    "    })\n",
    "    \n",
    "    result = (\n",
    "        shape[[groupby_var, \"geometry\"]]\n",
    "        .merge(pop_by_adm, on=groupby_var)\n",
    "    )\n",
    "    return result \n",
    "\n",
    "def compute_zonal_stats_over_time(dataset, shape, value_var, groupby_var, population_data=None, gridded_groups=None, stats_funcs=[\"mean\"], delayed=True):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    # Rasterize shape\n",
    "    if gridded_groups is None:\n",
    "        gridded_groups = rasterize_shape_like_dataset(shape[[groupby_var, \"geometry\"]], dataset)\n",
    "\n",
    "    # Compute zonal stats  \n",
    "    assert gridded_groups.chunks is not None, \"Please, chunk the dataset before computing zonal stats! (e.g. dataset.chunk({'x': 100, 'y': 100})). Otherwise, you will get a MemoryError.\"\n",
    "    assert dataset.chunks is not None, \"Please, chunk the dataset before computing zonal stats! (e.g. dataset.chunk({'x': 100, 'y': 100})). Otherwise, you will get a MemoryError.\"\n",
    "    assert \"year\" in dataset.dims, \"Please, add a 'year' dimension to the dataset before computing zonal stats! (e.g. dataset = dataset.assign_coords(year=dataset.time.dt.year)).\"\n",
    " \n",
    "    print(\"Setting up zonal stats...\")\n",
    "    tasks = []\n",
    "    for year in tqdm(dataset[\"year\"].values):\n",
    "        dataset_year = dataset.sel(year=year).drop_vars(\"year\")\n",
    "        if population_data is not None:\n",
    "            dataset_year = dataset_year * population_data.sel(year=year, method=\"nearest\").drop_vars(\"year\")\n",
    "        else:\n",
    "            dataset_year[value_var] = dataset_year[value_var].astype(\"float32\")\n",
    "\n",
    "        tasks += [xrspatial.zonal.stats(zones=gridded_groups[groupby_var], values=dataset_year[value_var], stats_funcs=stats_funcs)]\n",
    "    if delayed:\n",
    "        return tasks\n",
    "\n",
    "    print(\"Done! Computing zonal stats...\")        \n",
    "    with ProgressBar():\n",
    "        result = dask.compute(*tasks)\n",
    "        \n",
    "    return result \n",
    "\n",
    "def compile_zonal_stats_over_time(tasks_results, shape, groupby_var, value_var):\n",
    "    \n",
    "    # Compile results into a single df\n",
    "    out_dict = {year: data.set_index(\"zone\") for year, data in zip(range(1970,2021), tasks_results)}\n",
    "    df = pd.concat(out_dict)\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={\"level_0\":\"year\"})\n",
    "    \n",
    "    # Format zonal_stats dataframe\n",
    "    df = df.rename(columns={\n",
    "        \"sum\": value_var,\n",
    "        \"mean\": value_var,\n",
    "        \"zone\": groupby_var,\n",
    "    })\n",
    "    \n",
    "    result = (\n",
    "        shape[[groupby_var, \"geometry\"]]\n",
    "        .merge(df, on=groupby_var)\n",
    "    )\n",
    "    return result\n",
    "\n",
    "def process_era5_data():\n",
    "   \n",
    "    # Load ERA5 data\n",
    "    \n",
    "    # Create droughts dummies\n",
    "    \n",
    "    # Annualize series\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesa WB country Data (administrative boundaries) y GPW (population data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading World Bank country data...\n",
      "Data loaded!\n",
      "Processing Population data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:80: RuntimeWarning: Engine 'cfgrib' loading failed:\n",
      "Cannot find the ecCodes library\n",
      "  warnings.warn(f\"Engine {name!r} loading failed:\\n{ex}\", RuntimeWarning)\n",
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      " 20%|██        | 1/5 [00:04<00:18,  4.72s/it]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "WB_country = load_WB_country_data()\n",
    "population = load_population_data(bounds=WB_country.total_bounds)\n",
    "\n",
    "# # Rasterize WB_country\n",
    "# WB_country_grid = rasterize_shape_like_dataset(\n",
    "#     WB_country[[\"ID\", \"geometry\"]], \n",
    "#     population\n",
    "# )\n",
    "\n",
    "# WB_country_path = rf\"E:\\client_v2_data\\WB_country_grid.nc\"\n",
    "# print(\"Saving WB_country_grid...\")\n",
    "# with ProgressBar():\n",
    "#     WB_country_grid.to_netcdf(WB_country_path)\n",
    "        \n",
    "# WB_country[[\"ID\", \"OBJECTID\", \"ADM2_CODE\", \"ADM2_NAME\", \"ADM1_CODE\", \"ADM1_NAME\", \"ADM0_CODE\", \"ADM0_NAME\", \"ADMLAST_CODE\", \"ADMLAST_NAME\", \"geometry\"]].to_feather(rf\"E:\\client_v2_data\\WB_country_IDs.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERA Base de shocks, sin interpolar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "droughts_path = rf\"{DATA_OUT}\\ERA5_droughts_yearly.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing droughts dataset...\n",
      "[########################################] | 100% Completed | 2hr 1ms\n"
     ]
    }
   ],
   "source": [
    "# if not os.path.exists(droughts_path):\n",
    "print(\"Preparing droughts dataset...\")\n",
    "# Genera base de sequías\n",
    "era5 = xr.open_dataset(rf\"{DATA_OUT}\\ERA5_monthly_1970-2021_SPI-SPEI.nc\", chunks={'latitude': 1000, 'longitude': 1000})\n",
    "# Corrije la dimensión x, que va de 0 a 360\n",
    "era5 = era5.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "era5 = utils.coordinates_from_0_360_to_180_180(era5)\n",
    "# era5 = era5.chunk({'time': 5})\n",
    "\n",
    "# Calcula las sequías anuales\n",
    "spi_yearly = era5.groupby(\"time.year\").min()\n",
    "with ProgressBar():\n",
    "    spi_yearly.to_netcdf(rf\"E:\\client_v2_data\\ERA5_yearly_1970-2021_SPI-SPEI.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 228.71 s\n"
     ]
    }
   ],
   "source": [
    "spi_yearly = xr.open_dataset(rf\"E:\\client_v2_data\\ERA5_yearly_1970-2021_SPI-SPEI.nc\", chunks={\"x\": 900, \"y\": 1800})\n",
    "\n",
    "spi_spei_vars = [var for var in spi_yearly.data_vars if \"-\" in var]\n",
    "for var in spi_spei_vars:\n",
    "    for threshold_str in [\"1_0\", \"1_5\", \"2_0\", \"2_5\"]:\n",
    "        threshold = float(threshold_str.replace(\"_\", \".\"))\n",
    "        spi_yearly[f\"drought_{var}_{threshold_str}\"] = (spi_yearly[var] < -threshold).astype(\"bool\")\n",
    "\n",
    "spi_yearly = spi_yearly[[var for var in spi_yearly.data_vars if \"drought\" in var]]\n",
    "with ProgressBar():\n",
    "    spi_yearly.to_netcdf(droughts_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
