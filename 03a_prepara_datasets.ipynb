{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "import dask\n",
    "import xarray as xr\n",
    "import xrspatial\n",
    "from dask.diagnostics import ProgressBar\n",
    "from geocube.api.core import make_geocube\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "PATH = \"D:\\World Bank\\CLIENT v2\"\n",
    "DATA_RAW = rf\"{PATH}\\Data\\Data_raw\"\n",
    "DATA_PROC = rf\"{PATH}\\Data\\Data_proc\"\n",
    "DATA_OUT = rf\"{PATH}\\Data\\Data_out\"\n",
    "GPW_PATH = rf\"D:\\Datasets\\Gridded Population of the World\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floods = pd.read_csv(rf\"{DATA_RAW}\\Floods\\GloFAS_floods.csv\")\n",
    "\n",
    "def load_population_data(bounds=None, generate=False):\n",
    "    print(\"Processing Population data...\")\n",
    "\n",
    "    # Select all files in GPW folder\n",
    "    files = os.listdir(GPW_PATH)\n",
    "    files = [f for f in files if f.endswith(\".tif\")]\n",
    "    \n",
    "    # Compile into a single dataset\n",
    "    dss = []\n",
    "    for f in tqdm(files):\n",
    "        \n",
    "        ds = xr.open_dataset(os.path.join(GPW_PATH, f), chunks={\"x\": 1000, \"y\": 1000})\n",
    "        ds[\"band_data\"] = ds[\"band_data\"].astype(np.uint32)\n",
    "        if bounds is not None:\n",
    "            ds = ds.sel(\n",
    "                x=slice(bounds[0], bounds[2]), y=slice(bounds[3], bounds[1])\n",
    "            )\n",
    "        if generate:\n",
    "            with ProgressBar():\n",
    "                ds.sel(band=1).drop_vars(\"band\").band_data.rio.to_raster(rf\"{DATA_PROC}\\{f.replace('.tif','_proc.tif')}\")\n",
    "                print(f\"Saved {f.replace('.tif','_proc.tif')}\")\n",
    "        \n",
    "        ds[\"year\"] = int(f.split(\"_\")[5])\n",
    "        ds = ds.set_coords('year')\n",
    "        dss += [ds]\n",
    "        \n",
    "    population = xr.concat(dss, dim=\"year\")    \n",
    "    \n",
    "    # Filter if bounds are provided\n",
    "    if bounds is not None:\n",
    "        population = population.sel(\n",
    "            x=slice(bounds[0], bounds[2]), y=slice(bounds[3], bounds[1])\n",
    "        )\n",
    "        \n",
    "    # Clean band dimension\n",
    "    population = population.sel(band=1).drop_vars([\"band\"])\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return population\n",
    "\n",
    "def load_precipitation_data():\n",
    "    era5 = xr.open_dataset(\n",
    "        rf\"{DATA_OUT}\\ERA5_monthly_1970-2021_SPI-SPEI.nc\",\n",
    "        chunks={\"latitude\": 100, \"longitude\": 100},\n",
    "    )\n",
    "    era5 = era5.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "    return\n",
    "\n",
    "def load_IPUMS_country_data(wb_map):\n",
    "    from osgeo import gdal\n",
    "    import geopandas as gpd\n",
    "    from IPython.display import display\n",
    "\n",
    "    gdal.SetConfigOption('SHAPE_RESTORE_SHX', 'YES')\n",
    "\n",
    "    print(\"Loading IPUMS country data...\")\n",
    "\n",
    "    path = rf\"{DATA_RAW}\\IPUMS Fixed\"\n",
    "    files = os.listdir(path)\n",
    "    shpfiles = [f for f in files if \".shp\" in f]\n",
    "    geo2_files = [f for f in shpfiles if \"geo2_\" in f]\n",
    "    geo2_countries = [name.split(\"_\")[1][:2] for name in geo2_files]\n",
    "    geo1_files = [f for f in shpfiles if \"geo1_\" in f]\n",
    "    geo1_files = [f for f in geo1_files if f.split(\"_\")[1][:2] not in geo2_countries]\n",
    "    # return geo1_files, geo2_files\n",
    "    files = geo2_files + geo1_files\n",
    "\n",
    "    gdfs = []\n",
    "    file_created = False\n",
    "    schema = None\n",
    "    pqwriter = None\n",
    "    for f in tqdm(files):\n",
    "        gdf = gpd.read_file(os.path.join(path, f))\n",
    "        \n",
    "        # Store in parquet\n",
    "        pqwriter, file_created, schema = store_in_parquet_file(gdf, pqwriter, file_created, schema)\n",
    "        # Guardo el chunk en un archivo parquet\n",
    "\n",
    "        ctry_name  = gdf.CNTRY_NAME[0]\n",
    "        \n",
    "        # MANUAL FIXES\n",
    "        if ctry_name == \"United States\":\n",
    "            print(\"Removing Puerto Rico from US...\")\n",
    "            gdf = gdf[~gdf.GEOLEVEL2.isin([\"840721079\", \"840721080\", \"840721081\", \"840721082\", \"840721083\", \"840721084\", \"840721085\"])]\n",
    "            assert gdf.shape[0] > 0\n",
    "        if ctry_name == \"Nigeria\":\n",
    "            print(\"Removing glitch from Nigeria...\")\n",
    "            gdf = gdf[gdf.GEOLEVEL2 != \"566024008\"]\n",
    "            assert gdf.shape[0] > 0\n",
    "        if ctry_name == \"Israel\":\n",
    "            print(\"Normalizing Israel map to match WB...\")\n",
    "            # FIXME: there should be a better way to write this...\n",
    "            fix = pd.read_stata(rf\"{DATA_PROC}\\fixes\\fix_israel_geo2_adm1.dta\").astype({\"adm2\":str, \"adm2_new\":int})\n",
    "            gdf = gdf.merge(fix, left_on=\"GEOLEVEL2\", right_on=\"adm2\").drop(columns=\"geometry\").drop_duplicates(subset=\"adm2_new\")\n",
    "            gdf = wb_map[[\"geometry\", \"ADM1_CODE\"]].merge(gdf, on=\"ADM1_CODE\")\n",
    "            gdf[\"GEOLEVEL2\"] = gdf[\"adm2_new\"]\n",
    "            gdf = gdf.drop(columns=[\"ADM1_CODE\", \"adm2\", \"adm2_new\"])\n",
    "            assert gdf.shape[0] > 0\n",
    "        if ctry_name == \"Palestine\":\n",
    "            print(\"Normalizing Palestine map to match WB...\")\n",
    "            fix = pd.read_stata(rf\"{DATA_PROC}\\fixes\\fix_palestine_geo1_adm1.dta\").astype({\"adm1\": str, \"ADM1_CODE\": int, \"adm1_new\": str})\n",
    "            gdf = gdf.merge(fix, left_on=\"GEOLEVEL1\", right_on=\"adm1\").drop(columns=\"geometry\").drop_duplicates(subset=\"ADM1_CODE\")\n",
    "            gdf = wb_map[[\"geometry\", \"ADM1_CODE\"]].merge(gdf, on=\"ADM1_CODE\")\n",
    "            gdf[\"GEOLEVEL1\"] = gdf[\"adm1_new\"]\n",
    "            gdf = gdf.drop(columns=[\"ADM1_CODE\", \"adm1\", \"adm1_new\"])\n",
    "            assert gdf.shape[0] > 0\n",
    "\n",
    "        gdfs += [gdf]\n",
    "        \n",
    "    pqwriter.close()\n",
    "    gdf = gpd.GeoDataFrame(pd.concat(gdfs))\n",
    "    gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "    \n",
    "    # Keep only relevant columns\n",
    "    gdf.loc[gdf[\"GEOLEVEL1\"].isna(), \"GEOLEVEL1\"] = gdf.loc[gdf[\"GEOLEVEL1\"].isna(), \"GEOLEVEL2\"].str[:6] \n",
    "    id_cols = [\"CNTRY_CODE\", \"GEOLEVEL1\", \"GEOLEVEL2\"] \n",
    "    gdf = gdf[[\"geometry\"] + id_cols]\n",
    "    \n",
    "    for col in id_cols:\n",
    "        gdf[col] = pd.to_numeric(gdf[col]).fillna(0)\n",
    "    gdf = gdf[gdf.GEOLEVEL1 != 0] # Drop unavailable data\n",
    "    gdf = gdf[gdf.GEOLEVEL2 != 888888888] # Drop unavailable data\n",
    "    assert gdf.duplicated(subset=id_cols).sum() == 0, \"There are duplicated rows in the data!\"\n",
    "    \n",
    "    # Create ID\n",
    "    gdf[\"ID\"] = gdf.groupby(id_cols).ngroup()\n",
    "    assert gdf.ID.nunique() == gdf.shape[0], \"ID is not unique!, there's some bug in the code...\"\n",
    "    print(\"Data loaded!\")\n",
    "    return gdf\n",
    "\n",
    "def load_WB_country_data():\n",
    "    print(\"Loading World Bank country data...\")\n",
    "    WB_country = gpd.read_file(rf\"{DATA_RAW}\\world_bank_adm2\\world_bank_adm2.shp\")\n",
    "    \n",
    "    # Assign nan when ADM2 is not available \n",
    "    WB_country.loc[WB_country.ADM2_NAME == \"Administrative unit not available\", \"ADM2_CODE\"] = (\n",
    "        np.nan\n",
    "    )\n",
    "    \n",
    "    # Create ADM_LAST variable: ADM2_NAME if available, else ADM1_NAME\n",
    "    for col in [\"ADM0_CODE\", \"ADM1_CODE\", \"ADM2_CODE\"]:\n",
    "        WB_country[col] = WB_country[col].fillna(0)\n",
    "\n",
    "    # Dissolve by ADM_LAST and country code    \n",
    "    WB_country = WB_country.dissolve(by=[\"ADM2_CODE\",\"ADM1_CODE\", \"ADM0_CODE\"]).reset_index()\n",
    "    \n",
    "    # # Create ID\n",
    "    WB_country[\"ID\"] = WB_country.groupby([\"ADM2_CODE\", \"ADM1_CODE\", \"ADM0_CODE\"]).ngroup()\n",
    "    assert WB_country.ID.nunique() == WB_country.shape[0], \"ID is not unique!, there's some bug in the code...\"\n",
    "    print(\"Data loaded!\")\n",
    "    return WB_country\n",
    "\n",
    "# def create_IPUMS_unique_ids():\n",
    "    \n",
    "\n",
    "\n",
    "def rasterize_shape_like_dataset(shape, dataset):\n",
    "    print(\"Rasterizing shape...\")\n",
    "    raster = make_geocube(\n",
    "        vector_data=shape,\n",
    "        like=dataset,\n",
    "    )\n",
    "    # For some reason, like option is not working, so I have to manually add x and y\n",
    "    assert (raster[\"x\"].shape == dataset[\"x\"].shape)\n",
    "    assert (raster[\"y\"].shape == dataset[\"y\"].shape)\n",
    "    raster[\"x\"] = dataset[\"x\"]\n",
    "    raster[\"y\"] = dataset[\"y\"]\n",
    "    raster = raster.drop_vars([\"spatial_ref\"])\n",
    "    print(\"Done!\")\n",
    "    return raster\n",
    "\n",
    "def store_in_parquet_file(gdf, pqwriter, file_created, schema):\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    parquet_file_path = fr\"{DATA_PROC}\\IPUMS_full.parquet\"\n",
    "    df = pd.DataFrame(gdf.copy())\n",
    "    df['geometry'] = df['geometry'].apply(lambda x: x.wkt if x is not None else None)\n",
    "    \n",
    "    if \"GEOLEVEL1\" not in df.columns:\n",
    "        df[\"GEOLEVEL1\"] = \"None\"\n",
    "    elif \"GEOLEVEL2\" not in df.columns:\n",
    "        df[\"GEOLEVEL2\"] = \"None\"\n",
    "        \n",
    "    table = pa.Table.from_pandas(df)\n",
    "    if file_created is False:\n",
    "        # create a parquet write object giving it an output file\n",
    "        schema = table.schema\n",
    "        pqwriter = pq.ParquetWriter(parquet_file_path, schema)\n",
    "        file_created = True\n",
    "    else:\n",
    "        # Ensure the columns are in the same order as the initial schema\n",
    "        df = df[[field.name for field in schema]]\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "    pqwriter.write_table(table)\n",
    "    \n",
    "    return pqwriter, file_created, schema\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "def fix_IPUMS_conflicting_international_boundaries(wb, ipums, export_maps=False):\n",
    "    ''' Fix IPUMS conflicting international boundaries by clipping the data to the World Bank boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    wb (GeoDataFrame): World Bank boundaries\n",
    "    ipums (GeoDataFrame): IPUMS boundaries\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame: IPUMS boundaries with the conflicting boundaries clipped to the World Bank boundaries\n",
    "    '''\n",
    "    countries_to_clip = {\n",
    "        # Countries with conflicting boundaries\n",
    "        \"Marruecos\": {\"WB\": 169, \"IPUMS\": \"504\"},\n",
    "        \"South Sudan\": {\"WB\": 74, \"IPUMS\": \"728\"},\n",
    "        \"Sudan\": {\"WB\": 6, \"IPUMS\": \"729\"},\n",
    "        \"Egypt\": {\"WB\": 40765, \"IPUMS\": \"818\"},\n",
    "        \"Kenya\": {\"WB\": 133, \"IPUMS\": \"404\"},\n",
    "        \"Russia\": {\"WB\": 204, \"IPUMS\": \"643\"},\n",
    "        \"India\": {\"WB\": 115, \"IPUMS\": \"356\"},\n",
    "        \"China\": {\"WB\": 147295, \"IPUMS\": \"156\"},\n",
    "        \"Kyrghyzstan\": {\"WB\": 138, \"IPUMS\": \"417\"},\n",
    "    }                        \n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for country, codes in countries_to_clip.items():\n",
    "            print(country)\n",
    "            wbcode = codes[\"WB\"]\n",
    "            ipumscode = codes[\"IPUMS\"]\n",
    "            \n",
    "            # Clip IPUMS using WB\n",
    "            clipped = (\n",
    "                ipums[ipums.CNTRY_CODE == ipumscode]\n",
    "                .clip(wb[wb.ADM0_CODE == wbcode])\n",
    "            )\n",
    "            \n",
    "            if export_maps:\n",
    "                # Plot the clipped data\n",
    "                fig, ax = plt.subplots(figsize=(10, 10))\n",
    "                ipums[ipums.CNTRY_CODE == ipumscode].plot(ax=ax)\n",
    "                clipped.plot(ax=ax, facecolor=\"none\", edgecolor=\"red\")\n",
    "                plt.savefig(f\"{DATA_PROC}/fixes/{country}.png\")\n",
    "            \n",
    "            # remove areas with residual geometry\n",
    "            clipped = clipped[clipped.geometry.area > .0001]\n",
    "            print(\"Se eliminaron \", len(ipums[ipums.CNTRY_CODE == ipumscode]) - len(clipped), \" registros\")\n",
    "            \n",
    "            # Update the original dataframe\n",
    "            ipums.loc[ipums.CNTRY_CODE == ipumscode] = clipped\n",
    "            \n",
    "        # Remove unwanted shapes from Israel & Palestine\n",
    "        ipums = ipums[~(ipums[\"GEOLEVEL2\"].astype(str).str[-2:].isin([\"97\", \"98\", \"99\"]) & ipums[\"CNTRY_CODE\"].isin([\"376\", \"275\"]))]\n",
    "        \n",
    "    return ipums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genera y Carga bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_country = load_WB_country_data()\n",
    "IPUMS_country = load_IPUMS_country_data(WB_country)\n",
    "IPUMS_country = IPUMS_country.clip(WB_country.total_bounds)\n",
    "population = load_population_data(bounds=WB_country.total_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valida IDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data de SHPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpia_ids(df):\n",
    "    # Make string columns\n",
    "    for col in [\"CNTRY_CODE\", \"GEOLEVEL1\", \"GEOLEVEL2\"]:\n",
    "        df[col] = df[col].replace({\"None\": np.nan, \"nan\": np.nan, None: np.nan})\n",
    "        df[col] = df[col].apply(lambda x: str(int(x)) if not pd.isna(x) else np.nan)\n",
    "\n",
    "    df[\"CNTRY_CODE\"] = df[\"CNTRY_CODE\"].str.zfill(3)\n",
    "    df[\"GEOLEVEL1\"] = df[\"GEOLEVEL1\"].str.zfill(6)\n",
    "    df[\"GEOLEVEL2\"] = df[\"GEOLEVEL2\"].str.zfill(9)\n",
    "    \n",
    "    ## Transformaciones para conseguir ids unicos en IPUMS:\n",
    "    # 1) LLevnar GEOLEVE2\n",
    "    df.loc[df[\"GEOLEVEL2\"].isna(), \"GEOLEVEL2\"] = df.loc[df[\"GEOLEVEL2\"].isna(), \"GEOLEVEL1\"]\n",
    "\n",
    "    # 2) Eliminar GEOLEVEL2 con 888888888 / 888888 o nan en geo2 y geo1\n",
    "    df = df.drop(df[df[\"GEOLEVEL2\"].isin([\"888888888\", \"888888\"])].index)\n",
    "    df = df.drop(df[df[\"GEOLEVEL1\"].isna() & df[\"GEOLEVEL2\"].isna()].index)\n",
    "    \n",
    "    # 3) Eliminar geometrías nulas\n",
    "    if \"geometry\" in df.columns:\n",
    "        df = df.drop(df[df[\"geometry\"].isna()].index)\n",
    "        \n",
    "    return df\n",
    "    # Verifico codigos consistentes\n",
    "    assert df[(df.GEOLEVEL2.str[:3] != df.CNTRY_CODE)].shape[0] == 0\n",
    "    # Verifico duplicados\n",
    "    assert df[df.GEOLEVEL2.duplicated(keep=False)].shape[0] == 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata(r\"D:\\World Bank\\CLIENT\\Data\\IPUMS_asia_5.dta\", convert_categoricals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_shps = pd.read_parquet(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\IPUMS_full.parquet\")\n",
    "id_IPUMS_shps = limpia_ids(id_IPUMS_shps)\n",
    "id_IPUMS_shps.drop(columns=\"geometry\").to_stata(rf\"{DATA_PROC}\\IPUMS_unique_ids_verifier.dta\", version=117)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data de IPUMS censos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\IPUMS_ids\"\n",
    "\n",
    "continentes = os.listdir(path)\n",
    "dfs = []\n",
    "for continente in continentes:\n",
    "    df = pd.read_stata(os.path.join(path, continente), convert_categoricals=False)\n",
    "    df[\"file\"] = continente\n",
    "    dfs += [df]\n",
    "\n",
    "id_IPUMS_data = pd.concat(dfs)\n",
    "id_IPUMS_data = id_IPUMS_data.rename(columns={\"country\":\"CNTRY_CODE\", \"geolev1\":\"GEOLEVEL1\", \"geolev2\":\"GEOLEVEL2\"})\n",
    "id_IPUMS_data = limpia_ids(id_IPUMS_data)\n",
    "\n",
    "id_IPUMS_data\n",
    "assert id_IPUMS_data[(id_IPUMS_data.GEOLEVEL2.str[:3] != id_IPUMS_data.CNTRY_CODE)].shape[0] == 0\n",
    "# Verifico duplicados\n",
    "assert id_IPUMS_data[id_IPUMS_data.GEOLEVEL2.duplicated(keep=False)].shape[0] == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_geo2_data = id_IPUMS_data.drop_duplicates(subset=\"GEOLEVEL2\")\n",
    "unique_geo2_shps = id_IPUMS_shps.drop_duplicates(subset=\"GEOLEVEL2\")\n",
    "\n",
    "merge = unique_geo2_shps.merge(unique_geo2_data, on=\"GEOLEVEL2\", how=\"outer\", indicator=True, validate=\"1:1\")\n",
    "print(merge._merge.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[merge._merge == \"right_only\"].CNTRY_CODE_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[merge.CNTRY_CODE_y == \"704\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_data[id_IPUMS_data.GEOLEVEL2.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_data.iloc[772]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_data[(id_IPUMS_data.GEOLEVEL2.str[:3] != id_IPUMS_data.CNTRY_CODE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_ids = id_IPUMS_data.geolev2.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesa WB/IPUMS shapes (administrative boundaries) y GPW (population data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rasterize WB_country\n",
    "WB_country_grid = rasterize_shape_like_dataset(\n",
    "    WB_country[[\"ID\", \"geometry\"]], \n",
    "    population\n",
    ")\n",
    "\n",
    "print(\"Saving WB_country_grid...\")\n",
    "WB_country_grid.to_netcdf(rf\"{DATA_PROC}\\WB_country_grid.nc\")\n",
    "        \n",
    "WB_country[[\"ID\", \"OBJECTID\", \"ADM2_CODE\", \"ADM1_CODE\", \"ADM0_CODE\",  \"geometry\"]].to_feather(rf\"{DATA_PROC}\\WB_country_IDs.feather\")\n",
    "\n",
    "\n",
    "### Rasterize IPUMS_country\n",
    "IPUMS_country_grid = rasterize_shape_like_dataset(\n",
    "    IPUMS_country[[\"ID\", \"geometry\"]], \n",
    "    population\n",
    ")\n",
    "\n",
    "IPUMS_country_path = rf\"{DATA_PROC}\\IPUMS_country_grid.nc\"\n",
    "print(\"Saving IPUMS_country_grid...\")\n",
    "IPUMS_country_grid.to_netcdf(rf\"{DATA_PROC}\\IPUMS_country_grid.nc\")\n",
    "        \n",
    "IPUMS_country.to_feather(rf\"{DATA_PROC}\\IPUMS_country_IDs.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Droughts, con resolución original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "droughts_path = rf\"{DATA_OUT}\\ERA5_droughts_yearly.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not os.path.exists(droughts_path):\n",
    "print(\"Preparing droughts dataset...\")\n",
    "# Genera base de sequías\n",
    "era5 = xr.open_dataset(rf\"{DATA_OUT}\\ERA5_monthly_1970-2021_SPI-SPEI.nc\", chunks={'latitude': 1000, 'longitude': 1000})\n",
    "# Corrije la dimensión x, que va de 0 a 360\n",
    "era5 = era5.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "era5 = utils.coordinates_from_0_360_to_180_180(era5) # FIXME: no se si esto está andando bien, pero creo que si. VERIFICAR\n",
    "\n",
    "# Calcula las sequías anuales\n",
    "spi_yearly = era5.groupby(\"time.year\").min()\n",
    "with ProgressBar():\n",
    "    spi_yearly.to_netcdf(rf\"{DATA_PROC}\\ERA5_yearly_1970-2021_SPI-SPEI.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spi_yearly = xr.open_dataset(rf\"{DATA_PROC}\\ERA5_yearly_1970-2021_SPI-SPEI.nc\", chunks={\"x\": 900, \"y\": 1800})\n",
    "\n",
    "spi_spei_vars = [var for var in spi_yearly.data_vars if \"-\" in var]\n",
    "for var in spi_spei_vars:\n",
    "    for threshold_str in [\"1_0\", \"1_5\", \"2_0\", \"2_5\"]:\n",
    "        threshold = float(threshold_str.replace(\"_\", \".\"))\n",
    "        threshold_str = threshold_str.replace(\"_\", \"\")\n",
    "        spi_yearly[f\"drought_{var}_{threshold_str}sd\"] = (spi_yearly[var] < -threshold).astype(\"bool\")\n",
    "\n",
    "spi_yearly = spi_yearly[[var for var in spi_yearly.data_vars if \"drought\" in var]]\n",
    "spi_yearly = spi_yearly.rename({\n",
    "    var: var.replace(\"drought_\", \"\").replace(\"-\", \"\") for var in spi_yearly.data_vars\n",
    "})\n",
    "with ProgressBar():\n",
    "    spi_yearly.drop_duplicates(dim=\"x\").to_netcdf(droughts_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validación de IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_shp = gpd.read_feather(rf\"{DATA_PROC}\\IPUMS_country_IDs.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_shp = (\n",
    "    id_IPUMS_shp\n",
    "        .rename(columns={\"CNTRY_CODE\":\"country\", \"GEOLEVEL1\":\"geolev1\", \"GEOLEVEL2\": \"geolev2\"})\n",
    "        .drop_duplicates(subset=[\"country\", \"geolev1\", \"geolev2\"]))\n",
    "id_IPUMS_shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\IPUMS_ids\"\n",
    "\n",
    "continentes = os.listdir(path)\n",
    "dfs = []\n",
    "for continente in continentes:\n",
    "    dfs += [pd.read_stata(os.path.join(path, continente), convert_categoricals=False)]\n",
    "\n",
    "id_IPUMS_data = pd.concat(dfs)\n",
    "id_IPUMS_data[id_IPUMS_data.duplicated(subset=\"geolev2\", keep=False)].dropna()\n",
    "# id_IPUMS_data = id_IPUMS_data.fillna(0).drop_duplicates()\n",
    "\n",
    "\n",
    "# id_IPUMS_data.loc[id_IPUMS_data[\"geolev2\"]==0, \"geolev2\"] = id_IPUMS_data.loc[id_IPUMS_data[\"geolev2\"]==0, \"geolev1\"]\n",
    "# id_IPUMS_data = id_IPUMS_data.fillna(0).drop_duplicates()\n",
    "\n",
    "# id_IPUMS_data.loc[id_IPUMS_data[\"geolev2\"]!=0, \"geolev1\"] = id_IPUMS_data.loc[id_IPUMS_data[\"geolev2\"]!=0, \"geolev2\"].astype(str).str.zfill(9).str[:6].astype(float) \n",
    "# id_IPUMS_data = id_IPUMS_data.fillna(0).drop_duplicates()\n",
    "\n",
    "# id_IPUMS_data = id_IPUMS_data[id_IPUMS_data.geolev1 != 0] # Drop unavailable data\n",
    "# id_IPUMS_data = id_IPUMS_data[id_IPUMS_data.geolev2 != 888888888] # Drop unavailable data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "480: Geolev1 está bien seleccionando [:6]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gpd.read_file(r\"C:\\Users\\ofici\\Downloads\\geo2_mu1990_2011\\geo2_mu1990_2011.shp\")\n",
    "test[test[\"GEOLEVEL2\"]==\"480014002\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_data[id_IPUMS_data.duplicated(subset=\"geolev2\", keep=False)].dropna()#.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_IPUMS_shp.loc[id_IPUMS_shp[\"geolev2\"]==0, \"geolev2\"] = id_IPUMS_shp.loc[id_IPUMS_shp[\"geolev2\"]==0, \"geolev1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = id_IPUMS_shp.merge(id_IPUMS_data, on=[\"country\", \"geolev2\"], how=\"outer\", indicator=True)\n",
    "m._merge.value_counts()\n",
    "# m[m._merge == \"both\"].explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errores = m[m._merge!=\"both\"]\n",
    "pd.crosstab(errores.country, errores._merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errores[errores.country==170].sort_values([\"geolev1\", \"geolev2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errores[errores.geolev2==170005002]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huracanes, con resolución completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIXED Parameters\n",
    "sshws_min_wind = {\n",
    "    # Saffir-Simpson Hurricane Wind Scale\n",
    "    # Measured in knots\n",
    "    # https://www.nhc.noaa.gov/aboutsshws.php\n",
    "    5: 137,\n",
    "    4: 113,\n",
    "    3: 96,\n",
    "    2: 83,\n",
    "    1: 64,\n",
    "}\n",
    "\n",
    "agency_measurements = {\n",
    "    \"USA\": 1, # 1-m measurement\n",
    "    \"TOK\": 3, # 3-m measurement\n",
    "    \"CMA\": 2, # 2-m measurement\n",
    "    \"HKO\": 10, # 10-m measurement\n",
    "    \"KMA\": 10,\n",
    "    \"NEW\": 3,\n",
    "    \"REU\": 10,\n",
    "    \"BOM\": 10,\n",
    "    \"NAD\": 10,\n",
    "    \"WEL\": 10,\n",
    "    \"DS8\": 1,\n",
    "    \"TD6\": 1,\n",
    "    \"TD5\": 1,\n",
    "    \"NEU\": 1,\n",
    "    \"MLC\": 1,\n",
    "}\n",
    "\n",
    "conversion_factor_to_1m = {\n",
    "    1: 1,\n",
    "    2: (1.22/1.15+1.17/1.11)/2,\n",
    "    3: (1.22/1.12+1.17/1.09)/2,\n",
    "    10: (1.22/1.06+1.17/1.05)/2,\n",
    "}\n",
    "\n",
    "agency_1m_conversion_factor = {\n",
    "    k:conversion_factor_to_1m[v] for k, v in agency_measurements.items()\n",
    "}\n",
    "\n",
    "## Functions\n",
    "def convert_wind_to_1m(wind, agency):\n",
    "    if agency in agency_1m_conversion_factor:\n",
    "        return wind * agency_1m_conversion_factor[agency]\n",
    "    return wind\n",
    "\n",
    "def convert_wind_to_sshws(wind):\n",
    "    for cat, min_wind in sshws_min_wind.items():\n",
    "        if wind >= min_wind:\n",
    "            return cat\n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "gdf = gpd.read_file(r\"D:\\Datasets\\International Best Track Archive for Climate Stewardship (IBTrACS)\\IBTrACS.ALL.list.v04r01.lines.shp\")#, \"BASIN\", \"SUBBASIN\", \"NAME\", \"ISO_TIME\", \"LAT\", \"LON\", \"WMO_WIND\", \"WMO_PRES\", \"WMO_AGENCY\", \"TRACK_TYPE\"])\n",
    "\n",
    "# Fill interpolated xy values\n",
    "gdf[\"WMO_WIND\"] = gdf[\"WMO_WIND\"].ffill()\n",
    "\n",
    "# Data from 1950 onwards\n",
    "gdf[\"year\"] = gdf[\"ISO_TIME\"].str.split(\"-\").str[0].astype(int)\n",
    "gdf = gdf[gdf[\"year\"] >= 1970]\n",
    "\n",
    "# Convert each Agency wind to 1m-MSW\n",
    "wind_cols = [col for col in gdf.columns if \"_WIND\" in col and \"WMO_WIND\" not in col]\n",
    "agencies = [col.replace(\"_WIND\", \"\") for col in wind_cols]\n",
    "\n",
    "for col in tqdm(wind_cols):\n",
    "    agency = col.split(\"_\")[0]\n",
    "    gdf[col] = gdf[col].apply(lambda x: convert_wind_to_1m(x, agency))\n",
    "\n",
    "gdf[\"wind_speed\"] = gdf[wind_cols].max(axis=1)\n",
    "\n",
    "# Convert wind to SSHWS category\n",
    "gdf[\"category\"] = gdf[\"wind_speed\"].apply(convert_wind_to_sshws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "buffers = [0.1, 0.25, 0.50, 1.00]\n",
    "years = gdf[\"year\"].unique()\n",
    "\n",
    "paths_by_year = {}\n",
    "for year in tqdm(years, leave=False):\n",
    "    paths_by_year[year] = []\n",
    "    for buffer in buffers:\n",
    "        ### Filter year and create raster map based on the buffered best-track \n",
    "        ###     of the hurricane\n",
    "        print(buffer)\n",
    "        gdf_year = gdf[gdf.year == year]\n",
    "        gdf_year = gdf_year[[\"wind_speed\", \"geometry\"]].fillna(0)\n",
    "\n",
    "        # Apply buffer to center of the storm\n",
    "        gdf_year[\"geometry\"] = gdf_year.geometry.buffer(buffer)\n",
    "        \n",
    "        # Make the biggest shock at a certain location the one shown in the xr.dataset \n",
    "        gdf_year = gdf_year.sort_values(\"wind_speed\", ascending=True) \n",
    "        \n",
    "        raster = make_geocube(\n",
    "            vector_data=gdf_year,\n",
    "            like=population,\n",
    "        )\n",
    "        raster = raster.assign_coords({\"year\": year})\n",
    "\n",
    "        for category in [3, 4, 5]:\n",
    "            ### Once the raster wind_speed is created, create a new boolean raster \n",
    "            ###     where the winds are greater than the minimum for the category\n",
    "            varname = f\"category_{category}_b{int(buffer*100)}\"\n",
    "            # Keep only hurricanes of a certain category\n",
    "            raster_b = xr.where(\n",
    "                raster.rename({\"wind_speed\":varname})[varname] >= sshws_min_wind[category], \n",
    "                True, \n",
    "                False\n",
    "            )\n",
    "            # Transform wind_speed to boolean\n",
    "            raster_path = rf\"{DATA_PROC}\\shocks_by_grid\\hurricanes_{year}_{varname}.nc\"\n",
    "            raster_b.to_netcdf(raster_path, encoding={varname:{\"zlib\": True, \"complevel\": 7}})\n",
    "            paths_by_year[year] += [raster_path]\n",
    "            break\n",
    "        # xr.concat(dss, dim=\"year\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(memory_limit='7GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_dataset(files_year[0])[\"category_3_b10\"].encoding[\"chunksizes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compile all the data into a single dataset\n",
    "path = rf\"{DATA_PROC}\\shocks_by_grid\"\n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if \"hurricanes_\" in f and f.endswith(\".nc\")]\n",
    "\n",
    "dss = []\n",
    "for year in range(1970, 2021):\n",
    "    \n",
    "    files_year = [os.path.join(path, f) for f in files if f\"{year}\" in f]\n",
    "    ds = xr.open_mfdataset(files_year,  parallel=True, chunks=\"auto\")\n",
    "    ds = ds.assign_coords({\"year\": year})\n",
    "    dss += [ds]\n",
    "    \n",
    "ds = xr.concat(dss, dim=\"year\")\n",
    "ds.to_netcdf(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\IBTrACS_hurricanes_yearly.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\IBTrACS_hurricanes_yearly.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizer\n",
    "year = 2020\n",
    "hurr_by_name = gdf[(gdf.NAME == \"BELNA\") & (gdf.year == year)]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "hurr_by_name.plot(column=\"category\", legend=True, ax=ax)\n",
    "\n",
    "xmin, ymin, xmax, ymax = hurr_by_name.total_bounds\n",
    "hurr_proc.sel(year=year, x=slice(xmin, xmax), y=slice(ymax, ymin))[\"category_1_b10\"].plot(ax=ax, cmap=\"Greys\", alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatwave y Coldwaves, resolución original (ERA5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shocks = {\n",
    "    \"timeseries-fd-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\": {\n",
    "        \"name\": \"frostdays\",\n",
    "        \"variable\": \"timeseries-fd-monthly-mean\",\n",
    "        \"poslat\": [3,4,5,6,7,8,9,10,11], \n",
    "        \"neglat\": [1,2,3,4,5,9,10,11,12],\n",
    "        \"spell_index\": \"csdi\",\n",
    "    },\n",
    "    \"timeseries-id-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\": {\n",
    "        \"name\": \"icedays\",\n",
    "        \"variable\": \"timeseries-id-monthly-mean\",\n",
    "        \"poslat\": [3,4,5,6,7,8,9,10,11], \n",
    "        \"neglat\": [1,2,3,4,5,9,10,11,12],\n",
    "        \"spell_index\": \"csdi\",\n",
    "    },\n",
    "    \"timeseries-hd35-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\": {\n",
    "        \"name\": \"heatdays35\",\n",
    "        \"variable\": \"timeseries-hd35-monthly-mean\",\n",
    "        \"poslat\": [1,2,3,4,5,9,10,11,12], \n",
    "        \"neglat\": [3,4,5,6,7,8,9,10,11],\n",
    "        \"spell_index\": \"wsdi\",\n",
    "    },\n",
    "    \"timeseries-hd40-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\": {\n",
    "        \"name\": \"heatdays40\",\n",
    "        \"variable\": \"timeseries-hd40-monthly-mean\",\n",
    "        \"poslat\": [1,2,3,4,5,9,10,11,12], \n",
    "        \"neglat\": [3,4,5,6,7,8,9,10,11],\n",
    "        \"spell_index\": \"wsdi\",\n",
    "    }\n",
    "}\n",
    "\n",
    "tasks = []\n",
    "for file, params in shocks.items():\n",
    "    \n",
    "    ds = xr.open_dataset(rf\"{DATA_RAW}\\ERA5_CCKP\\{file}\", chunks=\"auto\")\n",
    "    name = params[\"name\"]\n",
    "    var = params[\"variable\"]\n",
    "\n",
    "    ds = ds.sel(bnds=0) # both bands are the same\n",
    "    ds = ds.drop_vars([\"lon_bnds\", \"lat_bnds\", \"bnds\"]) # drop last timestep\n",
    "    ds[var] =  (ds[var] / np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "        \n",
    "    # 1) heatwaves/coldwaves are rare events, n° days > 2/2.5 std\n",
    "    \n",
    "    stand_anomalies = xr.apply_ufunc(\n",
    "        lambda x, m, s: (x - m) / s,\n",
    "        ds.groupby(\"time.month\"),\n",
    "        ds.groupby(\"time.month\").mean(\"time\"),\n",
    "        ds.groupby(\"time.month\").std(\"time\"),\n",
    "        dask=\"parallelized\",\n",
    "    )\n",
    "    \n",
    "    tasks += [stand_anomalies.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_standardized.nc\", compute=False)]\n",
    "    \n",
    "    ds_more_than_2_std = (stand_anomalies > 2)\n",
    "    ds_more_than_25_std = (stand_anomalies > 2.5)\n",
    "    \n",
    "    tasks += [ds_more_than_2_std.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_2_std.nc\", compute=False)]\n",
    "    tasks += [ds_more_than_25_std.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_25_std.nc\", compute=False)]\n",
    " \n",
    "    # 2)  Heatwaves/Coldwaves only occur in summer/winter months:\n",
    "    ds[var] = xr.where(\n",
    "        ds.time.dt.month.isin(params[\"poslat\"]) & (ds.lat > 0), \n",
    "        0,\n",
    "        ds[var], \n",
    "    )\n",
    "    ds[var] = xr.where(\n",
    "        ds.time.dt.month.isin(params[\"neglat\"]) & (ds.lat < 0), \n",
    "        0,\n",
    "        ds[var], \n",
    "    )\n",
    "        \n",
    "    # 3) More than 6 hotdays/coldays\n",
    "    more_than_6_days = (ds[var] >= 6).astype(bool)\n",
    "    tasks += [more_than_6_days.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_6.nc\", compute=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute yearly values\n",
    "tasks = []\n",
    "for file, params in shocks.items():\n",
    "\n",
    "    name = params[\"name\"]\n",
    "    var = params[\"variable\"]\n",
    "\n",
    "    # (1) heatwaves/coldwaves are rare events, n° days > 2/2.5 std\n",
    "    ds_more_than_2_std = xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_2_std.nc\", chunks={\"time\":60})\n",
    "    ds_more_than_25_std = xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_25_std.nc\", chunks={\"time\":60})\n",
    "\n",
    "    # (2+3)  Heatwaves/Coldwaves only occur in summer/winter months:\n",
    "    more_than_6_days = xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_more_than_6.nc\", chunks={\"time\":60})\n",
    "\n",
    "    # (1+2+3) all together\n",
    "    ds_2_std = (more_than_6_days[var] * ds_more_than_2_std[var])\n",
    "    ds_25_std = (more_than_6_days[var] * ds_more_than_25_std[var])\n",
    "\n",
    "    # Annual values\n",
    "    ds_2_std = ds_2_std.groupby('time.year').max()\n",
    "    ds_25_std = ds_25_std.groupby('time.year').max()\n",
    "\n",
    "    # WSDI/CSDI (Warm/Cold Spell Duration Index) \n",
    "    spell_index = params[\"spell_index\"]\n",
    "    sdi = xr.open_dataset(rf\"{DATA_RAW}\\ERA5_CCKP\\timeseries-{spell_index}-annual-mean_era_annual_era5-0.5x0.5-timeseries_mean_1950-2020.nc\", chunks={\"year\":5})\n",
    "    sdi = sdi.sel(bnds=0) # both bands are the same\n",
    "    sdi = sdi.drop_vars([\"lon_bnds\", \"lat_bnds\", \"bnds\"]) # drop last timestep\n",
    "    sdi[f\"timeseries-{spell_index}-annual-mean\"] =  (sdi[f\"timeseries-{spell_index}-annual-mean\"] / np.timedelta64(1, 'D')).astype(int)\n",
    "    sdi = (sdi[f\"timeseries-{spell_index}-annual-mean\"] >= 6)\n",
    "    \n",
    "    # Final computation\n",
    "    waves_2_std = (ds_2_std * sdi)\n",
    "    waves_25_std = (ds_25_std * sdi)\n",
    "\n",
    "    tasks += [waves_2_std.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_annual_2_std.nc\", compute=False)]\n",
    "    tasks += [waves_25_std.to_netcdf(rf\"{DATA_PROC}\\heatwaves and coldwaves\\{name}_annual_25_std.nc\", compute=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tasks))\n",
    "dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hws = {\n",
    "    \"hw3520\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\heatdays35_annual_2_std.nc\"), \n",
    "    \"hw3525\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\heatdays35_annual_25_std.nc\"), \n",
    "    \"hw4020\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\heatdays40_annual_2_std.nc\"), \n",
    "    \"hw4025\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\heatdays40_annual_25_std.nc\"),\n",
    "}\n",
    "\n",
    "for name, ds in hws.items():\n",
    "    ds = ds.rename({\"__xarray_dataarray_variable__\":name})\n",
    "    hws[name] = ds\n",
    "    \n",
    "ds = xr.combine_by_coords(list(hws.values()))\n",
    "ds = ds.rename({\"lat\":\"y\", \"lon\":\"x\"})\n",
    "ds = ds.sortby(ds.x).sortby(ds.y, ascending=False)\n",
    "ds.to_netcdf(rf\"{DATA_OUT}\\CCKP_heatwaves_yearly.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cws = {\n",
    "    \"fd20\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\frostdays_annual_2_std.nc\"), \n",
    "    \"fd25\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\frostdays_annual_25_std.nc\"), \n",
    "    \"id20\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\icedays_annual_2_std.nc\"), \n",
    "    \"id25\": xr.open_dataset(rf\"{DATA_PROC}\\heatwaves and coldwaves\\icedays_annual_25_std.nc\"),\n",
    "}\n",
    "\n",
    "for name, ds in cws.items():\n",
    "    ds = ds.rename({\"__xarray_dataarray_variable__\":name})\n",
    "    cws[name] = ds\n",
    "    \n",
    "ds = xr.combine_by_coords(list(cws.values()))\n",
    "ds = ds.rename({\"lat\":\"y\", \"lon\":\"x\"})\n",
    "ds = ds.sortby(ds.x).sortby(ds.y, ascending=False)\n",
    "ds.to_netcdf(rf\"{DATA_OUT}\\CCKP_coldwaves_yearly.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intense Rain, resolucion original (ERA5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shocks_by_year = []\n",
    "for shock in [\"rx1day\", \"rx5day\"]:\n",
    "    file = f\"timeseries-{shock}-monthly-mean_era_monthly_era5-0.5x0.5-timeseries_mean_1950-2020.nc\"\n",
    "    ds = xr.open_dataset(rf\"{DATA_RAW}\\ERA5_CCKP\\{file}\", chunks={\"time\": 60})\n",
    "    ds = ds.sel(bnds=0) # both bands are the same\n",
    "    ds = ds.drop_vars([\"lon_bnds\", \"lat_bnds\", \"bnds\"])\n",
    "    for rainfall in [100, 200, 300, 400, 500, 600]:\n",
    "        ds_shock = (ds[f\"timeseries-{shock}-monthly-mean\"] >= rainfall)\n",
    "        ds_shock = ds_shock.rename(f\"{shock}_{rainfall}\")\n",
    "        ds_year = ds_shock.groupby('time.year').max()\n",
    "        shocks_by_year += [ds_year]\n",
    "        \n",
    "full_ds = xr.combine_by_coords(shocks_by_year)\n",
    "full_ds = full_ds.rename({\"lat\":\"y\", \"lon\":\"x\"})\n",
    "full_ds = full_ds.sortby(full_ds.x).sortby(full_ds.y, ascending=False)\n",
    "full_ds.to_netcdf(rf\"{DATA_OUT}\\CCKP_intenserain_yearly.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
