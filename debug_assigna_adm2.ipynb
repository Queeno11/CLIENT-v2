{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To access Global Flood Database Data, run:\n",
    "# gsutil -m cp -r gs://gfd_v1_4 \"your\\local\\repository\"\n",
    "## Install gsutil first: https://cloud.google.com/storage/docs/gsutil_install\n",
    "## GFD repo is available at: https://github.com/cloudtostreet/MODIS_GlobalFloodDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dask.diagnostics import ProgressBar\n",
    "from geocube.api.core import make_geocube\n",
    "\n",
    "PATH = \"Z:\\Laboral\\World Bank\\CLIENT v2\"\n",
    "DATA_RAW = rf\"{PATH}\\Data\\Data_raw\"\n",
    "DATA_PROC = rf\"{PATH}\\Data\\Data_proc\"\n",
    "DATA_OUT = rf\"{PATH}\\Data\\Data_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xr.open_dataset(rf\"Z:\\WB Data\\Global Flood Database\\gfd_v1_4\\DFO_1972_From_20020630_to_20020723.tif\")\n",
    "masked = xr.where(test.sel(band=5)==1, 0, test.sel(band=1))\n",
    "masked.band_data.fillna(0).astype(bool).to_netcdf(\"test.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xr.open_dataset(rf\"Z:\\WB Data\\Global Flood Database\\gfd_v1_4\\DFO_1972_From_20020630_to_20020723.tif\")\n",
    "masked = xr.where(test.sel(band=5)==1, 0, test.sel(band=1))\n",
    "masked.band_data.to_netcdf(\"test2.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test.sel(band=1) - test.sel(band=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds_from_chunk_number(chunk_number, total_chunks=8):\n",
    "    \"\"\"Get the bounding box coordinates for a given chunk number.\n",
    "\n",
    "    Data is divided into total_chunks chunks, each covering an 1/total_chunks of the globe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    chunk_number: int\n",
    "        Chunk number < total_chunks.\n",
    "    total_chunks: int\n",
    "        Total number of chunks to divide the globe into.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: Bounding box coordinates (left, bottom, right, top).\n",
    "    \"\"\"\n",
    "\n",
    "    if chunk_number > total_chunks - 1:\n",
    "        raise ValueError(\"Chunk number must be less than total_chunks.\")\n",
    "\n",
    "    # Define the bounding box coordinates for each chunk\n",
    "    x_min = -180\n",
    "    x_max = 180\n",
    "    y_min = -90\n",
    "    y_max = 90\n",
    "\n",
    "    # Calculate the bounding box coordinates for the given chunk number\n",
    "    side_chunks = np.sqrt(total_chunks)\n",
    "    if not side_chunks.is_integer():\n",
    "        raise ValueError(\"Total chunks must be a square number.\")\n",
    "    side_chunks = int(side_chunks)\n",
    "\n",
    "    chunk_position = np.unravel_index(chunk_number, (side_chunks, side_chunks))\n",
    "\n",
    "    x_step = (x_max - x_min) / side_chunks\n",
    "    y_step = (y_max - y_min) / side_chunks\n",
    "\n",
    "    left = x_min + chunk_position[0] * x_step\n",
    "    right = left + x_step\n",
    "    bottom = y_min + chunk_position[1] * y_step\n",
    "    top = bottom + y_step\n",
    "\n",
    "    return (left, bottom, right, top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = 11\n",
    "year = 2002\n",
    "var = \"SPEI-12_1_0\"\n",
    "\n",
    "df = pd.read_parquet(rf\"Z:\\Laboral\\World Bank\\CLIENT v2\\Data\\Data_proc\\shocks\\drought_{var}_{year}_{chunk}_zonal_stats.parquet\")\n",
    "gdf = gpd.read_feather(r\"E:\\client_v2_data\\WB_country_IDs.feather\")\n",
    "merged = gdf.merge(df, left_on=\"ID\", right_index=True, validate=\"1:1\")\n",
    "\n",
    "ds = xr.open_dataset(r\"E:\\client_v2_data\\ERA5_droughts_1970-2021.nc\")\n",
    "chunk_bounds = get_bounds_from_chunk_number(chunk, 16)\n",
    "ds = ds.sel(x=slice(chunk_bounds[0], chunk_bounds[2]), y=slice(chunk_bounds[3], chunk_bounds[1]), year=year)\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "# merged.plot(column=\"area_affected\", ax=axs[1])\n",
    "merged.plot(column=\"area_affected\", ax=axs[0], legend=True, cmap=\"Spectral\")\n",
    "merged.plot(column=\"area_affected\", ax=axs[1], legend=True, cmap=\"Spectral\", alpha=0)\n",
    "merged.plot(column=\"area_affected\", ax=axs[2], legend=True, cmap=\"Spectral\")\n",
    "ds[f\"drought_{var}\"].plot(ax=axs[1], add_colorbar=False, cmap=\"Greys\")\n",
    "ds[f\"drought_{var}\"].plot(ax=axs[2], add_colorbar=False, cmap=\"Greys\", alpha=0.6)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(chunk_bounds[0], chunk_bounds[2])\n",
    "    ax.set_ylim(chunk_bounds[1], chunk_bounds[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged[\"ID\"]==13736]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.cx[-100:-90, 35:45].explore(column=\"area_affected\", cmap=\"Spectral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droughts = xr.open_dataset(rf\"E:/client_v2_data/ERA5_droughts_1970-2021.nc\")\n",
    "chunk_bounds = (-180, -90, 90, 0)\n",
    "\n",
    "chunk_droughts = droughts.sel(\n",
    "    x=slice(chunk_bounds[0], chunk_bounds[2]), y=slice(chunk_bounds[3], chunk_bounds[1])\n",
    ").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_droughts[\"drought_SPI-1_1_0\"].sel(year=1997).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5 = xr.open_dataset(rf\"{DATA_PROC}\\ERA5_monthly_1970-2021.nc\", chunks={'latitude': 100, 'longitude': 100, 'time': 5})\n",
    "era5 = era5.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "era5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_country_grid = xr.open_dataset(rf\"{DATA_PROC}\\WB_country_grid.nc\", chunks={'x': 100, 'y': 100})\n",
    "WB_country_grid\n",
    "\n",
    "# Filter these bounds 5.343009,40.639947,20.244439,50.052846\n",
    "WB_country_grid = WB_country_grid.sel(x=slice(5.343009, 10.244439), y=slice(50.052846, 45.639947))\n",
    "# era5 = era5.sel(x=slice(5.343009, 10.244439), y=slice(50.052846, 45.639947))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_resamp = era5.interp_like(WB_country_grid, method=\"nearest\")\n",
    "with ProgressBar():\n",
    "    era5_resamp = era5_resamp.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_resamp = era5_resamp.chunk({'x': 100, 'y': 100, 'time': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_yearly = era5_resamp[\"PET\"].groupby('time.year').mean()\n",
    "with ProgressBar():\n",
    "    era5_yearly = era5_yearly.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xrspatial\n",
    "import dask\n",
    "\n",
    "\n",
    "tasks = []\n",
    "era5_yearly = era5_yearly.chunk({\"x\":100, \"y\":100, \"year\":1})\n",
    "for year in range(1970, 2021):\n",
    "    era5_year = era5_yearly.sel(year=year).drop_vars(\"year\")\n",
    "    tasks += [xrspatial.zonal.stats(zones=WB_country_grid.ADM2_CODE, values=era5_year)]\n",
    "with ProgressBar():\n",
    "    results = dask.compute(*tasks)#.compute()\n",
    "# era5_resamp.PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {year: data.set_index(\"zone\") for year, data in zip(range(1970,2021), results)}\n",
    "df = pd.concat(out_dict)\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns={\"level_0\":\"year\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_country_grid.ADM2_CODE.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_xarray_by_poygon(dataset, polygon):\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "\n",
    "    dataset = dataset.sel(y=slice(maxy, miny), x=slice(minx, maxx))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_carray_mask_from_polygon(dataset, polygon):\n",
    "    import rasterio\n",
    "\n",
    "    arr = rasterio.features.geometry_mask(\n",
    "        [polygon],\n",
    "        out_shape=(len(dataset.y), len(dataset.x)),\n",
    "        transform=dataset.rio.transform(),\n",
    "        invert=True,\n",
    "    )\n",
    "    mask = xr.DataArray(\n",
    "        arr,\n",
    "        coords={\n",
    "            \"x\": dataset.x,\n",
    "            \"y\": dataset.y,\n",
    "        },\n",
    "        dims=[\"y\", \"x\"],\n",
    "    )\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floods = pd.read_csv(rf\"{DATA_RAW}\\Floods\\GloFAS_floods.csv\")\n",
    "population = xr.open_dataset(\n",
    "    r\"Z:\\Laboral\\World Bank\\Vulnerability Index\\data\\data_in\\Grided Population\\gpw_v4_population_count_rev11_2020_30_sec.tif\"\n",
    ")  # FIXME: Tendría que usar la del año correspondiente, no la de 2020\n",
    "population = population.sel(band=1).drop_vars([\"band\"])\n",
    "\n",
    "era5 = xr.open_dataset(\n",
    "    rf\"{DATA_OUT}\\ERA5_monthly_1970-2021_SPI-SPEI.nc\",\n",
    "    chunks={\"latitude\": 1000, \"longitude\": 1000},\n",
    ")\n",
    "era5 = era5.rename({\"latitude\": \"y\", \"longitude\": \"x\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_shp = gpd.read_file(rf\"{DATA_RAW}\\world_bank_adm2.zip\")\n",
    "wb_shp.loc[wb_shp.ADM2_NAME == \"Administrative unit not available\", \"ADM2_CODE\"] = (\n",
    "    np.nan\n",
    ")\n",
    "# Create ADM_LAST variable: ADM2_NAME if available, else ADM1_NAME\n",
    "wb_shp[\"ADM_LAST\"] = wb_shp.ADM2_NAME\n",
    "wb_shp.loc[wb_shp.ADM_LAST.isnull(), \"ADM_LAST\"] = wb_shp.ADM1_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = wb_shp[wb_shp.ADM0_CODE == 122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygon with the country shape\n",
    "country_polygon = country.unary_union\n",
    "\n",
    "# Keep only India from population and floods\n",
    "min_x, min_y, max_x, max_y = country.total_bounds\n",
    "population = population.sel(y=slice(max_y, min_y), x=slice(min_x, max_x))\n",
    "era5 = crop_xarray_by_poygon(era5, country_polygon)\n",
    "\n",
    "# # Interpolate like floods\n",
    "# era5_resamp = era5.interp_like(population, method=\"nearest\")\n",
    "# with ProgressBar():\n",
    "#     era5_resamp = era5_resamp.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.combine_by_coords([population, era5_resamp], combine_attrs=\"override\")\n",
    "ds = ds.sel(band=1).drop_vars([\"band\", \"spatial_ref\"])\n",
    "ds[\"x\"] = ds[\"x\"].astype(\"float32\")\n",
    "ds[\"y\"] = ds[\"y\"].astype(\"float32\")\n",
    "ds = ds.chunk({\"y\": 1000, \"x\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.chunk({\"y\": 1000, \"x\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_grid[[\"x\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize the country shapes\n",
    "country_grid = make_geocube(\n",
    "    vector_data=country[[\"ADM1_CODE\", \"ADM2_CODE\", \"ADM_LAST\", \"geometry\"]],\n",
    "    like=population,\n",
    ")\n",
    "# For some reason, like option is not working, so I have to manually add x and y\n",
    "assert (country_grid[\"x\"].shape == population[\"x\"].shape)\n",
    "assert (country_grid[\"y\"].shape == population[\"y\"].shape)\n",
    "country_grid[\"x\"] = population[\"x\"]\n",
    "country_grid[\"y\"] = population[\"y\"]\n",
    "\n",
    "# Combine the population and country grid\n",
    "ds_geocube = xr.combine_by_coords(\n",
    "    [\n",
    "        population,\n",
    "        country_grid,\n",
    "    ],\n",
    "    combine_attrs=\"override\",\n",
    ")\n",
    "ds_geocube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.band_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_adm = (\n",
    "    ds_geocube[\"band_data\"].groupby(ds_geocube[\"ADM2_CODE\"]).sum().to_dataframe()\n",
    ")\n",
    "joined = (\n",
    "    country[[\"ADM1_CODE\", \"ADM2_CODE\", \"ADM_LAST\", \"geometry\"]]\n",
    "    .set_index(\"ADM2_CODE\")\n",
    "    .join(pop_by_adm)\n",
    ")\n",
    "joined.explore(column=\"band_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regionmask\n",
    "\n",
    "# Create mask of multiple regions from shapefile\n",
    "eu_mask = regionmask.mask_3D_geopandas(\n",
    "    country[[\"ADM2_CODE\", \"geometry\"]].dropna(subset=\"ADM2_CODE\"),\n",
    "    ds.x,\n",
    "    ds.y,\n",
    "    drop=False,\n",
    "    # numbers=\"ADM2_CODE\"\n",
    ")\n",
    "# Apply mask on our dataset\n",
    "test = ds[\"SPI-1\"].where(eu_mask)\n",
    "test = test.chunk({\"x\": 20, \"y\": 20, \"time\": 1, \"region\": 1})\n",
    "\n",
    "with ProgressBar():\n",
    "    agg_by_time = (\n",
    "        test.groupby(\"region\")\n",
    "        .mean(dim=[\"x\", \"y\"], method=\"map-reduce\", engine=\"flox\")\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    agg_by_time = test.groupby(\"region\").mean(dim=[\"x\", \"y\"]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isel(region=25).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(ADM1_CODE=40544)[\"SPI-1\"].isel(time=30).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(ADM1_CODE=40542)[\"SPI-1\"].isel(time=30).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupby(\"ADM2_CODE\")[\"SPEI-12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[[\"SPI-1\", \"ADM2_CODE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sel(band=1).drop([\"spatial_ref\", \"band\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, let's say ADM1 is a 2D array with same x, y dimensions\n",
    "adm1_dim = ds.ADM1_CODE\n",
    "\n",
    "# Create a new dimension 'adm1' for ADM1 levels\n",
    "dataset = ds.expand_dims(dim=\"ADM1_level\", axis=2)\n",
    "\n",
    "# Assign precipitation data to this new dataset structure\n",
    "dataset[\"SPI-1\"] = dataset[\"SPI-1\"].assign_coords(ADM1_level=adm1_dim)\n",
    "\n",
    "# Now your dataset has 'x', 'y', and 'ADM1' as dimensions\n",
    "# (x, y, ADM1_level) structure with precipitation values tagged with corresponding ADM1 codes\n",
    "\n",
    "# Optionally, if the ADM1 codes are unique and you want to have them as a coordinate dimension:\n",
    "adm1_unique = adm1_dim.values.flatten()\n",
    "adm1_unique = np.unique(adm1_unique)\n",
    "\n",
    "dataset = dataset.assign_coords(ADM1_level=(\"ADM1_level\", adm1_unique))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_flat = ds.stack(z=(\"y\", \"x\"))\n",
    "ds_flat.groupby(\"ADM1_CODE\")[\"SPI-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(time=30).groupby(\"ADM1_CODE\")  # [\"SPI-1\"]#.mean(dim=[\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds[\"SPI-1\"] * ds.band_data).groupby(\"ADM2_CODE\").sum(dim=(\"x\", \"y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is OLD\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def compute_share_affected_population(floods, population):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Calculate affected population for the selected municipality\n",
    "    population_affected = (floods_resamp * population).sum(dim=(\"x\", \"y\"))\n",
    "    area_affected = (floods).sum(dim=(\"x\", \"y\"))\n",
    "\n",
    "    # Calculate total population for the selected municipality\n",
    "    total_population = population.sum(dim=(\"x\", \"y\"))\n",
    "    total_area = floods.count(dim=(\"x\", \"y\"))\n",
    "\n",
    "    # Calculate share of population affected by floods\n",
    "    share_population_affected = (\n",
    "        population_affected / total_population\n",
    "    ).band_data.values[0]\n",
    "    share_area_affected = (area_affected / total_area).band_data.values[0]\n",
    "\n",
    "    # Return value\n",
    "    return share_population_affected, share_area_affected\n",
    "\n",
    "\n",
    "def plot_population_vs_floods(floods, population, polygon):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    floods.rio.clip([polygon]).band_data.plot(ax=ax[0])\n",
    "    population.rio.clip([polygon]).band_data.plot(ax=ax[1])\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Create emty dataframe to store results\n",
    "\n",
    "if KIND == \"DHS\":\n",
    "    admin_name_col = \"ADM2_NAME\"  # Esto solo si trabajamos con DHS desagregada! FIXME\n",
    "    admin_code_col = \"ADM2_CODE\"\n",
    "    country_map = country_map.dissolve(by=\"ADM2_CODE\").reset_index()\n",
    "elif KIND == \"IPUMS\":\n",
    "    admin_name_col = \"ADMIN_NAME\"\n",
    "    admin_code_col = \"GEOLEVEL2\"\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for i in tqdm(range(0, len(country_map))):\n",
    "\n",
    "    polygon = country_map.geometry[i]\n",
    "\n",
    "    # Crop xarray to reduce size\n",
    "    floods_min = crop_xarray_by_poygon(floods, polygon)\n",
    "    floods_resamp_min = crop_xarray_by_poygon(floods_resamp, polygon)\n",
    "    population_min = crop_xarray_by_poygon(population, polygon)\n",
    "\n",
    "    # Clip xarray by poluygon\n",
    "    floods_min = floods_min.where(create_carray_mask_from_polygon(floods_min, polygon))\n",
    "    floods_resamp_min = floods_resamp_min.where(\n",
    "        create_carray_mask_from_polygon(floods_resamp_min, polygon)\n",
    "    )\n",
    "    population_min = population_min.where(\n",
    "        create_carray_mask_from_polygon(population_min, polygon)\n",
    "    )\n",
    "\n",
    "    # plot_population_vs_floods(floods, population, polygon)\n",
    "    share_pop_affected, share_area_affected = compute_share_affected_population(\n",
    "        floods_min, floods_resamp_min, population_min\n",
    "    )\n",
    "\n",
    "    # add to dataframe\n",
    "    results.loc[i, admin_name_col] = country_map[admin_name_col][i]\n",
    "    results.loc[i, admin_code_col] = country_map[admin_code_col][i]\n",
    "    results.loc[i, \"POP_PERCENT_AFFECTED\"] = share_pop_affected * 100\n",
    "    results.loc[i, \"AREA_PERCENT_AFFECTED\"] = share_area_affected * 100\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    xr.combine_by_coords([population, era5_resamp])\n",
    "    .rename({\"band_data\": \"POPULATION\"})\n",
    "    .sel(band=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[\"drought_spi1_2\"] =\n",
    "ds[\"drought\"] = ds[\"SPI-1\"] < -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_per_municipality(ds, geom):\n",
    "    population_affected = (ds.drought * ds.POPULATION.where(geom)).sum(dim=(\"x\", \"y\"))\n",
    "    # area_affected = (ds..where(geom)).sum(dim=('x', 'y'))\n",
    "    # total_population = population_min.where(geom).sum(dim=('x', 'y'))\n",
    "    # total_area = ds.where(geom).count(dim=('x', 'y'))\n",
    "\n",
    "    # share_population_affected = (population_affected / total_population).data[0]\n",
    "    # share_area_affected = (area_affected / total_area).data[0]\n",
    "    return population_affected  # share_population_affected, share_area_affected\n",
    "\n",
    "\n",
    "calculate_per_municipality(ds, country.geometry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "# Assuming your geodataframe's geometry column is called 'geometry'\n",
    "def vectorized_calculation(ds, polygon):\n",
    "    # 1. Use xr.apply_ufunc with a custom function for municipality-wise calculation\n",
    "    def calculate_per_municipality(ds, geom):\n",
    "        population_affected = (ds.drought * population_min.where(geom)).sum(\n",
    "            dim=(\"x\", \"y\")\n",
    "        )\n",
    "        # area_affected = (ds..where(geom)).sum(dim=('x', 'y'))\n",
    "        # total_population = population_min.where(geom).sum(dim=('x', 'y'))\n",
    "        # total_area = ds.where(geom).count(dim=('x', 'y'))\n",
    "\n",
    "        # share_population_affected = (population_affected / total_population).data[0]\n",
    "        # share_area_affected = (area_affected / total_area).data[0]\n",
    "        return population_affected  # share_population_affected, share_area_affected\n",
    "\n",
    "    # Crop xarray to reduce size\n",
    "    ds = crop_xarray_by_poygon(ds, polygon)\n",
    "    ds = ds.where(create_carray_mask_from_polygon(ds, polygon))\n",
    "\n",
    "    # 2. Apply the function along the geometry dimension with vectorized operations\n",
    "    results = xr.apply_ufunc(ds, polygon, vectorize=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Call the function with the geodataframe geometry\n",
    "share_pop_affected, share_area_affected = vectorized_calculation(ds, country.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "results.PERCENT_AFFECTED.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population.to_netcdf(\"pop_india.nc\")ds.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "floods_resamp = floods_resamp.sortby([\"y\", \"x\", \"band\"])\n",
    "floods_resamp.rio.write_crs(\"epsg:4326\")\n",
    "floods_resamp.band_data.rio.to_raster(\"floods_india.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_treatment(india_map, results):\n",
    "\n",
    "    india_map.merge(results, on=\"ADM2_CODE\", how=\"inner\").plot(\n",
    "        column=\"TREATED\", legend=True\n",
    "    )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "plot_treatment(country_map, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
