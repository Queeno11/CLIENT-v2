{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To access Global Flood Database Data, run:\n",
    "# gsutil -m cp -r gs://gfd_v1_4 \"your\\local\\repository\"\n",
    "## Install gsutil first: https://cloud.google.com/storage/docs/gsutil_install\n",
    "## GFD repo is available at: https://github.com/cloudtostreet/MODIS_GlobalFloodDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\cupy\\_environment.py:216: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dask.diagnostics import ProgressBar\n",
    "from geocube.api.core import make_geocube\n",
    "\n",
    "PATH = \"D:\\World Bank\\CLIENT v2\"\n",
    "DATA_RAW = rf\"{PATH}\\Data\\Data_raw\"\n",
    "DATA_PROC = rf\"{PATH}\\Data\\Data_proc\"\n",
    "DATA_OUT = rf\"{PATH}\\Data\\Data_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_population_data(bounds=None, generate=False):\n",
    "    print(\"Processing Population data...\")\n",
    "\n",
    "    # Select all files in GPW folder\n",
    "    gpw_path = r\"Z:\\WB Data\\Gridded Population of The World (GPWv4)\"\n",
    "    files = os.listdir(gpw_path)\n",
    "    files = [f for f in files if f.endswith(\".tif\")]\n",
    "    \n",
    "    # Compile into a single dataset\n",
    "    dss = []\n",
    "    for f in tqdm(files):\n",
    "        \n",
    "        ds = xr.open_dataset(os.path.join(gpw_path, f), chunks={\"x\": 10000, \"y\": 10000})\n",
    "        ds[\"band_data\"] = ds[\"band_data\"].astype(np.uint32)\n",
    "        if bounds is not None:\n",
    "            ds = ds.sel(\n",
    "                x=slice(bounds[0], bounds[2]), y=slice(bounds[3], bounds[1])\n",
    "            )\n",
    "        if generate:\n",
    "            with ProgressBar():\n",
    "                ds.sel(band=1).drop_vars(\"band\").band_data.rio.to_raster(rf\"E:\\client_v2_data\\{f.replace('.tif','_proc.tif')}\")\n",
    "                print(f\"Saved {f.replace('.tif','_proc.tif')}\")\n",
    "        \n",
    "        ds[\"year\"] = int(f.split(\"_\")[5])\n",
    "        ds = ds.set_coords('year')\n",
    "        dss += [ds]\n",
    "        \n",
    "    population = xr.concat(dss, dim=\"year\")    \n",
    "    \n",
    "    # Filter if bounds are provided\n",
    "    if bounds is not None:\n",
    "        population = population.sel(\n",
    "            x=slice(bounds[0], bounds[2]), y=slice(bounds[3], bounds[1])\n",
    "        )\n",
    "        \n",
    "    # Clean band dimension\n",
    "    population = population.sel(band=1).drop_vars([\"band\"])\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    return population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Population data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:80: RuntimeWarning: Engine 'cfgrib' loading failed:\n",
      "Cannot find the ecCodes library\n",
      "  warnings.warn(f\"Engine {name!r} loading failed:\\n{ex}\", RuntimeWarning)\n",
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      " 20%|██        | 1/5 [00:03<00:12,  3.12s/it]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\backends\\plugins.py:159: RuntimeWarning: 'ee' fails while guessing\n",
      "  warnings.warn(f\"{engine!r} fails while guessing\", RuntimeWarning)\n",
      "100%|██████████| 5/5 [00:03<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pop = load_population_data(bounds=None, generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files checked: 913\n",
      "Files containing 'INDIA': 149\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re \n",
    "from datetime import datetime\n",
    "\n",
    "def extract_number_from_filename(filename):\n",
    "    # Define the regex pattern to extract the number\n",
    "    match = re.search(r'DFO_(\\d+)_', filename)\n",
    "    if match:\n",
    "        assert len(match.groups()) == 1\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def extract_year_from_filename(filename):\n",
    "    import re\n",
    "\n",
    "    year = None\n",
    "    # Define a regular expression pattern to match the dates\n",
    "    pattern = r'From_(\\d{8})_to_(\\d{8})'\n",
    "\n",
    "    # Use re.search to find the dates in the string\n",
    "    match = re.search(pattern, filename)\n",
    "\n",
    "    if match:\n",
    "        # Extract the dates from the matched groups\n",
    "        date_from = match.group(1)\n",
    "        date_to = match.group(2)\n",
    "        \n",
    "        # Convert to a more readable format if needed\n",
    "        # Convert strings to datetime.date objects\n",
    "        date_from_dt = datetime.strptime(date_from, r'%Y%m%d').date()\n",
    "        date_to_dt = datetime.strptime(date_to, r'%Y%m%d').date()\n",
    "\n",
    "        # Calculate the average date\n",
    "        average_date = date_from_dt + (date_to_dt - date_from_dt) / 2\n",
    "        average_date = average_date.strftime(r'%Y-%m-%d')\n",
    "        year = average_date.split(\"-\")[0]\n",
    "    else:\n",
    "        print(\"Dates not found in the string.\")\n",
    "\n",
    "    return year\n",
    "\n",
    "\n",
    "def check_word_in_json_files(directory, word):\n",
    "    word = word.upper()\n",
    "    files_checked = 0\n",
    "    files_containing_word = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.json'):\n",
    "            files_checked += 1\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            with open(filepath, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # Check if the word is in any of the values\n",
    "                for value in data.values():\n",
    "                    if isinstance(value, str) and word in value.upper():\n",
    "                        files_containing_word += [filepath]\n",
    "                        break\n",
    "                    elif isinstance(value, list) and any(word in str(item).upper() for item in value):\n",
    "                        files_containing_word += [filepath]\n",
    "                        break\n",
    "    \n",
    "    print(f\"Files checked: {files_checked}\")\n",
    "    print(f\"Files containing '{word_to_check}': {len(files_containing_word)}\")\n",
    "    \n",
    "    ids = [extract_number_from_filename(f) for f in files_containing_word]\n",
    "    return ids\n",
    "\n",
    "directory = r'Z:\\WB Data\\Global Flood Database\\gfd_v1_4'  # Replace with the path to your JSON files\n",
    "word_to_check = 'INDIA'\n",
    "containing = check_word_in_json_files(directory, word_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gdw_file(ds):\n",
    "    masked = xr.where(ds.sel(band=5)==1, 0, ds.sel(band=1))\n",
    "    masked[\"band_data\"] = masked[\"band_data\"].fillna(0).astype(bool)\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51/937 [00:06<01:29,  9.91it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:1622: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "  8%|▊         | 74/937 [00:09<01:44,  8.23it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:1622: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      " 14%|█▎        | 128/937 [00:15<01:27,  9.28it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:1622: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      " 55%|█████▌    | 517/937 [00:56<00:42,  9.81it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:1622: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      " 59%|█████▉    | 557/937 [01:00<00:36, 10.38it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:1622: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      " 76%|███████▋  | 715/937 [01:17<00:25,  8.86it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:1622: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      " 81%|████████▏ | 763/937 [01:21<00:21,  8.21it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:1622: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      " 95%|█████████▌| 891/937 [01:34<00:04, 11.15it/s]c:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:1622: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  value = value[(slice(None),) * axis + (subkey,)]\n",
      "100%|██████████| 937/937 [01:38<00:00,  9.48it/s]\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(directory)\n",
    "files = [f for f in files if f.endswith(\".tif\")]\n",
    "# Keep only those files that contain the id in the filename\n",
    "# files = [f for f in files if extract_number_from_filename(f) in containing]\n",
    "# assert len(files) == len(containing)\n",
    "\n",
    "pop_to_fill = xr.zeros_like(pop.sel(year=2000))\n",
    "\n",
    "tasks = []\n",
    "for i, f in tqdm(enumerate(files), total=len(files)):\n",
    "    year = extract_year_from_filename(f)\n",
    "    shockid = extract_number_from_filename(f)\n",
    "    ds = xr.open_dataset(os.path.join(directory, f), engine=\"rasterio\", chunks={\"x\": 500, \"y\": 500})\n",
    "    ds = process_gdw_file(ds)\n",
    "    # x_bounds = slice(ds.x.min(), ds.x.max())\n",
    "    # y_bounds = slice(ds.y.max(), ds.y.min())\n",
    "    # pop_slice = pop.sel(year=2000, x=x_bounds, y=y_bounds)\n",
    "    ds_aggregated = ds.interp_like(pop_to_fill, method=\"linear\")\n",
    "    ds_aggregated = ds_aggregated.assign_coords({\"year\": year})\n",
    "    ds_aggregated = ds_aggregated.assign_coords({\"id\": shockid})\n",
    "    ds_aggregated = ds_aggregated.to_netcdf(os.path.join(DATA_PROC, \"GFD\", f\"{f.replace('.tif', f'_proc.nc')}\"), compute=False)\n",
    "    tasks += [ds_aggregated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_regular_grid(ds):\n",
    "    # Assuming the coordinates are named 'lat' and 'lon'\n",
    "    lat = ds.coords['y'].values\n",
    "    lon = ds.coords['x'].values\n",
    "    \n",
    "    # Check if the latitude and longitude differences are uniform\n",
    "    lat_diff = np.diff(lat)\n",
    "    lon_diff = np.diff(lon)\n",
    "    \n",
    "    # A grid is regular if the differences are all the same\n",
    "    is_lat_regular = np.allclose(lat_diff, lat_diff[0])\n",
    "    is_lon_regular = np.allclose(lon_diff, lon_diff[0])\n",
    "    \n",
    "    return is_lat_regular & is_lon_regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[###########################             ] | 69% Completed | 3hr 8mms\n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "Read or write failed. Z:/WB Data/Global Flood Database/gfd_v1_4/DFO_1921_From_20020507_to_20020606.tif, band 1: IReadBlock failed at X offset 10, Y offset 9: TIFFReadEncodedTile() failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCPLE_AppDefinedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32mrasterio\\_io.pyx:975\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase._read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\_io.pyx:213\u001b[0m, in \u001b[0;36mrasterio._io.io_multi_band\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\_err.pyx:195\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_int\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCPLE_AppDefinedError\u001b[0m: Z:/WB Data/Global Flood Database/gfd_v1_4/DFO_1921_From_20020507_to_20020606.tif, band 1: IReadBlock failed at X offset 10, Y offset 9: TIFFReadEncodedTile() failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProgressBar():\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:572\u001b[0m, in \u001b[0;36mImplicitToExplicitIndexingAdapter.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: np\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:575\u001b[0m, in \u001b[0;36mImplicitToExplicitIndexingAdapter.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:784\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:833\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 833\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:827\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:784\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:647\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m     array \u001b[38;5;241m=\u001b[39m apply_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey)\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;66;03m# If the array is not an ExplicitlyIndexedNDArrayMixin,\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;66;03m# it may wrap a BackendArray so use its __getitem__\u001b[39;00m\n\u001b[1;32m--> 647\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rioxarray\\_io.py:453\u001b[0m, in \u001b[0;36mRasterioArrayWrapper.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexingSupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\indexing.py:1011\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[1;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \n\u001b[0;32m    991\u001b[0m \u001b[38;5;124;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;124;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1010\u001b[0m raw_key, numpy_indices \u001b[38;5;241m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[1;32m-> 1011\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices\u001b[38;5;241m.\u001b[39mtuple:\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;66;03m# index the loaded np.ndarray\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m     indexable \u001b[38;5;241m=\u001b[39m NumpyIndexingAdapter(result)\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\rioxarray\\_io.py:430\u001b[0m, in \u001b[0;36mRasterioArrayWrapper._getitem\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    427\u001b[0m     riods \u001b[38;5;241m=\u001b[39m _ensure_warped_vrt(\n\u001b[0;32m    428\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanager\u001b[38;5;241m.\u001b[39macquire(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvrt_params\n\u001b[0;32m    429\u001b[0m     )\n\u001b[1;32m--> 430\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mriods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mband_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unsigned_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    432\u001b[0m         out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unsigned_dtype)\n",
      "File \u001b[1;32mrasterio\\_io.pyx:651\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mrasterio\\_io.pyx:978\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase._read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mRasterioIOError\u001b[0m: Read or write failed. Z:/WB Data/Global Flood Database/gfd_v1_4/DFO_1921_From_20020507_to_20020606.tif, band 1: IReadBlock failed at X offset 10, Y offset 9: TIFFReadEncodedTile() failed."
     ]
    }
   ],
   "source": [
    "import dask\n",
    "\n",
    "with ProgressBar():\n",
    "    dask.compute(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# dss = xr.align(*dss, join=\"outer\")\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ds_full \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Compute max affected per year\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ds_full \u001b[38;5;241m=\u001b[39m ds_full\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime.year\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\concat.py:276\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs, create_index_for_new_dim)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _dataarray_concat(\n\u001b[0;32m    264\u001b[0m         objs,\n\u001b[0;32m    265\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    273\u001b[0m         create_index_for_new_dim\u001b[38;5;241m=\u001b[39mcreate_index_for_new_dim,\n\u001b[0;32m    274\u001b[0m     )\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_obj, Dataset):\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dataset_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_index_for_new_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_index_for_new_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only concatenate xarray Dataset and DataArray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\concat.py:662\u001b[0m, in \u001b[0;36m_dataset_concat\u001b[1;34m(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs, create_index_for_new_dim)\u001b[0m\n\u001b[0;32m    660\u001b[0m         result_vars[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 662\u001b[0m     combined_var \u001b[38;5;241m=\u001b[39m \u001b[43mconcat_vars\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;66;03m# reindex if variable is not present in all datasets\u001b[39;00m\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(variable_index) \u001b[38;5;241m<\u001b[39m concat_index_size:\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\variable.py:2986\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(variables, dim, positions, shortcut, combine_attrs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IndexVariable\u001b[38;5;241m.\u001b[39mconcat(variables, dim, positions, shortcut, combine_attrs)\n\u001b[0;32m   2985\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshortcut\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\variable.py:1737\u001b[0m, in \u001b[0;36mVariable.concat\u001b[1;34m(cls, variables, dim, positions, shortcut, combine_attrs)\u001b[0m\n\u001b[0;32m   1735\u001b[0m axis \u001b[38;5;241m=\u001b[39m first_var\u001b[38;5;241m.\u001b[39mget_axis_num(dim)\n\u001b[0;32m   1736\u001b[0m dims \u001b[38;5;241m=\u001b[39m first_var_dims\n\u001b[1;32m-> 1737\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mduck_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m positions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1739\u001b[0m     \u001b[38;5;66;03m# TODO: deprecate this option -- we don't need it for groupby\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m     \u001b[38;5;66;03m# any more.\u001b[39;00m\n\u001b[0;32m   1741\u001b[0m     indices \u001b[38;5;241m=\u001b[39m nputils\u001b[38;5;241m.\u001b[39minverse_permutation(np\u001b[38;5;241m.\u001b[39mconcatenate(positions))\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\core\\duck_array_ops.py:368\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(arrays, axis)\u001b[0m\n\u001b[0;32m    366\u001b[0m     xp \u001b[38;5;241m=\u001b[39m get_array_namespace(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39mconcat(as_shared_dtype(arrays, xp\u001b[38;5;241m=\u001b[39mxp), axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m--> 368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_shared_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\array\\core.py:1753\u001b[0m, in \u001b[0;36mArray.__array_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_keyword(da_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlike\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1751\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlike\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m-> 1753\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mda_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\array\\core.py:4300\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(seq, axis, allow_unknown_chunksizes)\u001b[0m\n\u001b[0;32m   4297\u001b[0m     ind[axis] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   4299\u001b[0m uc_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(concat(\u001b[38;5;28mzip\u001b[39m(seq2, inds)))\n\u001b[1;32m-> 4300\u001b[0m _, seq2 \u001b[38;5;241m=\u001b[39m \u001b[43munify_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43muc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   4302\u001b[0m bds \u001b[38;5;241m=\u001b[39m [a\u001b[38;5;241m.\u001b[39mchunks \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m seq2]\n\u001b[0;32m   4304\u001b[0m chunks \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4305\u001b[0m     seq2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchunks[:axis]\n\u001b[0;32m   4306\u001b[0m     \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28msum\u001b[39m((bd[axis] \u001b[38;5;28;01mfor\u001b[39;00m bd \u001b[38;5;129;01min\u001b[39;00m bds), ()),)\n\u001b[0;32m   4307\u001b[0m     \u001b[38;5;241m+\u001b[39m seq2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mchunks[axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]\n\u001b[0;32m   4308\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\array\\core.py:3995\u001b[0m, in \u001b[0;36munify_chunks\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3986\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m   3987\u001b[0m     chunkss[j]\n\u001b[0;32m   3988\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mshape[n] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3992\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(i)\n\u001b[0;32m   3993\u001b[0m )\n\u001b[0;32m   3994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;241m!=\u001b[39m a\u001b[38;5;241m.\u001b[39mchunks \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(a\u001b[38;5;241m.\u001b[39mchunks):\n\u001b[1;32m-> 3995\u001b[0m     arrays\u001b[38;5;241m.\u001b[39mappend(\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrechunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   3996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3997\u001b[0m     arrays\u001b[38;5;241m.\u001b[39mappend(a)\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\array\\core.py:2763\u001b[0m, in \u001b[0;36mArray.rechunk\u001b[1;34m(self, chunks, threshold, block_size_limit, balance, method)\u001b[0m\n\u001b[0;32m   2753\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert blocks in dask array x for new chunks.\u001b[39;00m\n\u001b[0;32m   2754\u001b[0m \n\u001b[0;32m   2755\u001b[0m \u001b[38;5;124;03mRefer to :func:`dask.array.rechunk` for full documentation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2759\u001b[0m \u001b[38;5;124;03mdask.array.rechunk : equivalent function\u001b[39;00m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrechunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rechunk  \u001b[38;5;66;03m# avoid circular import\u001b[39;00m\n\u001b[1;32m-> 2763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrechunk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbalance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\array\\rechunk.py:371\u001b[0m, in \u001b[0;36mrechunk\u001b[1;34m(x, chunks, threshold, block_size_limit, balance, method)\u001b[0m\n\u001b[0;32m    367\u001b[0m     steps \u001b[38;5;241m=\u001b[39m plan_rechunk(\n\u001b[0;32m    368\u001b[0m         x\u001b[38;5;241m.\u001b[39mchunks, chunks, x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mitemsize, threshold, block_size_limit\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m--> 371\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43m_compute_rechunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp2p\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\array\\rechunk.py:681\u001b[0m, in \u001b[0;36m_compute_rechunk\u001b[1;34m(x, chunks)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;66;03m# Iterate over all new blocks\u001b[39;00m\n\u001b[0;32m    679\u001b[0m new_index \u001b[38;5;241m=\u001b[39m product(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(c)) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chunks))\n\u001b[1;32m--> 681\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m new_idx, cross1 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(new_index, crossed):\n\u001b[0;32m    682\u001b[0m     key \u001b[38;5;241m=\u001b[39m (merge_name,) \u001b[38;5;241m+\u001b[39m new_idx\n\u001b[0;32m    683\u001b[0m     old_block_indices \u001b[38;5;241m=\u001b[39m [[cr[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m cr \u001b[38;5;129;01min\u001b[39;00m cross1] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ndim)]\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\dask\\array\\rechunk.py:237\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03mMake dask.array slices as intersection of old and new chunks.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;124;03m    block sizes along each dimension (converts to new_chunks)\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m cross1 \u001b[38;5;241m=\u001b[39m product(\u001b[38;5;241m*\u001b[39mold_to_new(old_chunks, new_chunks))\n\u001b[1;32m--> 237\u001b[0m cross \u001b[38;5;241m=\u001b[39m chain(\u001b[38;5;28mtuple\u001b[39m(product(\u001b[38;5;241m*\u001b[39mcr)) \u001b[38;5;28;01mfor\u001b[39;00m cr \u001b[38;5;129;01min\u001b[39;00m cross1)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cross\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dss = xr.align(*dss, join=\"outer\")\n",
    "ds_full = xr.concat(dss, dim=\"time\")\n",
    "# Compute max affected per year\n",
    "ds_full = ds_full.groupby('time.year').mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    ds_full.to_netcdf(rf\"{DATA_PROC}\\gfd_v1_4_max.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for ds in out:\n",
    "    print(is_regular_grid(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = xr.open_dataset(rf\"Z:\\WB Data\\Global Flood Database\\gfd_v1_4\\DFO_1972_From_20020630_to_20020723.tif\")\n",
    "masked = xr.where(test.sel(band=5)==1, 0, test.sel(band=1))\n",
    "masked.band_data.to_netcdf(\"test2.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test.sel(band=1) - test.sel(band=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds_from_chunk_number(chunk_number, total_chunks=8):\n",
    "    \"\"\"Get the bounding box coordinates for a given chunk number.\n",
    "\n",
    "    Data is divided into total_chunks chunks, each covering an 1/total_chunks of the globe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    chunk_number: int\n",
    "        Chunk number < total_chunks.\n",
    "    total_chunks: int\n",
    "        Total number of chunks to divide the globe into.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: Bounding box coordinates (left, bottom, right, top).\n",
    "    \"\"\"\n",
    "\n",
    "    if chunk_number > total_chunks - 1:\n",
    "        raise ValueError(\"Chunk number must be less than total_chunks.\")\n",
    "\n",
    "    # Define the bounding box coordinates for each chunk\n",
    "    x_min = -180\n",
    "    x_max = 180\n",
    "    y_min = -90\n",
    "    y_max = 90\n",
    "\n",
    "    # Calculate the bounding box coordinates for the given chunk number\n",
    "    side_chunks = np.sqrt(total_chunks)\n",
    "    if not side_chunks.is_integer():\n",
    "        raise ValueError(\"Total chunks must be a square number.\")\n",
    "    side_chunks = int(side_chunks)\n",
    "\n",
    "    chunk_position = np.unravel_index(chunk_number, (side_chunks, side_chunks))\n",
    "\n",
    "    x_step = (x_max - x_min) / side_chunks\n",
    "    y_step = (y_max - y_min) / side_chunks\n",
    "\n",
    "    left = x_min + chunk_position[0] * x_step\n",
    "    right = left + x_step\n",
    "    bottom = y_min + chunk_position[1] * y_step\n",
    "    top = bottom + y_step\n",
    "\n",
    "    return (left, bottom, right, top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\World Bank\\\\CLIENT v2\\\\Data\\\\Data_proc\\\\shocks\\\\drought_SPEI-12_1_0_2002_11_zonal_stats.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m year \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2002\u001b[39m\n\u001b[0;32m      3\u001b[0m var \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPEI-12_1_0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mWorld Bank\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mCLIENT v2\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mData_proc\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mshocks\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdrought_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvar\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mchunk\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_zonal_stats.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m gdf \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_feather(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mE:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mclient_v2_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWB_country_IDs.feather\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m merged \u001b[38;5;241m=\u001b[39m gdf\u001b[38;5;241m.\u001b[39mmerge(df, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1:1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ofici\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\World Bank\\\\CLIENT v2\\\\Data\\\\Data_proc\\\\shocks\\\\drought_SPEI-12_1_0_2002_11_zonal_stats.parquet'"
     ]
    }
   ],
   "source": [
    "chunk = 11\n",
    "year = 2002\n",
    "var = \"flooded\"\n",
    "\n",
    "df = pd.read_parquet(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\shocks\\floods_{var}_{year}_{chunk}_zonal_stats.parquet\")\n",
    "gdf = gpd.read_feather(r\"E:\\client_v2_data\\WB_country_IDs.feather\")\n",
    "merged = gdf.merge(df, left_on=\"ID\", right_index=True, validate=\"1:1\")\n",
    "\n",
    "ds = xr.open_dataset(r\"E:\\client_v2_data\\ERA5_droughts_1970-2021.nc\")\n",
    "chunk_bounds = get_bounds_from_chunk_number(chunk, 16)\n",
    "ds = ds.sel(x=slice(chunk_bounds[0], chunk_bounds[2]), y=slice(chunk_bounds[3], chunk_bounds[1]), year=year)\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 15))\n",
    "\n",
    "# merged.plot(column=\"area_affected\", ax=axs[1])\n",
    "merged.plot(column=\"area_affected\", ax=axs[0], legend=True, cmap=\"Spectral\")\n",
    "merged.plot(column=\"area_affected\", ax=axs[1], legend=True, cmap=\"Spectral\", alpha=0)\n",
    "merged.plot(column=\"area_affected\", ax=axs[2], legend=True, cmap=\"Spectral\")\n",
    "ds[f\"drought_{var}\"].plot(ax=axs[1], add_colorbar=False, cmap=\"Greys\")\n",
    "ds[f\"drought_{var}\"].plot(ax=axs[2], add_colorbar=False, cmap=\"Greys\", alpha=0.6)\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim(chunk_bounds[0], chunk_bounds[2])\n",
    "    ax.set_ylim(chunk_bounds[1], chunk_bounds[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[merged[\"ID\"]==13736]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.cx[-100:-90, 35:45].explore(column=\"area_affected\", cmap=\"Spectral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "droughts = xr.open_dataset(rf\"E:/client_v2_data/ERA5_droughts_1970-2021.nc\")\n",
    "chunk_bounds = (-180, -90, 90, 0)\n",
    "\n",
    "chunk_droughts = droughts.sel(\n",
    "    x=slice(chunk_bounds[0], chunk_bounds[2]), y=slice(chunk_bounds[3], chunk_bounds[1])\n",
    ").load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_droughts[\"drought_SPI-1_1_0\"].sel(year=1997).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5 = xr.open_dataset(rf\"{DATA_PROC}\\ERA5_monthly_1970-2021.nc\", chunks={'latitude': 100, 'longitude': 100, 'time': 5})\n",
    "era5 = era5.rename({'latitude': 'y', 'longitude': 'x'})\n",
    "era5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_country_grid = xr.open_dataset(rf\"{DATA_PROC}\\WB_country_grid.nc\", chunks={'x': 100, 'y': 100})\n",
    "WB_country_grid\n",
    "\n",
    "# Filter these bounds 5.343009,40.639947,20.244439,50.052846\n",
    "WB_country_grid = WB_country_grid.sel(x=slice(5.343009, 10.244439), y=slice(50.052846, 45.639947))\n",
    "# era5 = era5.sel(x=slice(5.343009, 10.244439), y=slice(50.052846, 45.639947))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_resamp = era5.interp_like(WB_country_grid, method=\"nearest\")\n",
    "with ProgressBar():\n",
    "    era5_resamp = era5_resamp.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_resamp = era5_resamp.chunk({'x': 100, 'y': 100, 'time': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_yearly = era5_resamp[\"PET\"].groupby('time.year').mean()\n",
    "with ProgressBar():\n",
    "    era5_yearly = era5_yearly.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xrspatial\n",
    "import dask\n",
    "\n",
    "\n",
    "tasks = []\n",
    "era5_yearly = era5_yearly.chunk({\"x\":100, \"y\":100, \"year\":1})\n",
    "for year in range(1970, 2021):\n",
    "    era5_year = era5_yearly.sel(year=year).drop_vars(\"year\")\n",
    "    tasks += [xrspatial.zonal.stats(zones=WB_country_grid.ADM2_CODE, values=era5_year)]\n",
    "with ProgressBar():\n",
    "    results = dask.compute(*tasks)#.compute()\n",
    "# era5_resamp.PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dict = {year: data.set_index(\"zone\") for year, data in zip(range(1970,2021), results)}\n",
    "df = pd.concat(out_dict)\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns={\"level_0\":\"year\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB_country_grid.ADM2_CODE.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_xarray_by_poygon(dataset, polygon):\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "\n",
    "    dataset = dataset.sel(y=slice(maxy, miny), x=slice(minx, maxx))\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_carray_mask_from_polygon(dataset, polygon):\n",
    "    import rasterio\n",
    "\n",
    "    arr = rasterio.features.geometry_mask(\n",
    "        [polygon],\n",
    "        out_shape=(len(dataset.y), len(dataset.x)),\n",
    "        transform=dataset.rio.transform(),\n",
    "        invert=True,\n",
    "    )\n",
    "    mask = xr.DataArray(\n",
    "        arr,\n",
    "        coords={\n",
    "            \"x\": dataset.x,\n",
    "            \"y\": dataset.y,\n",
    "        },\n",
    "        dims=[\"y\", \"x\"],\n",
    "    )\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floods = pd.read_csv(rf\"{DATA_RAW}\\Floods\\GloFAS_floods.csv\")\n",
    "population = xr.open_dataset(\n",
    "    r\"Z:\\Laboral\\World Bank\\Vulnerability Index\\data\\data_in\\Grided Population\\gpw_v4_population_count_rev11_2020_30_sec.tif\"\n",
    ")  # FIXME: Tendría que usar la del año correspondiente, no la de 2020\n",
    "population = population.sel(band=1).drop_vars([\"band\"])\n",
    "\n",
    "era5 = xr.open_dataset(\n",
    "    rf\"{DATA_OUT}\\ERA5_monthly_1970-2021_SPI-SPEI.nc\",\n",
    "    chunks={\"latitude\": 1000, \"longitude\": 1000},\n",
    ")\n",
    "era5 = era5.rename({\"latitude\": \"y\", \"longitude\": \"x\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_shp = gpd.read_file(rf\"{DATA_RAW}\\world_bank_adm2.zip\")\n",
    "wb_shp.loc[wb_shp.ADM2_NAME == \"Administrative unit not available\", \"ADM2_CODE\"] = (\n",
    "    np.nan\n",
    ")\n",
    "# Create ADM_LAST variable: ADM2_NAME if available, else ADM1_NAME\n",
    "wb_shp[\"ADM_LAST\"] = wb_shp.ADM2_NAME\n",
    "wb_shp.loc[wb_shp.ADM_LAST.isnull(), \"ADM_LAST\"] = wb_shp.ADM1_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = wb_shp[wb_shp.ADM0_CODE == 122]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygon with the country shape\n",
    "country_polygon = country.unary_union\n",
    "\n",
    "# Keep only India from population and floods\n",
    "min_x, min_y, max_x, max_y = country.total_bounds\n",
    "population = population.sel(y=slice(max_y, min_y), x=slice(min_x, max_x))\n",
    "era5 = crop_xarray_by_poygon(era5, country_polygon)\n",
    "\n",
    "# # Interpolate like floods\n",
    "# era5_resamp = era5.interp_like(population, method=\"nearest\")\n",
    "# with ProgressBar():\n",
    "#     era5_resamp = era5_resamp.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.combine_by_coords([population, era5_resamp], combine_attrs=\"override\")\n",
    "ds = ds.sel(band=1).drop_vars([\"band\", \"spatial_ref\"])\n",
    "ds[\"x\"] = ds[\"x\"].astype(\"float32\")\n",
    "ds[\"y\"] = ds[\"y\"].astype(\"float32\")\n",
    "ds = ds.chunk({\"y\": 1000, \"x\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.chunk({\"y\": 1000, \"x\": 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_grid[[\"x\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize the country shapes\n",
    "country_grid = make_geocube(\n",
    "    vector_data=country[[\"ADM1_CODE\", \"ADM2_CODE\", \"ADM_LAST\", \"geometry\"]],\n",
    "    like=population,\n",
    ")\n",
    "# For some reason, like option is not working, so I have to manually add x and y\n",
    "assert (country_grid[\"x\"].shape == population[\"x\"].shape)\n",
    "assert (country_grid[\"y\"].shape == population[\"y\"].shape)\n",
    "country_grid[\"x\"] = population[\"x\"]\n",
    "country_grid[\"y\"] = population[\"y\"]\n",
    "\n",
    "# Combine the population and country grid\n",
    "ds_geocube = xr.combine_by_coords(\n",
    "    [\n",
    "        population,\n",
    "        country_grid,\n",
    "    ],\n",
    "    combine_attrs=\"override\",\n",
    ")\n",
    "ds_geocube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population.band_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_by_adm = (\n",
    "    ds_geocube[\"band_data\"].groupby(ds_geocube[\"ADM2_CODE\"]).sum().to_dataframe()\n",
    ")\n",
    "joined = (\n",
    "    country[[\"ADM1_CODE\", \"ADM2_CODE\", \"ADM_LAST\", \"geometry\"]]\n",
    "    .set_index(\"ADM2_CODE\")\n",
    "    .join(pop_by_adm)\n",
    ")\n",
    "joined.explore(column=\"band_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regionmask\n",
    "\n",
    "# Create mask of multiple regions from shapefile\n",
    "eu_mask = regionmask.mask_3D_geopandas(\n",
    "    country[[\"ADM2_CODE\", \"geometry\"]].dropna(subset=\"ADM2_CODE\"),\n",
    "    ds.x,\n",
    "    ds.y,\n",
    "    drop=False,\n",
    "    # numbers=\"ADM2_CODE\"\n",
    ")\n",
    "# Apply mask on our dataset\n",
    "test = ds[\"SPI-1\"].where(eu_mask)\n",
    "test = test.chunk({\"x\": 20, \"y\": 20, \"time\": 1, \"region\": 1})\n",
    "\n",
    "with ProgressBar():\n",
    "    agg_by_time = (\n",
    "        test.groupby(\"region\")\n",
    "        .mean(dim=[\"x\", \"y\"], method=\"map-reduce\", engine=\"flox\")\n",
    "        .compute()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ProgressBar():\n",
    "    agg_by_time = test.groupby(\"region\").mean(dim=[\"x\", \"y\"]).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isel(region=25).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(ADM1_CODE=40544)[\"SPI-1\"].isel(time=30).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(ADM1_CODE=40542)[\"SPI-1\"].isel(time=30).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.groupby(\"ADM2_CODE\")[\"SPEI-12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[[\"SPI-1\", \"ADM2_CODE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.sel(band=1).drop([\"spatial_ref\", \"band\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, let's say ADM1 is a 2D array with same x, y dimensions\n",
    "adm1_dim = ds.ADM1_CODE\n",
    "\n",
    "# Create a new dimension 'adm1' for ADM1 levels\n",
    "dataset = ds.expand_dims(dim=\"ADM1_level\", axis=2)\n",
    "\n",
    "# Assign precipitation data to this new dataset structure\n",
    "dataset[\"SPI-1\"] = dataset[\"SPI-1\"].assign_coords(ADM1_level=adm1_dim)\n",
    "\n",
    "# Now your dataset has 'x', 'y', and 'ADM1' as dimensions\n",
    "# (x, y, ADM1_level) structure with precipitation values tagged with corresponding ADM1 codes\n",
    "\n",
    "# Optionally, if the ADM1 codes are unique and you want to have them as a coordinate dimension:\n",
    "adm1_unique = adm1_dim.values.flatten()\n",
    "adm1_unique = np.unique(adm1_unique)\n",
    "\n",
    "dataset = dataset.assign_coords(ADM1_level=(\"ADM1_level\", adm1_unique))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_flat = ds.stack(z=(\"y\", \"x\"))\n",
    "ds_flat.groupby(\"ADM1_CODE\")[\"SPI-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.isel(time=30).groupby(\"ADM1_CODE\")  # [\"SPI-1\"]#.mean(dim=[\"x\", \"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds[\"SPI-1\"] * ds.band_data).groupby(\"ADM2_CODE\").sum(dim=(\"x\", \"y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is OLD\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def compute_share_affected_population(floods, population):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Calculate affected population for the selected municipality\n",
    "    population_affected = (floods_resamp * population).sum(dim=(\"x\", \"y\"))\n",
    "    area_affected = (floods).sum(dim=(\"x\", \"y\"))\n",
    "\n",
    "    # Calculate total population for the selected municipality\n",
    "    total_population = population.sum(dim=(\"x\", \"y\"))\n",
    "    total_area = floods.count(dim=(\"x\", \"y\"))\n",
    "\n",
    "    # Calculate share of population affected by floods\n",
    "    share_population_affected = (\n",
    "        population_affected / total_population\n",
    "    ).band_data.values[0]\n",
    "    share_area_affected = (area_affected / total_area).band_data.values[0]\n",
    "\n",
    "    # Return value\n",
    "    return share_population_affected, share_area_affected\n",
    "\n",
    "\n",
    "def plot_population_vs_floods(floods, population, polygon):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    floods.rio.clip([polygon]).band_data.plot(ax=ax[0])\n",
    "    population.rio.clip([polygon]).band_data.plot(ax=ax[1])\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Create emty dataframe to store results\n",
    "\n",
    "if KIND == \"DHS\":\n",
    "    admin_name_col = \"ADM2_NAME\"  # Esto solo si trabajamos con DHS desagregada! FIXME\n",
    "    admin_code_col = \"ADM2_CODE\"\n",
    "    country_map = country_map.dissolve(by=\"ADM2_CODE\").reset_index()\n",
    "elif KIND == \"IPUMS\":\n",
    "    admin_name_col = \"ADMIN_NAME\"\n",
    "    admin_code_col = \"GEOLEVEL2\"\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for i in tqdm(range(0, len(country_map))):\n",
    "\n",
    "    polygon = country_map.geometry[i]\n",
    "\n",
    "    # Crop xarray to reduce size\n",
    "    floods_min = crop_xarray_by_poygon(floods, polygon)\n",
    "    floods_resamp_min = crop_xarray_by_poygon(floods_resamp, polygon)\n",
    "    population_min = crop_xarray_by_poygon(population, polygon)\n",
    "\n",
    "    # Clip xarray by poluygon\n",
    "    floods_min = floods_min.where(create_carray_mask_from_polygon(floods_min, polygon))\n",
    "    floods_resamp_min = floods_resamp_min.where(\n",
    "        create_carray_mask_from_polygon(floods_resamp_min, polygon)\n",
    "    )\n",
    "    population_min = population_min.where(\n",
    "        create_carray_mask_from_polygon(population_min, polygon)\n",
    "    )\n",
    "\n",
    "    # plot_population_vs_floods(floods, population, polygon)\n",
    "    share_pop_affected, share_area_affected = compute_share_affected_population(\n",
    "        floods_min, floods_resamp_min, population_min\n",
    "    )\n",
    "\n",
    "    # add to dataframe\n",
    "    results.loc[i, admin_name_col] = country_map[admin_name_col][i]\n",
    "    results.loc[i, admin_code_col] = country_map[admin_code_col][i]\n",
    "    results.loc[i, \"POP_PERCENT_AFFECTED\"] = share_pop_affected * 100\n",
    "    results.loc[i, \"AREA_PERCENT_AFFECTED\"] = share_area_affected * 100\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    xr.combine_by_coords([population, era5_resamp])\n",
    "    .rename({\"band_data\": \"POPULATION\"})\n",
    "    .sel(band=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[\"drought_spi1_2\"] =\n",
    "ds[\"drought\"] = ds[\"SPI-1\"] < -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_per_municipality(ds, geom):\n",
    "    population_affected = (ds.drought * ds.POPULATION.where(geom)).sum(dim=(\"x\", \"y\"))\n",
    "    # area_affected = (ds..where(geom)).sum(dim=('x', 'y'))\n",
    "    # total_population = population_min.where(geom).sum(dim=('x', 'y'))\n",
    "    # total_area = ds.where(geom).count(dim=('x', 'y'))\n",
    "\n",
    "    # share_population_affected = (population_affected / total_population).data[0]\n",
    "    # share_area_affected = (area_affected / total_area).data[0]\n",
    "    return population_affected  # share_population_affected, share_area_affected\n",
    "\n",
    "\n",
    "calculate_per_municipality(ds, country.geometry[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "\n",
    "# Assuming your geodataframe's geometry column is called 'geometry'\n",
    "def vectorized_calculation(ds, polygon):\n",
    "    # 1. Use xr.apply_ufunc with a custom function for municipality-wise calculation\n",
    "    def calculate_per_municipality(ds, geom):\n",
    "        population_affected = (ds.drought * population_min.where(geom)).sum(\n",
    "            dim=(\"x\", \"y\")\n",
    "        )\n",
    "        # area_affected = (ds..where(geom)).sum(dim=('x', 'y'))\n",
    "        # total_population = population_min.where(geom).sum(dim=('x', 'y'))\n",
    "        # total_area = ds.where(geom).count(dim=('x', 'y'))\n",
    "\n",
    "        # share_population_affected = (population_affected / total_population).data[0]\n",
    "        # share_area_affected = (area_affected / total_area).data[0]\n",
    "        return population_affected  # share_population_affected, share_area_affected\n",
    "\n",
    "    # Crop xarray to reduce size\n",
    "    ds = crop_xarray_by_poygon(ds, polygon)\n",
    "    ds = ds.where(create_carray_mask_from_polygon(ds, polygon))\n",
    "\n",
    "    # 2. Apply the function along the geometry dimension with vectorized operations\n",
    "    results = xr.apply_ufunc(ds, polygon, vectorize=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Call the function with the geodataframe geometry\n",
    "share_pop_affected, share_area_affected = vectorized_calculation(ds, country.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "results.PERCENT_AFFECTED.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population.to_netcdf(\"pop_india.nc\")ds.rio.write_crs(\"epsg:4326\", inplace=True)\n",
    "floods_resamp = floods_resamp.sortby([\"y\", \"x\", \"band\"])\n",
    "floods_resamp.rio.write_crs(\"epsg:4326\")\n",
    "floods_resamp.band_data.rio.to_raster(\"floods_india.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_treatment(india_map, results):\n",
    "\n",
    "    india_map.merge(results, on=\"ADM2_CODE\", how=\"inner\").plot(\n",
    "        column=\"TREATED\", legend=True\n",
    "    )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "plot_treatment(country_map, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
