{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_25864\\4087382191.py:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  PATH = \"D:\\World Bank\\CLIENT v2\"\n",
      "d:\\World Bank\\CLIENT v2\\procesa_bases.py:15: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  PATH = \"D:\\World Bank\\CLIENT v2\"\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import test_tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from procesa_bases import load_WB_country_data\n",
    "\n",
    "PATH = \"D:\\World Bank\\CLIENT v2\"\n",
    "DATA_RAW = rf\"{PATH}\\Data\\Data_raw\"\n",
    "DATA_PROC = rf\"{PATH}\\Data\\Data_proc\"\n",
    "DATA_OUT = rf\"{PATH}\\Data\\Data_out\"\n",
    "GPW_PATH = rf\"D:\\Datasets\\Gridded Population of the World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera mapa con etiquetas de zona (adm0 adm1 adm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ID dataset\n",
    "gdf = gpd.read_feather(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\WB_country_IDs.feather\")\n",
    "gdf.columns = gdf.columns.str.lower()\n",
    "gdf = gdf.rename(columns={\"id\":\"ID\"}).drop(columns=\"objectid\")\n",
    "\n",
    "# Add names from the original WB adm2 dataset\n",
    "gdf_raw = load_WB_country_data()\n",
    "gdf_raw.columns = gdf_raw.columns.str.lower()\n",
    "gdf_raw = gdf_raw[[\"adm0_code\", \"adm1_code\", \"adm2_code\", \"adm0_name\", \"adm1_name\", \"adm2_name\", \"geometry\"]]\n",
    "assert gdf_raw.duplicated(subset=[\"adm0_code\", \"adm1_code\", \"adm2_code\"]).sum() == 0, \"There are duplicated entries in the raw dataset!!\"\n",
    "\n",
    "# Merge both datasets to assert that the codes are correct and consistent\n",
    "gdf = gdf.merge(gdf_raw.drop(columns=\"geometry\"), how=\"outer\", on=[\"adm0_code\", \"adm1_code\", \"adm2_code\"], indicator=True, validate=\"1:1\")\n",
    "assert (gdf._merge == \"both\").all(), \"There are problems with the merge!!\"\n",
    "gdf = gdf.drop(columns=\"_merge\")\n",
    "\n",
    "gdf.drop(columns=\"ID\").to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_map.csv\", index=False) # Export without the ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set admin level to categorical dtype (when the dataset is expanded, it will be more memory efficient)\n",
    "gdf[\"ID\"]        = gdf[\"ID\"].astype(\"category\")\n",
    "gdf[\"adm0_code\"] = gdf[\"adm0_code\"].astype(\"category\")\n",
    "gdf[\"adm1_code\"] = gdf[\"adm1_code\"].astype(\"category\")\n",
    "gdf[\"adm2_code\"] = gdf[\"adm2_code\"].astype(\"category\")\n",
    "gdf = gdf.set_index(\"ID\")\n",
    "\n",
    "gdf = gdf.drop(columns=[\"adm0_name\",\"adm1_name\",\"adm2_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera datos de shocks clim√°ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_raw = gpd.read_file(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_raw\\world_bank_adm2\\world_bank_adm2.shp\")\n",
    "gdf_raw.columns = gdf_raw.columns.str.lower()\n",
    "gdf_raw = gdf_raw[[\"adm0_code\", \"adm1_code\", \"adm2_code\", \"adm0_name\", \"adm1_name\", \"adm2_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "def expand_dataset(df, gdf):\n",
    "                    \n",
    "    # Collect all dimension values from df\n",
    "    all_years      = df.index.get_level_values(\"year\").categories\n",
    "    all_variables  = df.index.get_level_values(\"variable\").categories\n",
    "    all_thresholds = df.index.get_level_values(\"threshold\").categories\n",
    "    all_measures   = df.index.get_level_values(\"measure\").categories\n",
    "    all_regions    = gdf.index.categories # ID is the index of gdf\n",
    "\n",
    "    # Convert each list to a small DataFrame\n",
    "    df_years      = pd.DataFrame({'year': all_years}, dtype='category')\n",
    "    df_variables  = pd.DataFrame({'variable': all_variables}, dtype='category')\n",
    "    df_thresholds = pd.DataFrame({'threshold': all_thresholds}, dtype='category')\n",
    "    df_measures   = pd.DataFrame({'measure': all_measures}, dtype='category')\n",
    "    df_regions    = pd.DataFrame({'ID': all_regions}, dtype='category')\n",
    "\n",
    "    # Step-by-step merges using how='cross'\n",
    "    df_temp = df_years.merge(df_variables, how='cross')\n",
    "    df_temp = df_temp.merge(df_regions, how='cross')\n",
    "    df_temp = df_temp.merge(df_thresholds, how='cross')\n",
    "    df_temp = df_temp.merge(df_measures, how='cross')\n",
    "    expanded_without_data = df_temp.set_index([\"ID\", \"year\", \"variable\", \"threshold\", \"measure\"])\n",
    "    \n",
    "    # add admcodes to the expanded set\n",
    "    expanded_without_data = expanded_without_data.join(\n",
    "        gdf.drop(columns=[\"geometry\"]),\n",
    "        how=\"left\",\n",
    "        on=\"ID\",\n",
    "        validate=\"m:1\"\n",
    "    )\n",
    "    \n",
    "    # Merge original data (df) onto the expanded set\n",
    "    expanded_with_data = expanded_without_data.join(\n",
    "        df,\n",
    "        how=\"left\",\n",
    "        validate=\"1:1\",\n",
    "        rsuffix=\"_y\"\n",
    "    ).reset_index().drop(columns=\"ID\")\n",
    "    \n",
    "    expanded_with_data = test_tools.assert_correct_admcodes(expanded_with_data)        \n",
    "\n",
    "    return expanded_with_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "## Generados Nico\n",
    "# Set dtypes to make this loading efficient\n",
    "dtypes = {\"year\": np.int16, \"variable\":\"category\", \"threshold\":\"category\", \"area_affected\":np.float32, \"population_affected\":np.float32, \"ID\":np.int64}# \"adm2_code\": np.int16, \"adm1_code\": np.int16, \"adm0_code\": np.int16,\n",
    "\n",
    "for shock in [\"floods\", \"drought\", \"hurricanes\", \"intenserain\", \"heatwaves\", \"coldwaves\"]:\n",
    "    print(shock)\n",
    "    df = pd.read_csv(\n",
    "        rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\WB_{shock}_long.csv\",\n",
    "        dtype=dtypes, \n",
    "        usecols=dtypes.keys(),\n",
    "    )\n",
    "        \n",
    "    # Set ID to categorical dtype (this is after loading as int to match with the categories of gdf)\n",
    "    df[\"ID\"] = df[\"ID\"].astype(\"category\")\n",
    "    \n",
    "    # Reshape to long format\n",
    "    df = df.melt(id_vars=[\"ID\", \"year\", \"variable\", \"threshold\"], var_name=\"measure\", value_name=\"value\")\n",
    "\n",
    "    # Set categorical and index to make faster merges\n",
    "    df[\"measure\"] = df[\"measure\"].astype(\"category\")\n",
    "    df[\"year\"] = df[\"year\"].astype(\"category\")\n",
    "    df = df.set_index([\"ID\"])    \n",
    "    \n",
    "    # Add adm0, adm1 and adm2 codes    \n",
    "    df = gdf.drop(columns=[\"geometry\"]).join(df, on=[\"ID\"], how=\"inner\", validate=\"1:m\")\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Set index to make faster merges and expand dataset\n",
    "    #   Replace columns with null categories with zeros before setting the index to make it work as expected:\n",
    "    index = [\"ID\", \"year\", \"variable\", \"threshold\", \"measure\"]\n",
    "    for col in index:\n",
    "        if (df[col].dtype == \"category\"):\n",
    "            if (df[col].cat.categories.shape[0]==0):\n",
    "                df[col] = df[col].astype(float).fillna(0)\n",
    "                df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    df = df.set_index(index)\n",
    "    df = expand_dataset(df, gdf)\n",
    "\n",
    "    # Test the output\n",
    "    test_tools.assert_correct_colnames(df)\n",
    "    test_tools.assert_correct_shape(df, gdf)\n",
    "\n",
    "    # Export\n",
    "    df.to_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_{shock}.csv\", index=False)\n",
    "\n",
    "    df = None\n",
    "    gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all is ok\n",
    "from importlib import reload\n",
    "reload(test_tools)\n",
    "gdf = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_map.csv\")\n",
    "# gdf = gpd.GeoDataFrame(gdf, geometry=gpd.GeoSeries.from_wkt(gdf[\"geometry\"]))\n",
    "\n",
    "for shock in [\"floods\", \"drought\", \"hurricanes\", \"intenserain\", \"heatwaves\", \"coldwaves\"]:\n",
    "    print(\"Verifying\", shock)\n",
    "    df = pd.read_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_{shock}.csv\")\n",
    "    test_tools.validate_dataset_merge(df, gdf, dataset_name=\"climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "gdf_full = gpd.read_feather(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\IPUMS_country_IDs.feather\")\n",
    "gdf_full = gdf_full.drop(columns=[\"ID\"])\n",
    "gdf_full = gdf_full.rename(columns={\"CNTRY_CODE\":\"adm0\", \"GEOLEVEL1\":\"adm1\", \"GEOLEVEL2\":\"adm2\"})\n",
    "gdf_full[[\"adm0_name\", \"adm1_name\", \"adm2_name\"]] = \"To be filled\"\n",
    "ids = [\"adm0\", \"adm2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Shocks\n",
    "path = r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\"\n",
    "       \n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if \"HC_national_data\" in f and f.endswith(\".csv\")]\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\\{file}\")\n",
    "    df[\"s3\"] = pd.NA\n",
    "    df[\"s4\"] = pd.NA\n",
    "    \n",
    "    s3cols = [\"s3a\", \"s3b\", \"s3c\", \"s3d\", \"s3f\"]\n",
    "    s4cols = [\"s4a\", \"s4b\", \"s4c\"]\n",
    "\n",
    "    for col in s3cols: \n",
    "        df[\"s3\"] = df[\"s3\"].fillna(df[col])\n",
    "        assert (df[s3cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "    for col in s4cols:\n",
    "        df[\"s4\"] = df[\"s4\"].fillna(df[col])\n",
    "        assert (df[s4cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "\n",
    "    dfs += [df]\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "for col in df.columns:\n",
    "    assert not df[col].isna().all()\n",
    "\n",
    "# Drop s3* columns\n",
    "df = df.drop(columns=[col for col in df.columns if (\"s3\" in col or \"s4\" in col) and (col != \"s3\" and col != \"s4\")])\n",
    "# Order variables\n",
    "df = df[[\"adm0\", \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"outcome\", \"new\", \"v\"]]\n",
    "df = df.rename(columns={\"new\":\"time\", \"v\": \"value\", \"status\":\"treatment\"})\n",
    "df.loc[df.s1 == \"Hurricane\", \"s5\"] = df.loc[df.s1 == \"Hurricane\", \"s5\"] / 100\n",
    "\n",
    "df = df.merge(gdf_full[[\"adm0\"]].drop_duplicates(), on=[\"adm0\"], validate=\"m:1\")\n",
    "print(f\"Hay datos de {df.adm0.unique().size} pa√≠ses\")\n",
    "df.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_national_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"s1\":\"Shock\",\n",
    "    \"s2\":\"Weight\",\n",
    "    \"s3\": {\n",
    "        \"Cold wave\":\"Temperature <0 ¬∞C\",\n",
    "        \"Heat wave\":\"Degrees (¬∞C)\",\n",
    "        \"Drought\":\"Drought indicator\",\n",
    "        \"Intense rain\":\"Number of days\",\n",
    "        \"Hurricane\":\"Category\"\n",
    "    },\n",
    "    \"s4\": {\n",
    "        \"Cold wave\":\"Standard Deviations from historical mean\",\n",
    "        \"Heat wave\":\"Standard Deviations from historical mean\",\n",
    "        \"Drought\":\"Standard Deviations from historical mean\",\n",
    "        \"Intense rain\":\"Rainfall (mm)\",\n",
    "        \"Hurricane\":\"Distance from center of the storm (degrees)\" # Fixme: turn to km\n",
    "    },\n",
    "    \"s5\": r\"Threshold (% affected)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HC_geodata_category.csv',\n",
       " 'HC_geodata_cwa.csv',\n",
       " 'HC_geodata_flooded.csv',\n",
       " 'HC_geodata_hwa.csv',\n",
       " 'HC_geodata_ia.csv',\n",
       " 'HC_geodata_sp.csv']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:46<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb2 in position 188427: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm(files):\n\u001b[1;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mrf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mNico\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mHC_geodata_category.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mNA\n\u001b[0;32m     15\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms4\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mNA\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xb2 in position 188427: invalid start byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Shocks\n",
    "path = r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\"\n",
    "       \n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if \"HC_geodata\" in f and f.endswith(\".csv\")]\n",
    "files = ['HC_geodata_flooded.csv']\n",
    "\n",
    "dfs = []\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\\{file}\")\n",
    "    df[\"s3\"] = pd.NA\n",
    "    df[\"s4\"] = pd.NA\n",
    "    \n",
    "    s3cols = [\"s3a\", \"s3b\", \"s3c\", \"s3d\", \"s3f\"]\n",
    "    s4cols = [\"s4a\", \"s4b\", \"s4c\"]\n",
    "\n",
    "    for col in s3cols: \n",
    "        df[\"s3\"] = df[\"s3\"].fillna(df[col])\n",
    "        assert (df[s3cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "    for col in s4cols:\n",
    "        df[\"s4\"] = df[\"s4\"].fillna(df[col])\n",
    "        assert (df[s4cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "\n",
    "    dfs += [df]\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "# for col in df.columns:\n",
    "#     assert not df[col].isna().all()\n",
    "\n",
    "# Drop s3* columns\n",
    "df = df.drop(columns=[col for col in df.columns if (\"s3\" in col or \"s4\" in col) and (col != \"s3\" and col != \"s4\")])\n",
    "# Order variables\n",
    "df = df[[\"adm0\", \"adm1\", \"adm2\", \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"outcome\", \"status\", \"diftime\"]]\n",
    "df = df.rename(columns={\"status\":\"treatment_sub\", \"diftime\":\"diff\"})\n",
    "df.loc[df.s1 == \"Hurricane\", \"s5\"] = df.loc[df.s1 == \"Hurricane\", \"s5\"] / 100\n",
    "\n",
    "df.merge(gdf_full[[\"adm0\", \"adm2\"]], on=[\"adm0\", \"adm2\"], validate=\"m:1\", how=\"inner\")\n",
    "print(f\"Hay datos de {df.adm0.unique().size} pa√≠ses\")\n",
    "\n",
    "df.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>adm0</th>\n",
       "      <th>adm1</th>\n",
       "      <th>adm2</th>\n",
       "      <th>adm0_name</th>\n",
       "      <th>adm1_name</th>\n",
       "      <th>adm2_name</th>\n",
       "      <th>gdf_ismerged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MULTIPOLYGON (((-66.5425262452896 -55.05752944...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320940.0</td>\n",
       "      <td>32094002.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POLYGON ((-68.37408447301986 -52.8758621219392...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320940.0</td>\n",
       "      <td>32094001.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MULTIPOLYGON (((-69.2425308228391 -50.97999954...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320780.0</td>\n",
       "      <td>32078001.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MULTIPOLYGON (((-68.43839263900891 -50.0804176...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320780.0</td>\n",
       "      <td>32078003.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MULTIPOLYGON (((-67.0521087653071 -48.75419616...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320780.0</td>\n",
       "      <td>32078004.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>POLYGON ((-65.79555399971721 -22.0805559997593...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320380.0</td>\n",
       "      <td>32038005.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>POLYGON ((-62.783028000042016 -22.172739999751...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320660.0</td>\n",
       "      <td>32066010.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2298</th>\n",
       "      <td>POLYGON ((-62.31109799957291 -22.4872350002947...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320340.0</td>\n",
       "      <td>32034005.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>POLYGON ((-64.90527800027937 -22.1249989997038...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320660.0</td>\n",
       "      <td>32066003.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>POLYGON ((-63.14258003438168 -21.9994440004400...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>320660.0</td>\n",
       "      <td>32066002.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               geometry  adm0      adm1  \\\n",
       "0     MULTIPOLYGON (((-66.5425262452896 -55.05752944...  32.0  320940.0   \n",
       "2     POLYGON ((-68.37408447301986 -52.8758621219392...  32.0  320940.0   \n",
       "3     MULTIPOLYGON (((-69.2425308228391 -50.97999954...  32.0  320780.0   \n",
       "4     MULTIPOLYGON (((-68.43839263900891 -50.0804176...  32.0  320780.0   \n",
       "5     MULTIPOLYGON (((-67.0521087653071 -48.75419616...  32.0  320780.0   \n",
       "...                                                 ...   ...       ...   \n",
       "2294  POLYGON ((-65.79555399971721 -22.0805559997593...  32.0  320380.0   \n",
       "2296  POLYGON ((-62.783028000042016 -22.172739999751...  32.0  320660.0   \n",
       "2298  POLYGON ((-62.31109799957291 -22.4872350002947...  32.0  320340.0   \n",
       "2299  POLYGON ((-64.90527800027937 -22.1249989997038...  32.0  320660.0   \n",
       "2301  POLYGON ((-63.14258003438168 -21.9994440004400...  32.0  320660.0   \n",
       "\n",
       "            adm2     adm0_name     adm1_name     adm2_name  gdf_ismerged  \n",
       "0     32094002.0  To be filled  To be filled  To be filled          True  \n",
       "2     32094001.0  To be filled  To be filled  To be filled          True  \n",
       "3     32078001.0  To be filled  To be filled  To be filled          True  \n",
       "4     32078003.0  To be filled  To be filled  To be filled          True  \n",
       "5     32078004.0  To be filled  To be filled  To be filled          True  \n",
       "...          ...           ...           ...           ...           ...  \n",
       "2294  32038005.0  To be filled  To be filled  To be filled          True  \n",
       "2296  32066010.0  To be filled  To be filled  To be filled          True  \n",
       "2298  32034005.0  To be filled  To be filled  To be filled          True  \n",
       "2299  32066003.0  To be filled  To be filled  To be filled          True  \n",
       "2301  32066002.0  To be filled  To be filled  To be filled          True  \n",
       "\n",
       "[312 rows x 8 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf[gdf[\"adm0\"]==32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adm0</th>\n",
       "      <th>adm1</th>\n",
       "      <th>adm2</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>outcome</th>\n",
       "      <th>treatment_sub</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006005</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006007</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006010</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006023</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006052</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006061</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>32</td>\n",
       "      <td>320100</td>\n",
       "      <td>32010001</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>32</td>\n",
       "      <td>320180</td>\n",
       "      <td>32018008</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4171</th>\n",
       "      <td>32</td>\n",
       "      <td>320260</td>\n",
       "      <td>32026004</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>32</td>\n",
       "      <td>320300</td>\n",
       "      <td>32030002</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>32</td>\n",
       "      <td>320380</td>\n",
       "      <td>32038001</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4765</th>\n",
       "      <td>32</td>\n",
       "      <td>320380</td>\n",
       "      <td>32038004</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>32</td>\n",
       "      <td>320500</td>\n",
       "      <td>32050015</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6529</th>\n",
       "      <td>32</td>\n",
       "      <td>320660</td>\n",
       "      <td>32066007</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6936</th>\n",
       "      <td>32</td>\n",
       "      <td>320700</td>\n",
       "      <td>32070009</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7171</th>\n",
       "      <td>32</td>\n",
       "      <td>320780</td>\n",
       "      <td>32078001</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7381</th>\n",
       "      <td>32</td>\n",
       "      <td>320820</td>\n",
       "      <td>32082005</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Affected</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7888</th>\n",
       "      <td>32</td>\n",
       "      <td>320860</td>\n",
       "      <td>32086005</td>\n",
       "      <td>Flood</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      adm0    adm1      adm2     s1    s2  s3  s4  s5  \\\n",
       "136     32  320060  32006005  Flood  Area NaN NaN  10   \n",
       "192     32  320060  32006007  Flood  Area NaN NaN  10   \n",
       "274     32  320060  32006010  Flood  Area NaN NaN  10   \n",
       "622     32  320060  32006023  Flood  Area NaN NaN  10   \n",
       "1406    32  320060  32006052  Flood  Area NaN NaN  10   \n",
       "1650    32  320060  32006061  Flood  Area NaN NaN  10   \n",
       "2514    32  320100  32010001  Flood  Area NaN NaN  10   \n",
       "3447    32  320180  32018008  Flood  Area NaN NaN  10   \n",
       "4171    32  320260  32026004  Flood  Area NaN NaN  10   \n",
       "4277    32  320300  32030002  Flood  Area NaN NaN  10   \n",
       "4684    32  320380  32038001  Flood  Area NaN NaN  10   \n",
       "4765    32  320380  32038004  Flood  Area NaN NaN  10   \n",
       "5563    32  320500  32050015  Flood  Area NaN NaN  10   \n",
       "6529    32  320660  32066007  Flood  Area NaN NaN  10   \n",
       "6936    32  320700  32070009  Flood  Area NaN NaN  10   \n",
       "7171    32  320780  32078001  Flood  Area NaN NaN  10   \n",
       "7381    32  320820  32082005  Flood  Area NaN NaN  10   \n",
       "7888    32  320860  32086005  Flood  Area NaN NaN  10   \n",
       "\n",
       "                            outcome treatment_sub  diff  \n",
       "136   School attendance (ages 6-14)  Not-affected  -1.0  \n",
       "192   School attendance (ages 6-14)  Not-affected  -1.0  \n",
       "274   School attendance (ages 6-14)  Not-affected   1.0  \n",
       "622   School attendance (ages 6-14)  Not-affected   1.0  \n",
       "1406  School attendance (ages 6-14)  Not-affected  -1.0  \n",
       "1650  School attendance (ages 6-14)  Not-affected   1.0  \n",
       "2514  School attendance (ages 6-14)  Not-affected   1.0  \n",
       "3447  School attendance (ages 6-14)  Not-affected  -1.0  \n",
       "4171  School attendance (ages 6-14)  Not-affected  -1.0  \n",
       "4277  School attendance (ages 6-14)  Not-affected   1.0  \n",
       "4684  School attendance (ages 6-14)  Not-affected  -1.0  \n",
       "4765  School attendance (ages 6-14)  Not-affected   1.0  \n",
       "5563  School attendance (ages 6-14)  Not-affected  -1.0  \n",
       "6529  School attendance (ages 6-14)  Not-affected   1.0  \n",
       "6936  School attendance (ages 6-14)  Not-affected   1.0  \n",
       "7171  School attendance (ages 6-14)  Not-affected   1.0  \n",
       "7381  School attendance (ages 6-14)      Affected  -1.0  \n",
       "7888  School attendance (ages 6-14)  Not-affected  -1.0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"adm0==32 and s2=='Area' and outcome=='School attendance (ages 6-14)' and s5==10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_25864\\4134839371.py:5: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddf3bef73204995bf72aa2798d8ab64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking groups:   0%|          | 0/13650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of polygons without data: 13117\n",
      "Number of polygons without data: 13068\n",
      "Number of polygons without data: 12933\n",
      "Number of polygons without data: 13019\n",
      "Number of polygons without data: 13069\n",
      "Number of polygons without data: 13085\n",
      "Number of polygons without data: 13971\n",
      "Number of polygons without data: 13331\n",
      "Number of polygons without data: 13521\n",
      "Number of polygons without data: 13472\n",
      "Number of polygons without data: 13200\n",
      "Number of polygons without data: 13280\n",
      "Number of polygons without data: 13244\n",
      "Number of polygons without data: 13284\n",
      "Number of polygons without data: 13215\n",
      "Number of polygons without data: 13238\n",
      "Number of polygons without data: 13350\n",
      "Number of polygons without data: 13298\n",
      "Number of polygons without data: 13327\n"
     ]
    },
    {
     "ename": "MergeError",
     "evalue": "Merge keys are not unique in right dataset; not a one-to-one merge",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m gdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWorld Bank\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCLIENT v2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData_out\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfor webpage\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHC_geo_map.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWorld Bank\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCLIENT v2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData_out\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfor webpage\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHC_geo_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtest_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_hc_merge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\World Bank\\CLIENT v2\\test_tools.py:192\u001b[0m, in \u001b[0;36mvalidate_hc_merge\u001b[1;34m(df, gdf)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo groups found in the dataframe. Check the input data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m selection, group \u001b[38;5;129;01min\u001b[39;00m tqdm(grouped, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking groups\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    188\u001b[0m     \n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# Reset index of the group\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# df_filtered = group.reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     merged \u001b[38;5;241m=\u001b[39m \u001b[43mgdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# merged.loc[merged[\"gdf_ismerge\"] & merged[\"df_ismerge\"], \"_merge\"] = \"both\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# merged.loc[merged[\"gdf_ismerge\"].isna() & merged[\"df_ismerge\"], \"_merge\"] = \"right_only\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# merged.loc[merged[\"gdf_ismerge\"] & merged[\"df_ismerge\"].isna(), \"_merge\"] = \"left_only\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m merged[merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_merge\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright_only\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\merge.py:813\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 813\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_validate_kwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\merge.py:1657\u001b[0m, in \u001b[0;36m_MergeOperation._validate_validate_kwd\u001b[1;34m(self, validate)\u001b[0m\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   1654\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge keys are not unique in left dataset; not a one-to-one merge\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1655\u001b[0m         )\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m right_unique:\n\u001b[1;32m-> 1657\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   1658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge keys are not unique in right dataset; not a one-to-one merge\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m validate \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone_to_many\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1:m\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m left_unique:\n",
      "\u001b[1;31mMergeError\u001b[0m: Merge keys are not unique in right dataset; not a one-to-one merge"
     ]
    }
   ],
   "source": [
    "# test all is ok\n",
    "from importlib import reload\n",
    "reload(test_tools)\n",
    "gdf = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_map.csv\")\n",
    "df = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\")\n",
    "test_tools.validate_hc_merge(df, gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adm_unique = df[[\"adm0\",\"adm1\",\"adm2\"]].drop_duplicates()\n",
    "gdf_adm_unique = gdf[[\"adm0\",\"adm1\",\"adm2\"]].drop_duplicates()\n",
    "if df_adm_unique.shape[0] != gdf_adm_unique.shape[0]:\n",
    "    print(\"Removing geometries where there is no data...\")\n",
    "    gdf_shape_pre = gdf.shape[0]\n",
    "    gdf = gdf[gdf[[\"adm0\",\"adm1\",\"adm2\"]].isin(df_adm_unique).all(axis=1)]\n",
    "    print(f\"Removed {gdf_shape_pre - gdf.shape[0]} geometries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adm0</th>\n",
       "      <th>adm1</th>\n",
       "      <th>adm2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>320020</td>\n",
       "      <td>32002001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>32</td>\n",
       "      <td>320060</td>\n",
       "      <td>32006004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704720</th>\n",
       "      <td>894</td>\n",
       "      <td>894010</td>\n",
       "      <td>894010002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704840</th>\n",
       "      <td>894</td>\n",
       "      <td>894010</td>\n",
       "      <td>894010003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704960</th>\n",
       "      <td>894</td>\n",
       "      <td>894010</td>\n",
       "      <td>894010004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705080</th>\n",
       "      <td>894</td>\n",
       "      <td>894010</td>\n",
       "      <td>894010005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705200</th>\n",
       "      <td>894</td>\n",
       "      <td>894010</td>\n",
       "      <td>894010006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14211 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         adm0    adm1       adm2\n",
       "0          32  320020   32002001\n",
       "120        32  320060   32006001\n",
       "240        32  320060   32006002\n",
       "360        32  320060   32006003\n",
       "480        32  320060   32006004\n",
       "...       ...     ...        ...\n",
       "1704720   894  894010  894010002\n",
       "1704840   894  894010  894010003\n",
       "1704960   894  894010  894010004\n",
       "1705080   894  894010  894010005\n",
       "1705200   894  894010  894010006\n",
       "\n",
       "[14211 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_adm_unique = df[[\"adm0\",\"adm1\",\"adm2\"]].drop_duplicates()\n",
    "df_adm_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_merge\n",
       "both          14211\n",
       "left_only         0\n",
       "right_only        0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_adm_unique.merge(df_adm_unique, on=[\"adm0\",\"adm1\",\"adm2\"], how=\"outer\", indicator=True)._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 4/87 [00:00<00:16,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 6/87 [00:01<00:16,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 11/87 [00:01<00:11,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñç        | 13/87 [00:02<00:12,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñé       | 20/87 [00:03<00:09,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñä       | 25/87 [00:04<00:09,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 28/87 [00:04<00:08,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 31/87 [00:05<00:10,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 37/87 [00:06<00:08,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 43/87 [00:07<00:06,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 46/87 [00:07<00:06,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 49/87 [00:08<00:06,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 51/87 [00:08<00:06,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 55/87 [00:09<00:05,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 61/87 [00:10<00:04,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 66/87 [00:11<00:03,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 75/87 [00:12<00:01,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 79/87 [00:13<00:01,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 81/87 [00:13<00:00,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 82/87 [00:13<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 83/87 [00:14<00:00,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87/87 [00:14<00:00,  5.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for code in tqdm(df.adm0.unique()):\n",
    "    arg_df = df.query(f\"adm0=={code} and s1=='Hurricane' and s2=='Area' and s3f==3 and s4c==10 and s5==0\")\n",
    "    arg_gdf = gdf_full.query(f\"adm0=={code}\")\n",
    "    merged = arg_gdf.merge(arg_df, on=[\"adm0\", \"adm1\", \"adm2\"], how=\"outer\", indicator=True, validate=\"1:1\")\n",
    "    assert (merged._merge != \"right_only\").all()\n",
    "    if not (merged._merge == \"both\").all():\n",
    "        print(merged[merged._merge != \"both\"].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>adm0</th>\n",
       "      <th>adm1</th>\n",
       "      <th>adm2</th>\n",
       "      <th>adm0_name</th>\n",
       "      <th>adm1_name</th>\n",
       "      <th>adm2_name</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3a</th>\n",
       "      <th>...</th>\n",
       "      <th>s4a</th>\n",
       "      <th>s4b</th>\n",
       "      <th>s4c</th>\n",
       "      <th>s5</th>\n",
       "      <th>outcome</th>\n",
       "      <th>diftime</th>\n",
       "      <th>status</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLYGON ((44.65813 40.16007, 44.65044 40.15238...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51901.0</td>\n",
       "      <td>51901.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>School attendance (ages 6-14)</td>\n",
       "      <td>-0.984655</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLYGON ((44.25292 40.74393, 44.26475 40.7321,...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51902.0</td>\n",
       "      <td>51902.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dwelling with electricity</td>\n",
       "      <td>-0.996409</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POLYGON ((45.04264 40.18965, 45.04264 40.18373...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51903.0</td>\n",
       "      <td>51903.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Completed secondary education (ages 25+)</td>\n",
       "      <td>-0.851793</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLYGON ((43.78795 40.27337, 43.80676 40.2708,...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51904.0</td>\n",
       "      <td>51904.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Water inside dwelling</td>\n",
       "      <td>0.614333</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MULTIPOLYGON (((45.35171 40.68144, 45.36178 40...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51905.0</td>\n",
       "      <td>51905.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Completed secondary education (ages 25+)</td>\n",
       "      <td>0.916258</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>POLYGON ((44.82257 41.29952, 44.8242 41.29927,...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51906.0</td>\n",
       "      <td>51906.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>School attendance (ages 15-24)</td>\n",
       "      <td>0.343972</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>POLYGON ((44.79153 40.67022, 44.7936 40.6694, ...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51907.0</td>\n",
       "      <td>51907.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Internal migration (Any)(ages 15-24)</td>\n",
       "      <td>-0.127840</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>POLYGON ((44.0345 41.18864, 44.0307 41.181, 44...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51908.0</td>\n",
       "      <td>51908.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Water inside dwelling</td>\n",
       "      <td>0.754097</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>POLYGON ((45.82676 39.82643, 45.8277 39.82496,...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51909.0</td>\n",
       "      <td>51909.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dwelling with electricity</td>\n",
       "      <td>-0.981919</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>POLYGON ((45.57385 40.00509, 45.5786 39.99997,...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51910.0</td>\n",
       "      <td>51910.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Water inside dwelling</td>\n",
       "      <td>0.660144</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>POLYGON ((45.01994 41.28564, 45.02044 41.28398...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>51911.0</td>\n",
       "      <td>51911.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Internal migration (ADM1)(ages 15-24)</td>\n",
       "      <td>-0.072437</td>\n",
       "      <td>Not-affected</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>POLYGON ((45.14775 40.56686, 45.15806 40.56416...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>888888.0</td>\n",
       "      <td>888888.0</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>To be filled</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>left_only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows √ó 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             geometry  adm0      adm1  \\\n",
       "0   POLYGON ((44.65813 40.16007, 44.65044 40.15238...  51.0   51901.0   \n",
       "1   POLYGON ((44.25292 40.74393, 44.26475 40.7321,...  51.0   51902.0   \n",
       "2   POLYGON ((45.04264 40.18965, 45.04264 40.18373...  51.0   51903.0   \n",
       "3   POLYGON ((43.78795 40.27337, 43.80676 40.2708,...  51.0   51904.0   \n",
       "4   MULTIPOLYGON (((45.35171 40.68144, 45.36178 40...  51.0   51905.0   \n",
       "5   POLYGON ((44.82257 41.29952, 44.8242 41.29927,...  51.0   51906.0   \n",
       "6   POLYGON ((44.79153 40.67022, 44.7936 40.6694, ...  51.0   51907.0   \n",
       "7   POLYGON ((44.0345 41.18864, 44.0307 41.181, 44...  51.0   51908.0   \n",
       "8   POLYGON ((45.82676 39.82643, 45.8277 39.82496,...  51.0   51909.0   \n",
       "9   POLYGON ((45.57385 40.00509, 45.5786 39.99997,...  51.0   51910.0   \n",
       "10  POLYGON ((45.01994 41.28564, 45.02044 41.28398...  51.0   51911.0   \n",
       "11  POLYGON ((45.14775 40.56686, 45.15806 40.56416...  51.0  888888.0   \n",
       "\n",
       "        adm2     adm0_name     adm1_name     adm2_name         s1    s2  s3a  \\\n",
       "0    51901.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "1    51902.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "2    51903.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "3    51904.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "4    51905.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "5    51906.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "6    51907.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "7    51908.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "8    51909.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "9    51910.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "10   51911.0  To be filled  To be filled  To be filled  Hurricane  Area  NaN   \n",
       "11  888888.0  To be filled  To be filled  To be filled        NaN   NaN  NaN   \n",
       "\n",
       "    ...  s4a  s4b   s4c   s5                                   outcome  \\\n",
       "0   ...  NaN  NaN  10.0  0.0             School attendance (ages 6-14)   \n",
       "1   ...  NaN  NaN  10.0  0.0                 Dwelling with electricity   \n",
       "2   ...  NaN  NaN  10.0  0.0  Completed secondary education (ages 25+)   \n",
       "3   ...  NaN  NaN  10.0  0.0                     Water inside dwelling   \n",
       "4   ...  NaN  NaN  10.0  0.0  Completed secondary education (ages 25+)   \n",
       "5   ...  NaN  NaN  10.0  0.0            School attendance (ages 15-24)   \n",
       "6   ...  NaN  NaN  10.0  0.0      Internal migration (Any)(ages 15-24)   \n",
       "7   ...  NaN  NaN  10.0  0.0                     Water inside dwelling   \n",
       "8   ...  NaN  NaN  10.0  0.0                 Dwelling with electricity   \n",
       "9   ...  NaN  NaN  10.0  0.0                     Water inside dwelling   \n",
       "10  ...  NaN  NaN  10.0  0.0     Internal migration (ADM1)(ages 15-24)   \n",
       "11  ...  NaN  NaN   NaN  NaN                                       NaN   \n",
       "\n",
       "     diftime        status   s3    s4     _merge  \n",
       "0  -0.984655  Not-affected  3.0  10.0       both  \n",
       "1  -0.996409  Not-affected  3.0  10.0       both  \n",
       "2  -0.851793  Not-affected  3.0  10.0       both  \n",
       "3   0.614333  Not-affected  3.0  10.0       both  \n",
       "4   0.916258  Not-affected  3.0  10.0       both  \n",
       "5   0.343972  Not-affected  3.0  10.0       both  \n",
       "6  -0.127840  Not-affected  3.0  10.0       both  \n",
       "7   0.754097  Not-affected  3.0  10.0       both  \n",
       "8  -0.981919  Not-affected  3.0  10.0       both  \n",
       "9   0.660144  Not-affected  3.0  10.0       both  \n",
       "10 -0.072437  Not-affected  3.0  10.0       both  \n",
       "11       NaN           NaN  NaN   NaN  left_only  \n",
       "\n",
       "[12 rows x 24 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import procesa_bases \n",
    "\n",
    "gdf_full = gpd.read_feather(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\IPUMS_country_IDs.feather\")\n",
    "gdf_full = gdf_full.drop(columns=[\"ID\"])\n",
    "\n",
    "WB_country = procesa_bases.load_WB_country_data()\n",
    "IPUMS_country = procesa_bases.load_IPUMS_country_data(WB_country, keep_name=True)\n",
    "IPUMS_country = IPUMS_country.clip(WB_country.total_bounds)\n",
    "\n",
    "assert (gdf_full.merge(IPUMS_country, on=[\"GEOLEVEL1\", \"GEOLEVEL2\", \"CNTRY_CODE\"], how=\"outer\", indicator=True, validate=\"1:1\")._merge == \"both\").all()\n",
    "\n",
    "# Rename columns to make it in the intended format\n",
    "IPUMS_country = IPUMS_country.rename(columns={\"CNTRY_CODE\":\"adm0\", \"GEOLEVEL1\":\"adm1\", \"GEOLEVEL2\":\"adm2\", \"CNTRY_NAME\":\"adm0_name\", \"ADMIN_NAME\":\"adm2_name\"})\n",
    "IPUMS_country = IPUMS_country.drop(columns=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPUMS_country.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_raw\\button_labels.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\selector_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "\n",
    "# Assert that all data merges correctly\n",
    "\n",
    "gdf = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\IPUMS_map.csv\")\n",
    "gdf['geometry'] = gdf['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(gdf, crs='epsg:4326')\n",
    "\n",
    "df_adm2 = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.merge(df_adm2, on=[\"adm0\", \"adm2\"], validate=\"1:m\", how=\"outer\", indicator=True)._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = gdf_full[[\"CNTRY_CODE\", \"GEOLEVEL2\", \"geometry\"]].merge(df, right_on=[\"adm0\", \"adm2\"], left_on=[\"CNTRY_CODE\", \"GEOLEVEL2\"], how=\"outer\", indicator=True)\n",
    "merged = merged[~merged.CNTRY_CODE.isin([231,276,356,368,376,504,566,586,662])]\n",
    "pd.crosstab(merged[merged._merge!=\"both\"].CNTRY_CODE, merged[merged._merge!=\"both\"]._merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palestina no est√° porque solo tiene after. El resto 10/10\n",
    "\n",
    "import folium\n",
    "m = merged[merged._merge!=\"both\"].drop_duplicates(subset=[\"CNTRY_CODE\", \"GEOLEVEL2\"]).explore()\n",
    "\n",
    "# add control for layers\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
