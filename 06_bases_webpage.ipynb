{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_25864\\4087382191.py:8: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  PATH = \"D:\\World Bank\\CLIENT v2\"\n",
      "d:\\World Bank\\CLIENT v2\\procesa_bases.py:15: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  PATH = \"D:\\World Bank\\CLIENT v2\"\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import test_tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from procesa_bases import load_WB_country_data\n",
    "\n",
    "PATH = \"D:\\World Bank\\CLIENT v2\"\n",
    "DATA_RAW = rf\"{PATH}\\Data\\Data_raw\"\n",
    "DATA_PROC = rf\"{PATH}\\Data\\Data_proc\"\n",
    "DATA_OUT = rf\"{PATH}\\Data\\Data_out\"\n",
    "GPW_PATH = rf\"D:\\Datasets\\Gridded Population of the World\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera mapa con etiquetas de zona (adm0 adm1 adm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ID dataset\n",
    "gdf = gpd.read_feather(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\WB_country_IDs.feather\")\n",
    "gdf.columns = gdf.columns.str.lower()\n",
    "gdf = gdf.rename(columns={\"id\":\"ID\"}).drop(columns=\"objectid\")\n",
    "\n",
    "# Add names from the original WB adm2 dataset\n",
    "gdf_raw = load_WB_country_data()\n",
    "gdf_raw.columns = gdf_raw.columns.str.lower()\n",
    "gdf_raw = gdf_raw[[\"adm0_code\", \"adm1_code\", \"adm2_code\", \"adm0_name\", \"adm1_name\", \"adm2_name\", \"geometry\"]]\n",
    "assert gdf_raw.duplicated(subset=[\"adm0_code\", \"adm1_code\", \"adm2_code\"]).sum() == 0, \"There are duplicated entries in the raw dataset!!\"\n",
    "\n",
    "# Merge both datasets to assert that the codes are correct and consistent\n",
    "gdf = gdf.merge(gdf_raw.drop(columns=\"geometry\"), how=\"outer\", on=[\"adm0_code\", \"adm1_code\", \"adm2_code\"], indicator=True, validate=\"1:1\")\n",
    "assert (gdf._merge == \"both\").all(), \"There are problems with the merge!!\"\n",
    "gdf = gdf.drop(columns=\"_merge\")\n",
    "\n",
    "gdf.drop(columns=\"ID\").to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_map.csv\", index=False) # Export without the ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set admin level to categorical dtype (when the dataset is expanded, it will be more memory efficient)\n",
    "gdf[\"ID\"]        = gdf[\"ID\"].astype(\"category\")\n",
    "gdf[\"adm0_code\"] = gdf[\"adm0_code\"].astype(\"category\")\n",
    "gdf[\"adm1_code\"] = gdf[\"adm1_code\"].astype(\"category\")\n",
    "gdf[\"adm2_code\"] = gdf[\"adm2_code\"].astype(\"category\")\n",
    "gdf = gdf.set_index(\"ID\")\n",
    "\n",
    "gdf = gdf.drop(columns=[\"adm0_name\",\"adm1_name\",\"adm2_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera datos de shocks climáticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_raw = gpd.read_file(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_raw\\world_bank_adm2\\world_bank_adm2.shp\")\n",
    "gdf_raw.columns = gdf_raw.columns.str.lower()\n",
    "gdf_raw = gdf_raw[[\"adm0_code\", \"adm1_code\", \"adm2_code\", \"adm0_name\", \"adm1_name\", \"adm2_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "def expand_dataset(df, gdf):\n",
    "                    \n",
    "    # Collect all dimension values from df\n",
    "    all_years      = df.index.get_level_values(\"year\").categories\n",
    "    all_variables  = df.index.get_level_values(\"variable\").categories\n",
    "    all_thresholds = df.index.get_level_values(\"threshold\").categories\n",
    "    all_measures   = df.index.get_level_values(\"measure\").categories\n",
    "    all_regions    = gdf.index.categories # ID is the index of gdf\n",
    "\n",
    "    # Convert each list to a small DataFrame\n",
    "    df_years      = pd.DataFrame({'year': all_years}, dtype='category')\n",
    "    df_variables  = pd.DataFrame({'variable': all_variables}, dtype='category')\n",
    "    df_thresholds = pd.DataFrame({'threshold': all_thresholds}, dtype='category')\n",
    "    df_measures   = pd.DataFrame({'measure': all_measures}, dtype='category')\n",
    "    df_regions    = pd.DataFrame({'ID': all_regions}, dtype='category')\n",
    "\n",
    "    # Step-by-step merges using how='cross'\n",
    "    df_temp = df_years.merge(df_variables, how='cross')\n",
    "    df_temp = df_temp.merge(df_regions, how='cross')\n",
    "    df_temp = df_temp.merge(df_thresholds, how='cross')\n",
    "    df_temp = df_temp.merge(df_measures, how='cross')\n",
    "    expanded_without_data = df_temp.set_index([\"ID\", \"year\", \"variable\", \"threshold\", \"measure\"])\n",
    "    \n",
    "    # add admcodes to the expanded set\n",
    "    expanded_without_data = expanded_without_data.join(\n",
    "        gdf.drop(columns=[\"geometry\"]),\n",
    "        how=\"left\",\n",
    "        on=\"ID\",\n",
    "        validate=\"m:1\"\n",
    "    )\n",
    "    \n",
    "    # Merge original data (df) onto the expanded set\n",
    "    expanded_with_data = expanded_without_data.join(\n",
    "        df,\n",
    "        how=\"left\",\n",
    "        validate=\"1:1\",\n",
    "        rsuffix=\"_y\"\n",
    "    ).reset_index().drop(columns=\"ID\")\n",
    "    \n",
    "    expanded_with_data = test_tools.assert_correct_admcodes(expanded_with_data)        \n",
    "\n",
    "    return expanded_with_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "## Generados Nico\n",
    "# Set dtypes to make this loading efficient\n",
    "dtypes = {\"year\": np.int16, \"variable\":\"category\", \"threshold\":\"category\", \"area_affected\":np.float32, \"population_affected\":np.float32, \"ID\":np.int64}# \"adm2_code\": np.int16, \"adm1_code\": np.int16, \"adm0_code\": np.int16,\n",
    "\n",
    "for shock in [\"floods\", \"drought\", \"hurricanes\", \"intenserain\", \"heatwaves\", \"coldwaves\"]:\n",
    "    print(shock)\n",
    "    df = pd.read_csv(\n",
    "        rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\WB_{shock}_long.csv\",\n",
    "        dtype=dtypes, \n",
    "        usecols=dtypes.keys(),\n",
    "    )\n",
    "        \n",
    "    # Set ID to categorical dtype (this is after loading as int to match with the categories of gdf)\n",
    "    df[\"ID\"] = df[\"ID\"].astype(\"category\")\n",
    "    \n",
    "    # Reshape to long format\n",
    "    df = df.melt(id_vars=[\"ID\", \"year\", \"variable\", \"threshold\"], var_name=\"measure\", value_name=\"value\")\n",
    "\n",
    "    # Set categorical and index to make faster merges\n",
    "    df[\"measure\"] = df[\"measure\"].astype(\"category\")\n",
    "    df[\"year\"] = df[\"year\"].astype(\"category\")\n",
    "    df = df.set_index([\"ID\"])    \n",
    "    \n",
    "    # Add adm0, adm1 and adm2 codes    \n",
    "    df = gdf.drop(columns=[\"geometry\"]).join(df, on=[\"ID\"], how=\"inner\", validate=\"1:m\")\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Set index to make faster merges and expand dataset\n",
    "    #   Replace columns with null categories with zeros before setting the index to make it work as expected:\n",
    "    index = [\"ID\", \"year\", \"variable\", \"threshold\", \"measure\"]\n",
    "    for col in index:\n",
    "        if (df[col].dtype == \"category\"):\n",
    "            if (df[col].cat.categories.shape[0]==0):\n",
    "                df[col] = df[col].astype(float).fillna(0)\n",
    "                df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    df = df.set_index(index)\n",
    "    df = expand_dataset(df, gdf)\n",
    "\n",
    "    # Test the output\n",
    "    test_tools.assert_correct_colnames(df)\n",
    "    test_tools.assert_correct_shape(df, gdf)\n",
    "\n",
    "    # Export\n",
    "    df.to_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_{shock}.csv\", index=False)\n",
    "\n",
    "    df = None\n",
    "    gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all is ok\n",
    "from importlib import reload\n",
    "reload(test_tools)\n",
    "gdf = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_map.csv\")\n",
    "# gdf = gpd.GeoDataFrame(gdf, geometry=gpd.GeoSeries.from_wkt(gdf[\"geometry\"]))\n",
    "\n",
    "for shock in [\"floods\", \"drought\", \"hurricanes\", \"intenserain\", \"heatwaves\", \"coldwaves\"]:\n",
    "    print(\"Verifying\", shock)\n",
    "    df = pd.read_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_{shock}.csv\")\n",
    "    test_tools.validate_dataset_merge(df, gdf, dataset_name=\"climate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "gdf_full = gpd.read_feather(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\IPUMS_country_IDs.feather\")\n",
    "gdf_full = gdf_full.drop(columns=[\"ID\"])\n",
    "gdf_full = gdf_full.rename(columns={\"CNTRY_CODE\":\"adm0\", \"GEOLEVEL1\":\"adm1\", \"GEOLEVEL2\":\"adm2\"})\n",
    "gdf_full[[\"adm0_name\", \"adm1_name\", \"adm2_name\"]] = \"To be filled\"\n",
    "ids = [\"adm0\", \"adm2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Shocks\n",
    "path = r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\"\n",
    "       \n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if \"HC_national_data\" in f and f.endswith(\".csv\")]\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\\{file}\")\n",
    "    df[\"s3\"] = pd.NA\n",
    "    df[\"s4\"] = pd.NA\n",
    "    \n",
    "    s3cols = [\"s3a\", \"s3b\", \"s3c\", \"s3d\", \"s3f\"]\n",
    "    s4cols = [\"s4a\", \"s4b\", \"s4c\"]\n",
    "\n",
    "    for col in s3cols: \n",
    "        df[\"s3\"] = df[\"s3\"].fillna(df[col])\n",
    "        assert (df[s3cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "    for col in s4cols:\n",
    "        df[\"s4\"] = df[\"s4\"].fillna(df[col])\n",
    "        assert (df[s4cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "\n",
    "    dfs += [df]\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "for col in df.columns:\n",
    "    assert not df[col].isna().all()\n",
    "\n",
    "# Drop s3* columns\n",
    "df = df.drop(columns=[col for col in df.columns if (\"s3\" in col or \"s4\" in col) and (col != \"s3\" and col != \"s4\")])\n",
    "# Order variables\n",
    "df = df[[\"adm0\", \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"outcome\", \"new\", \"v\"]]\n",
    "df = df.rename(columns={\"new\":\"time\", \"v\": \"value\", \"status\":\"treatment\"})\n",
    "df.loc[df.s1 == \"Hurricane\", \"s5\"] = df.loc[df.s1 == \"Hurricane\", \"s5\"] / 100\n",
    "\n",
    "df = df.merge(gdf_full[[\"adm0\"]].drop_duplicates(), on=[\"adm0\"], validate=\"m:1\")\n",
    "print(f\"Hay datos de {df.adm0.unique().size} países\")\n",
    "df.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_national_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"s1\":\"Shock\",\n",
    "    \"s2\":\"Weight\",\n",
    "    \"s3\": {\n",
    "        \"Cold wave\":\"Temperature <0 °C\",\n",
    "        \"Heat wave\":\"Degrees (°C)\",\n",
    "        \"Drought\":\"Drought indicator\",\n",
    "        \"Intense rain\":\"Number of days\",\n",
    "        \"Hurricane\":\"Category\"\n",
    "    },\n",
    "    \"s4\": {\n",
    "        \"Cold wave\":\"Standard Deviations from historical mean\",\n",
    "        \"Heat wave\":\"Standard Deviations from historical mean\",\n",
    "        \"Drought\":\"Standard Deviations from historical mean\",\n",
    "        \"Intense rain\":\"Rainfall (mm)\",\n",
    "        \"Hurricane\":\"Distance from center of the storm (degrees)\" # Fixme: turn to km\n",
    "    },\n",
    "    \"s5\": r\"Threshold (% affected)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HC_geodata_category.csv',\n",
       " 'HC_geodata_cwa.csv',\n",
       " 'HC_geodata_flooded.csv',\n",
       " 'HC_geodata_hwa.csv',\n",
       " 'HC_geodata_ia.csv',\n",
       " 'HC_geodata_sp.csv']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Shocks\n",
    "path = r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\"\n",
    "       \n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if \"HC_geodata\" in f and f.endswith(\".csv\")]\n",
    "\n",
    "dfs = []\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\\{file}\")\n",
    "    df[\"s3\"] = pd.NA\n",
    "    df[\"s4\"] = pd.NA\n",
    "    \n",
    "    s3cols = [\"s3a\", \"s3b\", \"s3c\", \"s3d\", \"s3f\"]\n",
    "    s4cols = [\"s4a\", \"s4b\", \"s4c\"]\n",
    "\n",
    "    for col in s3cols: \n",
    "        df[\"s3\"] = df[\"s3\"].fillna(df[col])\n",
    "        assert (df[s3cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "    for col in s4cols:\n",
    "        df[\"s4\"] = df[\"s4\"].fillna(df[col])\n",
    "        assert (df[s4cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "\n",
    "    dfs += [df]\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "for col in df.columns:\n",
    "    assert not df[col].isna().all()\n",
    "\n",
    "# Drop s3* columns\n",
    "df = df.drop(columns=[col for col in df.columns if (\"s3\" in col or \"s4\" in col) and (col != \"s3\" and col != \"s4\")])\n",
    "# Order variables\n",
    "df = df[[\"adm0\", \"adm1\", \"adm2\", \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"outcome\", \"status\", \"diftime\"]]\n",
    "df = df.rename(columns={\"status\":\"treatment_sub\", \"diftime\":\"diff\"})\n",
    "df.loc[df.s1 == \"Hurricane\", \"s5\"] = df.loc[df.s1 == \"Hurricane\", \"s5\"] / 100\n",
    "\n",
    "df.merge(gdf_full[[\"adm0\", \"adm2\"]], on=[\"adm0\", \"adm2\"], validate=\"m:1\", how=\"inner\")\n",
    "print(f\"Hay datos de {df.adm0.unique().size} países\")\n",
    "\n",
    "df.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nico\\AppData\\Local\\Temp\\ipykernel_25864\\4134839371.py:5: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddf3bef73204995bf72aa2798d8ab64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking groups:   0%|          | 0/13650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of polygons without data: 13117\n",
      "Number of polygons without data: 13068\n",
      "Number of polygons without data: 12933\n",
      "Number of polygons without data: 13019\n",
      "Number of polygons without data: 13069\n",
      "Number of polygons without data: 13085\n",
      "Number of polygons without data: 13971\n",
      "Number of polygons without data: 13331\n",
      "Number of polygons without data: 13521\n",
      "Number of polygons without data: 13472\n",
      "Number of polygons without data: 13200\n",
      "Number of polygons without data: 13280\n",
      "Number of polygons without data: 13244\n",
      "Number of polygons without data: 13284\n",
      "Number of polygons without data: 13215\n",
      "Number of polygons without data: 13238\n",
      "Number of polygons without data: 13350\n",
      "Number of polygons without data: 13298\n",
      "Number of polygons without data: 13327\n"
     ]
    },
    {
     "ename": "MergeError",
     "evalue": "Merge keys are not unique in right dataset; not a one-to-one merge",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m gdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWorld Bank\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCLIENT v2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData_out\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfor webpage\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHC_geo_map.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mWorld Bank\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCLIENT v2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData_out\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfor webpage\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHC_geo_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtest_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_hc_merge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\World Bank\\CLIENT v2\\test_tools.py:192\u001b[0m, in \u001b[0;36mvalidate_hc_merge\u001b[1;34m(df, gdf)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo groups found in the dataframe. Check the input data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m selection, group \u001b[38;5;129;01min\u001b[39;00m tqdm(grouped, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChecking groups\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    188\u001b[0m     \n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# Reset index of the group\u001b[39;00m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# df_filtered = group.reset_index(drop=True)\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     merged \u001b[38;5;241m=\u001b[39m \u001b[43mgdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1:1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# merged.loc[merged[\"gdf_ismerge\"] & merged[\"df_ismerge\"], \"_merge\"] = \"both\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# merged.loc[merged[\"gdf_ismerge\"].isna() & merged[\"df_ismerge\"], \"_merge\"] = \"right_only\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# merged.loc[merged[\"gdf_ismerge\"] & merged[\"df_ismerge\"].isna(), \"_merge\"] = \"left_only\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m merged[merged[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_merge\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright_only\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\merge.py:813\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 813\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_validate_kwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\merge.py:1657\u001b[0m, in \u001b[0;36m_MergeOperation._validate_validate_kwd\u001b[1;34m(self, validate)\u001b[0m\n\u001b[0;32m   1653\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   1654\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge keys are not unique in left dataset; not a one-to-one merge\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1655\u001b[0m         )\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m right_unique:\n\u001b[1;32m-> 1657\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   1658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMerge keys are not unique in right dataset; not a one-to-one merge\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1659\u001b[0m         )\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m validate \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone_to_many\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1:m\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m left_unique:\n",
      "\u001b[1;31mMergeError\u001b[0m: Merge keys are not unique in right dataset; not a one-to-one merge"
     ]
    }
   ],
   "source": [
    "# test all is ok\n",
    "from importlib import reload\n",
    "reload(test_tools)\n",
    "gdf = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_map.csv\")\n",
    "df = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\")\n",
    "test_tools.validate_hc_merge(df, gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4/87 [00:00<00:16,  5.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/87 [00:01<00:16,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 11/87 [00:01<00:11,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 13/87 [00:02<00:12,  5.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 20/87 [00:03<00:09,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 25/87 [00:04<00:09,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 28/87 [00:04<00:08,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 31/87 [00:05<00:10,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 37/87 [00:06<00:08,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 43/87 [00:07<00:06,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 46/87 [00:07<00:06,  5.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 49/87 [00:08<00:06,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 51/87 [00:08<00:06,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 55/87 [00:09<00:05,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 61/87 [00:10<00:04,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 66/87 [00:11<00:03,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 75/87 [00:12<00:01,  6.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 79/87 [00:13<00:01,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 81/87 [00:13<00:00,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 82/87 [00:13<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 83/87 [00:14<00:00,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:14<00:00,  5.87it/s]\n"
     ]
    }
   ],
   "source": [
    "for code in tqdm(df.adm0.unique()):\n",
    "    arg_df = df.query(f\"adm0=={code} and s1=='Hurricane' and s2=='Area' and s3f==3 and s4c==10 and s5==0\")\n",
    "    arg_gdf = gdf_full.query(f\"adm0=={code}\")\n",
    "    merged = arg_gdf.merge(arg_df, on=[\"adm0\", \"adm1\", \"adm2\"], how=\"outer\", indicator=True, validate=\"1:1\")\n",
    "    assert (merged._merge != \"right_only\").all()\n",
    "    if not (merged._merge == \"both\").all():\n",
    "        print(merged[merged._merge != \"both\"].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import procesa_bases \n",
    "\n",
    "gdf_full = gpd.read_feather(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\IPUMS_country_IDs.feather\")\n",
    "gdf_full = gdf_full.drop(columns=[\"ID\"])\n",
    "\n",
    "WB_country = procesa_bases.load_WB_country_data()\n",
    "IPUMS_country = procesa_bases.load_IPUMS_country_data(WB_country, keep_name=True)\n",
    "IPUMS_country = IPUMS_country.clip(WB_country.total_bounds)\n",
    "\n",
    "assert (gdf_full.merge(IPUMS_country, on=[\"GEOLEVEL1\", \"GEOLEVEL2\", \"CNTRY_CODE\"], how=\"outer\", indicator=True, validate=\"1:1\")._merge == \"both\").all()\n",
    "\n",
    "# Rename columns to make it in the intended format\n",
    "IPUMS_country = IPUMS_country.rename(columns={\"CNTRY_CODE\":\"adm0\", \"GEOLEVEL1\":\"adm1\", \"GEOLEVEL2\":\"adm2\", \"CNTRY_NAME\":\"adm0_name\", \"ADMIN_NAME\":\"adm2_name\"})\n",
    "IPUMS_country = IPUMS_country.drop(columns=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPUMS_country.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_raw\\button_labels.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\selector_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "\n",
    "# Assert that all data merges correctly\n",
    "\n",
    "gdf = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\IPUMS_map.csv\")\n",
    "gdf['geometry'] = gdf['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(gdf, crs='epsg:4326')\n",
    "\n",
    "df_adm2 = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.merge(df_adm2, on=[\"adm0\", \"adm2\"], validate=\"1:m\", how=\"outer\", indicator=True)._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = gdf_full[[\"CNTRY_CODE\", \"GEOLEVEL2\", \"geometry\"]].merge(df, right_on=[\"adm0\", \"adm2\"], left_on=[\"CNTRY_CODE\", \"GEOLEVEL2\"], how=\"outer\", indicator=True)\n",
    "merged = merged[~merged.CNTRY_CODE.isin([231,276,356,368,376,504,566,586,662])]\n",
    "pd.crosstab(merged[merged._merge!=\"both\"].CNTRY_CODE, merged[merged._merge!=\"both\"]._merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palestina no está porque solo tiene after. El resto 10/10\n",
    "\n",
    "import folium\n",
    "m = merged[merged._merge!=\"both\"].drop_duplicates(subset=[\"CNTRY_CODE\", \"GEOLEVEL2\"]).explore()\n",
    "\n",
    "# add control for layers\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
