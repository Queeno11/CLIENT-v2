{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import test_tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from procesa_bases import load_WB_country_data\n",
    "\n",
    "PATH = \"D:\\World Bank\\CLIENT v2\"\n",
    "DATA_RAW = rf\"{PATH}\\Data\\Data_raw\"\n",
    "DATA_PROC = rf\"{PATH}\\Data\\Data_proc\"\n",
    "DATA_OUT = rf\"{PATH}\\Data\\Data_out\"\n",
    "GPW_PATH = rf\"D:\\Datasets\\Gridded Population of the World\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera mapa con etiquetas de zona (adm0 adm1 adm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ID dataset\n",
    "gdf = gpd.read_feather(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\WB_country_IDs.feather\")\n",
    "gdf.columns = gdf.columns.str.lower()\n",
    "gdf = gdf.rename(columns={\"id\":\"ID\"}).drop(columns=\"objectid\")\n",
    "\n",
    "# Add names from the original WB adm2 dataset\n",
    "gdf_raw = load_WB_country_data()\n",
    "gdf_raw.columns = gdf_raw.columns.str.lower()\n",
    "gdf_raw = gdf_raw[[\"adm0_code\", \"adm1_code\", \"adm2_code\", \"adm0_name\", \"adm1_name\", \"adm2_name\", \"geometry\"]]\n",
    "assert gdf_raw.duplicated(subset=[\"adm0_code\", \"adm1_code\", \"adm2_code\"]).sum() == 0, \"There are duplicated entries in the raw dataset!!\"\n",
    "\n",
    "# Merge both datasets to assert that the codes are correct and consistent\n",
    "gdf = gdf.merge(gdf_raw.drop(columns=\"geometry\"), how=\"outer\", on=[\"adm0_code\", \"adm1_code\", \"adm2_code\"], indicator=True, validate=\"1:1\")\n",
    "assert (gdf._merge == \"both\").all(), \"There are problems with the merge!!\"\n",
    "gdf = gdf.drop(columns=\"_merge\")\n",
    "\n",
    "gdf.drop(columns=\"ID\").to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_map.csv\", index=False) # Export without the ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set admin level to categorical dtype (when the dataset is expanded, it will be more memory efficient)\n",
    "gdf[\"ID\"]        = gdf[\"ID\"].astype(\"category\")\n",
    "gdf[\"adm0_code\"] = gdf[\"adm0_code\"].astype(\"category\")\n",
    "gdf[\"adm1_code\"] = gdf[\"adm1_code\"].astype(\"category\")\n",
    "gdf[\"adm2_code\"] = gdf[\"adm2_code\"].astype(\"category\")\n",
    "gdf = gdf.set_index(\"ID\")\n",
    "\n",
    "gdf = gdf.drop(columns=[\"adm0_name\",\"adm1_name\",\"adm2_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera datos de shocks clim√°ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_raw = gpd.read_file(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_raw\\world_bank_adm2\\world_bank_adm2.shp\")\n",
    "gdf_raw.columns = gdf_raw.columns.str.lower()\n",
    "gdf_raw = gdf_raw[[\"adm0_code\", \"adm1_code\", \"adm2_code\", \"adm0_name\", \"adm1_name\", \"adm2_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "\n",
    "def expand_dataset(df, gdf):\n",
    "                    \n",
    "    # Collect all dimension values from df\n",
    "    all_years      = df.index.get_level_values(\"year\").categories\n",
    "    all_variables  = df.index.get_level_values(\"variable\").categories\n",
    "    all_thresholds = df.index.get_level_values(\"threshold\").categories\n",
    "    all_measures   = df.index.get_level_values(\"measure\").categories\n",
    "    all_regions    = gdf.index.categories # ID is the index of gdf\n",
    "\n",
    "    # Convert each list to a small DataFrame\n",
    "    df_years      = pd.DataFrame({'year': all_years}, dtype='category')\n",
    "    df_variables  = pd.DataFrame({'variable': all_variables}, dtype='category')\n",
    "    df_thresholds = pd.DataFrame({'threshold': all_thresholds}, dtype='category')\n",
    "    df_measures   = pd.DataFrame({'measure': all_measures}, dtype='category')\n",
    "    df_regions    = pd.DataFrame({'ID': all_regions}, dtype='category')\n",
    "\n",
    "    # Step-by-step merges using how='cross'\n",
    "    df_temp = df_years.merge(df_variables, how='cross')\n",
    "    df_temp = df_temp.merge(df_regions, how='cross')\n",
    "    df_temp = df_temp.merge(df_thresholds, how='cross')\n",
    "    df_temp = df_temp.merge(df_measures, how='cross')\n",
    "    expanded_without_data = df_temp.set_index([\"ID\", \"year\", \"variable\", \"threshold\", \"measure\"])\n",
    "    \n",
    "    # add admcodes to the expanded set\n",
    "    expanded_without_data = expanded_without_data.join(\n",
    "        gdf.drop(columns=[\"geometry\"]),\n",
    "        how=\"left\",\n",
    "        on=\"ID\",\n",
    "        validate=\"m:1\"\n",
    "    )\n",
    "    \n",
    "    # Merge original data (df) onto the expanded set\n",
    "    expanded_with_data = expanded_without_data.join(\n",
    "        df,\n",
    "        how=\"left\",\n",
    "        validate=\"1:1\",\n",
    "        rsuffix=\"_y\"\n",
    "    ).reset_index().drop(columns=\"ID\")\n",
    "    \n",
    "    expanded_with_data = test_tools.assert_correct_admcodes(expanded_with_data)        \n",
    "\n",
    "    return expanded_with_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "## Generados Nico\n",
    "# Set dtypes to make this loading efficient\n",
    "dtypes = {\"year\": np.int16, \"variable\":\"category\", \"threshold\":\"category\", \"area_affected\":np.float32, \"population_affected\":np.float32, \"ID\":np.int64}# \"adm2_code\": np.int16, \"adm1_code\": np.int16, \"adm0_code\": np.int16,\n",
    "\n",
    "for shock in [\"floods\", \"drought\", \"hurricanes\", \"intenserain\", \"heatwaves\", \"coldwaves\"]:\n",
    "    print(shock)\n",
    "    df = pd.read_csv(\n",
    "        rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\WB_{shock}_long.csv\",\n",
    "        dtype=dtypes, \n",
    "        usecols=dtypes.keys(),\n",
    "    )\n",
    "        \n",
    "    # Set ID to categorical dtype (this is after loading as int to match with the categories of gdf)\n",
    "    df[\"ID\"] = df[\"ID\"].astype(\"category\")\n",
    "    \n",
    "    # Reshape to long format\n",
    "    df = df.melt(id_vars=[\"ID\", \"year\", \"variable\", \"threshold\"], var_name=\"measure\", value_name=\"value\")\n",
    "\n",
    "    # Set categorical and index to make faster merges\n",
    "    df[\"measure\"] = df[\"measure\"].astype(\"category\")\n",
    "    df[\"year\"] = df[\"year\"].astype(\"category\")\n",
    "    df = df.set_index([\"ID\"])    \n",
    "    \n",
    "    # Add adm0, adm1 and adm2 codes    \n",
    "    df = gdf.drop(columns=[\"geometry\"]).join(df, on=[\"ID\"], how=\"inner\", validate=\"1:m\")\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Set index to make faster merges and expand dataset\n",
    "    #   Replace columns with null categories with zeros before setting the index to make it work as expected:\n",
    "    index = [\"ID\", \"year\", \"variable\", \"threshold\", \"measure\"]\n",
    "    for col in index:\n",
    "        if (df[col].dtype == \"category\"):\n",
    "            if (df[col].cat.categories.shape[0]==0):\n",
    "                df[col] = df[col].astype(float).fillna(0)\n",
    "                df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    df = df.set_index(index)\n",
    "    df = expand_dataset(df, gdf)\n",
    "\n",
    "    # Test the output\n",
    "    test_tools.assert_correct_colnames(df)\n",
    "    test_tools.assert_correct_shape(df, gdf)\n",
    "\n",
    "    # Export\n",
    "    df.to_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_{shock}.csv\", index=False)\n",
    "\n",
    "    df = None\n",
    "    gc.collect()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all is ok\n",
    "from importlib import reload\n",
    "reload(test_tools)\n",
    "gdf = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_map.csv\")\n",
    "# gdf = gpd.GeoDataFrame(gdf, geometry=gpd.GeoSeries.from_wkt(gdf[\"geometry\"]))\n",
    "\n",
    "for shock in [\"floods\", \"drought\", \"hurricanes\", \"intenserain\", \"heatwaves\", \"coldwaves\"]:\n",
    "    print(\"Verifying\", shock)\n",
    "    df = pd.read_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_{shock}.csv\")\n",
    "    test_tools.validate_climate_dataset(df, gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\WB_map.csv\")\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_b = gdf.sort_values([\"adm0_code\", \"adm1_code\", \"adm2_code\"])\n",
    "test_b = test_b.set_index([\"adm0_code\", \"adm1_code\", \"adm2_code\"])\n",
    "test_b = test_b.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = df.query(\"year==2000 and measure=='area_affected'\").sort_values([\"adm0_code\", \"adm1_code\", \"adm2_code\"])\n",
    "test_a = test_a.set_index([\"adm0_code\", \"adm1_code\", \"adm2_code\"])\n",
    "test_a = test_a.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a.merge(test_b, on=[\"adm0_code\", \"adm1_code\", \"adm2_code\"], how=\"outer\", validate=\"1:1\", indicator=True)._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPUMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "gdf_full = gpd.read_feather(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_proc\\IPUMS_country_IDs.feather\")\n",
    "gdf_full = gdf_full.drop(columns=[\"ID\"])\n",
    "gdf_full = gdf_full.rename(columns={\"CNTRY_CODE\":\"adm0\", \"GEOLEVEL1\":\"adm1\", \"GEOLEVEL2\":\"adm2\"})\n",
    "gdf_full[[\"adm0_name\", \"adm1_name\", \"adm2_name\"]] = \"To be filled\"\n",
    "ids = [\"adm0\", \"adm2\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Shocks\n",
    "path = r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\"\n",
    "       \n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if \"HC_national_data\" in f and f.endswith(\".csv\")]\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\\{file}\")\n",
    "    df[\"s3\"] = pd.NA\n",
    "    df[\"s4\"] = pd.NA\n",
    "    \n",
    "    s3cols = [\"s3a\", \"s3b\", \"s3c\", \"s3d\", \"s3f\"]\n",
    "    s4cols = [\"s4a\", \"s4b\", \"s4c\"]\n",
    "\n",
    "    for col in s3cols: \n",
    "        df[\"s3\"] = df[\"s3\"].fillna(df[col])\n",
    "        assert (df[s3cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "    for col in s4cols:\n",
    "        df[\"s4\"] = df[\"s4\"].fillna(df[col])\n",
    "        assert (df[s4cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "\n",
    "    dfs += [df]\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "for col in df.columns:\n",
    "    assert df[col].isna().all() == False\n",
    "\n",
    "# Drop s3* columns\n",
    "df = df.drop(columns=[col for col in df.columns if (\"s3\" in col or \"s4\" in col) and (col != \"s3\" and col != \"s4\")])\n",
    "# Order variables\n",
    "df = df[[\"adm0\", \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"outcome\", \"new\", \"v\"]]\n",
    "df = df.rename(columns={\"new\":\"time\", \"v\": \"value\", \"status\":\"treatment\"})\n",
    "df.loc[df.s1 == \"Hurricane\", \"s5\"] = df.loc[df.s1 == \"Hurricane\", \"s5\"] / 100\n",
    "\n",
    "df = df.merge(gdf_full[[\"adm0\"]].drop_duplicates(), on=[\"adm0\"], validate=\"m:1\")\n",
    "print(f\"Hay datos de {df.adm0.unique().size} pa√≠ses\")\n",
    "df.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_national_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    \"s1\":\"Shock\",\n",
    "    \"s2\":\"Weight\",\n",
    "    \"s3\": {\n",
    "        \"Cold wave\":\"Temperature <0 ¬∞C\",\n",
    "        \"Heat wave\":\"Degrees (¬∞C)\",\n",
    "        \"Drought\":\"Drought indicator\",\n",
    "        \"Intense rain\":\"Number of days\",\n",
    "        \"Hurricane\":\"Category\"\n",
    "    },\n",
    "    \"s4\": {\n",
    "        \"Cold wave\":\"Standard Deviations from historical mean\",\n",
    "        \"Heat wave\":\"Standard Deviations from historical mean\",\n",
    "        \"Drought\":\"Standard Deviations from historical mean\",\n",
    "        \"Intense rain\":\"Rainfall (mm)\",\n",
    "        \"Hurricane\":\"Distance from center of the storm (degrees)\" # Fixme: turn to km\n",
    "    },\n",
    "    \"s5\": r\"Threshold (% affected)\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Shocks\n",
    "path = r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\"\n",
    "       \n",
    "files = os.listdir(path)\n",
    "files = [f for f in files if \"HC_geodata\" in f and f.endswith(\".csv\")]\n",
    "\n",
    "dfs = []\n",
    "for file in tqdm(files):\n",
    "    df = pd.read_csv(rf\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\HC Treatment Complete\\{file}\")\n",
    "    df[\"s3\"] = pd.NA\n",
    "    df[\"s4\"] = pd.NA\n",
    "    \n",
    "    s3cols = [\"s3a\", \"s3b\", \"s3c\", \"s3d\", \"s3f\"]\n",
    "    s4cols = [\"s4a\", \"s4b\", \"s4c\"]\n",
    "\n",
    "    for col in s3cols: \n",
    "        df[\"s3\"] = df[\"s3\"].fillna(df[col])\n",
    "        assert (df[s3cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "    for col in s4cols:\n",
    "        df[\"s4\"] = df[\"s4\"].fillna(df[col])\n",
    "        assert (df[s4cols].notna().sum(axis=1) <= 1).all(), f\"{df[(df[s3cols].notna().sum(axis=1) > 1)]}\"\n",
    "\n",
    "    dfs += [df]\n",
    "    \n",
    "df = pd.concat(dfs)\n",
    "for col in df.columns:\n",
    "    assert df[col].isna().all() == False\n",
    "\n",
    "# Drop s3* columns\n",
    "df = df.drop(columns=[col for col in df.columns if (\"s3\" in col or \"s4\" in col) and (col != \"s3\" and col != \"s4\")])\n",
    "# Order variables\n",
    "df = df[[\"adm0\", \"adm1\", \"adm2\", \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"outcome\", \"status\", \"diftime\"]]\n",
    "df = df.rename(columns={\"status\":\"treatment_sub\", \"diftime\":\"diff\"})\n",
    "df.loc[df.s1 == \"Hurricane\", \"s5\"] = df.loc[df.s1 == \"Hurricane\", \"s5\"] / 100\n",
    "\n",
    "df.merge(gdf_full[[\"adm0\", \"adm2\"]], on=[\"adm0\", \"adm2\"], validate=\"m:1\", how=\"inner\")\n",
    "print(f\"Hay datos de {df.adm0.unique().size} pa√≠ses\")\n",
    "\n",
    "df.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\", dtype={\"adm0\":\"float\", \"adm2\":\"float\"})\n",
    "df = df[[\"adm0\", \"adm2\"]].drop_duplicates()\n",
    "gdf_full.merge(df, on=[\"adm0\", \"adm2\"], validate=\"1:m\", how=\"inner\").to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_map.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_map.csv\")\n",
    "df_national = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_national_data.csv\")\n",
    "df_geo = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = df_map.merge(df_national.drop_duplicates(subset=\"adm0\"), on=[\"adm0\"], validate=\"m:1\", how=\"outer\", indicator=True)\n",
    "assert m1[m1._merge != \"both\"].shape[0] == 0\n",
    "m2 = df_map.merge(df_geo, on=[\"adm0\", \"adm2\"], validate=\"1:m\", how=\"outer\", indicator=True)\n",
    "assert m2[m2._merge != \"both\"].shape[0] == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_raw\\button_labels.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\selector_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "\n",
    "# Assert that all data merges correctly\n",
    "\n",
    "gdf = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\IPUMS_map.csv\")\n",
    "gdf['geometry'] = gdf['geometry'].apply(wkt.loads)\n",
    "gdf = gpd.GeoDataFrame(gdf, crs='epsg:4326')\n",
    "\n",
    "df_adm2 = pd.read_csv(r\"D:\\World Bank\\CLIENT v2\\Data\\Data_out\\for webpage\\HC_geo_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.merge(df_adm2, on=[\"adm0\", \"adm2\"], validate=\"1:m\", how=\"outer\", indicator=True)._merge.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = gdf_full[[\"CNTRY_CODE\", \"GEOLEVEL2\", \"geometry\"]].merge(df, right_on=[\"adm0\", \"adm2\"], left_on=[\"CNTRY_CODE\", \"GEOLEVEL2\"], how=\"outer\", indicator=True)\n",
    "merged = merged[~merged.CNTRY_CODE.isin([231,276,356,368,376,504,566,586,662])]\n",
    "pd.crosstab(merged[merged._merge!=\"both\"].CNTRY_CODE, merged[merged._merge!=\"both\"]._merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palestina no est√° porque solo tiene after. El resto 10/10\n",
    "\n",
    "import folium\n",
    "m = merged[merged._merge!=\"both\"].drop_duplicates(subset=[\"CNTRY_CODE\", \"GEOLEVEL2\"]).explore()\n",
    "\n",
    "# add control for layers\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
